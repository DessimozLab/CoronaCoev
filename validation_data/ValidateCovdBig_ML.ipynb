{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import wget\n",
    "import requests\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "import h5py\n",
    "\n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit( 10 **9 )\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics import roc_curve , precision_recall_curve , auc\n",
    "\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import scipy\n",
    "import copy\n",
    "from numpy import linalg as LA\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "import itertools\n",
    "import dendropy\n",
    "import seaborn as sns\n",
    "\n",
    "overwrite = False\n",
    "jk_iterations = 5\n",
    "os.environ['MKL_ENABLE_INSTRUCTIONS'] = 'AVX2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#create graphs on the fly to represent pairs of profiles\n",
    "device = torch.device('cuda' )\n",
    "#device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import  to_hetero\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData ,InMemoryDataset\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = '/work/FAC/FBM/DBC/cdessim2/default/dmoi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_02021-09-27T12:05:34.021732small_test0.001_BS_coevmats.pkl', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_02021-09-27T12:05:34.021732small_test0.1_BS_coevmats.pkl', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_02021-09-27T12:05:34.021732small_test0.01_BS_coevmats.pkl']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob( filedir + 'datasets/covid_data/msa_0730/msa_0730.fasta*02021-09-27T12:05:34*pkl')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treefile = '../validation_data/covid19/gisaid_hcov-2020_08_25.QC.NSoutlier.filter.deMaiomask.aln.EPIID.treefile'\n",
    "#alnfile = '../validation_data/covid19/gisaid_hcov-2020_08_25.QC.NSoutlier.filter.deMaiomask.EPIID.aln'\n",
    "alnfile = filedir + 'datasets/covid_data/msa_0730/msa_0730.fasta'\n",
    "treefile = filedir + 'datasets/covid_data/msa_0730/global.tree'\n",
    "\n",
    "alnh5 = alnfile+'.h5'\n",
    "#ts = '2021-08-08T11:16:34.358764'\n",
    "ts = '2021-08-08T14:37:59.736512'\n",
    "events = alnfile+'*'+ts+'*'\n",
    "eventmats = glob.glob(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = dendropy.Tree.get( path=treefile, schema='newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the internal nodes for the fitch algo\n",
    "for i,l in enumerate(tree.nodes()):\n",
    "    l.event = {}\n",
    "    l.scores = {}\n",
    "    l.symbols = None\n",
    "    l.char= None\n",
    "    l.matrow = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import colour\n",
    "#quick and dirty check of trees\n",
    "\n",
    "def tree_circle(node, start , sliver  ):\n",
    "    if start == True :\n",
    "        global count\n",
    "        count = 0\n",
    "    for i,child in enumerate(node.child_nodes()):\n",
    "        if child.is_leaf() == True:\n",
    "            child.radians = count*sliver\n",
    "            count+=1\n",
    "        if child.radians is None and child.is_leaf() == False :\n",
    "            tree_circle( child , start = False , sliver = sliver  )\n",
    "    radians = np.mean([ child.radians if child.radians else 0 for child in node.child_nodes() ])\n",
    "    if node.radians is None:\n",
    "        node.radians = radians\n",
    "\n",
    "def phylograph(treein,labels , title = None):\n",
    "    N = len(treein.nodes())\n",
    "    tree = copy.deepcopy(treein)\n",
    "    pdm = tree.phylogenetic_distance_matrix()\n",
    "    sliver = 2*np.pi / len(tree.leaf_nodes())\n",
    "    \n",
    "    root = tree.seed_node\n",
    "    radii = [ n.distance_from_root() for n in tree.nodes()]\n",
    "    for n in tree.nodes():\n",
    "        n.radians = None\n",
    "    tree_circle(tree.seed_node, start=True , sliver = sliver)\n",
    "    thetas = [n.radians for n in tree.nodes() ]\n",
    "    pos = { i: [ np.sin(thetas[i])*radii[i] , np.cos(thetas[i])*radii[i]] for i in range(len(thetas)) }\n",
    "    index = np.vstack([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    connectmat = np.zeros(( N ,  N ) )\n",
    "    connectmat[index[:,0],index[:,1]] = 1 \n",
    "    connectmat += connectmat.T\n",
    "    G = nx.from_numpy_array(connectmat)\n",
    "    red = colour.Color(\"red\")\n",
    "    blue = colour.Color(\"blue\")\n",
    "    crange = dict( zip ( list(set(labels)),  [ c.hex_l for c in list(red.range_to(blue, len(set(labels)) ) ) ] ) )\n",
    "    colors = [crange[n] for n in labels ]\n",
    "    #color according to downstream node\n",
    "    edge_colors= [crange[labels[v]] for u,v in G.edges() ]\n",
    "    #node size inversly proportional to number in graph\n",
    "    plt.figure(figsize= (20,20) )\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    nx.draw_networkx(G, pos = pos,  node_color = colors , node_size = 5, width = .5 , edge_color = edge_colors , with_labels=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(y_data, label = None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        fpr, tpr, _ = roc_curve(   y_pred_grd ,y_test)\n",
    "        plt.plot(fpr, tpr, label=l + 'auc'+ str(auc(fpr, tpr) ))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_ROC.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    for l in y_data:\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        precision, recall, thresholds = precision_recall_curve( y_pred_grd, y_test)\n",
    "        plt.plot( recall, precision , label= l )\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "    \n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_PR.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "def ROC_curve_single(y_test, y_pred_grd):\n",
    "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_grd)\n",
    "    plt.plot(fpr, tpr, label='single')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(  y_test , y_pred_grd)\n",
    "    plt.plot(precision, recall , label='single')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "\n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_02021-08-08T14:37:59.736512small_test_BS_coevmats.pkl', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_22021-08-08T14:37:59.736512small_test_BS_coevmats.pkl', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_12021-08-08T14:37:59.736512small_test_BS_coevmats.pkl']\n"
     ]
    }
   ],
   "source": [
    "print( eventmats )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists( alnfile + '_IDs.pkl'):\n",
    "    with open( alnfile + '_IDs.pkl' , 'rb') as idxin:\n",
    "        IDindex = pickle.loads(idxin.read())\n",
    "    IDs = dict(zip( IDindex.values() , IDindex.keys() ) )\n",
    "else:\n",
    "    \n",
    "    msa = SeqIO.parse(alnfile , format = 'fasta')\n",
    "    def clipID(ID):\n",
    "        return ''.join( [ s +'|' for s in str(ID).split('|')[:-1] ])[:-1].replace('_',' ') \n",
    "    IDs = {i:rec.id for i,rec in enumerate(msa)}\n",
    "    IDindex = dict(zip( IDs.values() , IDs.keys() ) )\n",
    "    print( [(t,IDindex[t]) for t in list(IDindex.keys())[0:10]] )\n",
    "    with open( alnfile + '_IDs.pkl' , 'wb') as idxout:\n",
    "        idxout.write(pickle.dumps(IDindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2212775\n"
     ]
    }
   ],
   "source": [
    "print(len(IDindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = '/scratch/dmoi/datasets/covid_data/structs/'\n",
    "modelfiles = modeldir + '*.pdb'\n",
    "modelfiles = glob.glob( modelfiles )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import MMCIFParser, PDBParser , PDBIO\n",
    "from Bio.PDB.mmcifio import MMCIFIO\n",
    "parser = PDBParser()\n",
    "import warnings\n",
    "from Bio import SeqUtils\n",
    "###compile all pdbs to fasta \n",
    "converter = SeqUtils.IUPACData.protein_letters_3to1\n",
    "converter = { res.upper():converter[res] for res in converter}\n",
    "chain_sequences={}\n",
    "if overwrite == True:\n",
    "    for model in modelfiles:\n",
    "        with warnings.catch_warnings():\n",
    "            try:\n",
    "                m = model.split('/')[-1].replace('pdb' , '') \n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                structure = parser.get_structure(m, model)\n",
    "                io=PDBIO()\n",
    "                io.set_structure(structure)\n",
    "                chain_sequences[model]= { c.id : ''.join(  [ converter[ r.get_resname()] for r in c.get_residues() if 'CA' in r ] ) for c in structure.get_chains() }\n",
    "                chain_sequences[model]={ c: chain_sequences[model][c] for c in chain_sequences[model] if len(chain_sequences[model][c])> 0 }\n",
    "            except:\n",
    "                print('err', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( modeldir + 'struct_chains.fasta', 'w') as fastout:\n",
    "    for model in chain_sequences:\n",
    "        for c in chain_sequences[model]:\n",
    "            fastout.write( '> '+model+ ':'+c + '\\n' + chain_sequences[model][c] + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flashing up a dask cluster\n",
      "testing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dask.distributed import fire_and_forget\n",
    "from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import  utils_perf\n",
    "import gc\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "from dask import delayed, compute\n",
    "\n",
    "distributed_computation = False\n",
    "\n",
    "print('flashing up a dask cluster')\n",
    "if distributed_computation == True:\n",
    "    \n",
    "    NCORE = 10\n",
    "    njobs = 20\n",
    "    print('deploying cluster')\n",
    "    cluster = SLURMCluster(\n",
    "        walltime='24:00:00',\n",
    "        n_workers = NCORE,\n",
    "        cores=NCORE,\n",
    "        processes = NCORE,\n",
    "        interface='ib0',\n",
    "        memory=\"150GB\",\n",
    "        env_extra=[\n",
    "        'source /scratch/dmoi/miniconda/etc/profile.d/conda.sh',\n",
    "        'conda activate ML'\n",
    "        ],\n",
    "        scheduler_options={'interface': 'ens2f0' }\n",
    "    )\n",
    "    print(cluster.job_script())\n",
    "    #cluster.adapt(minimum=10, maximum=30)\n",
    "    cluster.scale(jobs=20)\n",
    "    time.sleep(5)\n",
    "\n",
    "    print(cluster)\n",
    "    print(cluster.dashboard_link)\n",
    "    client = Client(cluster , timeout='450s' , set_as_default=True )\n",
    "else:\n",
    "    if __name__ == '__main__':\n",
    "        NCORE = 5\n",
    "        njobs = 1\n",
    "        print('testing')\n",
    "        cluster = LocalCluster(n_workers = NCORE )    \n",
    "        print(cluster.dashboard_link)\n",
    "        client = Client(cluster)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotide_mutation = None\n",
    "AA_mutation = None\n",
    "for mat in eventmats:\n",
    "    with open( mat , 'rb') as pklin:\n",
    "        mats = pickle.loads(pklin.read())\n",
    "        print(mats)\n",
    "        if AA_mutation is None:\n",
    "            nucleotide_mutation = mats[1]\n",
    "            AA_mutation = mats[0]\n",
    "        else:\n",
    "            nucleotide_mutation += mats[1]\n",
    "            AA_mutation += mats[0]\n",
    "print(nucleotide_mutation)\n",
    "print(AA_mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_cleanup = True\n",
    "from scipy.stats import gamma\n",
    "\n",
    "if mat_cleanup == True:\n",
    "    for transition in range(AA_mutation.shape[2]):\n",
    "        if transition % 10 == 0 :\n",
    "            print(transition)\n",
    "        coevmat = AA_mutation[:,:,transition]\n",
    "        sumv = coevmat.sum(axis = 0).todense()\n",
    "\n",
    "        posi = np.log(sumv[sumv>0])\n",
    "        if len(posi)>0 and np.sum(posi)>10:\n",
    "            \n",
    "            try:\n",
    "                a,loc, scale =gamma.fit(posi)\n",
    "                probas = gamma.cdf( posi , a , loc , scale)\n",
    "\n",
    "                if np.amax(probas)>.999:\n",
    "                    thresh = np.amin(np.exp(posi[probas>.999]))\n",
    "                #remove odd looking transition columns\n",
    "            except:\n",
    "                thresh  = 0\n",
    "                \n",
    "                if transition == 0:\n",
    "                    AAmat = AA_mutation[:,:,transition].to_scipy_sparse()\n",
    "                    AAmat[:,sumv>thresh] = 0\n",
    "                else:\n",
    "                    AAmat_ =  scipy.sparse.csc_matrix(AA_mutation[:,:,transition].to_scipy_sparse())\n",
    "                    AAmat_[:,sumv>thresh] = 0\n",
    "                    AAmat += scipy.sparse.coo_matrix(AAmat_)        \n",
    "            cdf = np.array( np.cumsum(sumv) / np.sum(sumv))        \n",
    "        else:\n",
    "            if transition == 0:\n",
    "                AAmat =  AA_mutation[:,:,transition].to_scipy_sparse()\n",
    "            else:\n",
    "                AAmat +=  AA_mutation[:,:,transition].to_scipy_sparse()\n",
    "                \n",
    "    with open( alnfile + 'AAmat_sum.pkl' , 'wb')as AAmatout:\n",
    "        print(AAmat.shape)\n",
    "        AAmatout.write(pickle.dumps(AAmat))\n",
    "else:\n",
    "    with open( alnfile + 'AAmat_sum.pkl' , 'rb')as AAmatout:\n",
    "        AAmat = pickle.loads(AAmatout.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = [ b'A', b'C', b'G' , b'T' ]\n",
    "allowed_transitions = [ c1+c2 for c1 in allowed_symbols for c2 in allowed_symbols  if c1!= c2]\n",
    "print('allowed transitions',allowed_transitions)\n",
    "\n",
    "transition_dict = {  c : i  for i,c in enumerate( allowed_transitions )  }\n",
    "rev_transition_dict= dict( zip(transition_dict.values(), transition_dict.keys()))\n",
    "allowed_symbols = set(allowed_symbols)\n",
    "\n",
    "print('transition dict', transition_dict)\n",
    "ProteinAlphabet = [ 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y' ]\n",
    "allowed_AA_transitions = [ c1+c2 for c1 in ProteinAlphabet for c2 in ProteinAlphabet  if c1!= c2]\n",
    "transitiondict_AA = {  c : i  for i,c in enumerate( allowed_AA_transitions )  }\n",
    "rev_transitiondict_AA = dict( zip(transitiondict_AA.values(), transitiondict_AA.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,n in enumerate(tree.nodes()):\n",
    "    n.matrow = i\n",
    "    n.symbols = None\n",
    "    n.scores = None\n",
    "    n.event = None\n",
    "    n.char = None\n",
    "\n",
    "matsize = len(tree.nodes())\n",
    "print(matsize)\n",
    "print('nodes')\n",
    "#blur w connectivity mat\n",
    "blurfactor =  .25\n",
    "connectmat = scipy.sparse.csc_matrix((len(tree.nodes()), len(tree.nodes() ) ) )\n",
    "index = np.array([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "lengths = np.array([ c.edge_length for n in tree.nodes() for c in n.child_nodes()])\n",
    "total_len = np.sum(lengths)\n",
    "#fill diagonal\n",
    "#index = np.vstack( [index , np.array([ [n.matrow, n.matrow ] for n in tree.nodes() ]) ] )\n",
    "\n",
    "connectmat[index[:,0],index[:,1]] = 1\n",
    "connectmat[index[:,1],index[:,0]] = 1\n",
    "\n",
    "\n",
    "#connectmat = connectmat.todense()\n",
    "diag = [ i for i in range(connectmat.shape[0])]\n",
    "connectmat[diag,diag] = 1\n",
    "#connectmat = connectmat.todense()\n",
    "#connectmat = scipy.sparse.csc_matrix(connectmat)\n",
    "#np.fill_diagonal(connectmat , 1)\n",
    "connectmat = scipy.sparse.coo_matrix(connectmat)\n",
    "plt.figure( figsize=(10,10))\n",
    "plt.title( 'phylo tree connectivity matrix ' )\n",
    "plt.spy(connectmat, markersize= 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####define the aln row and structure here, for dengue they should all be the same\n",
    "selected_strain = list(IDindex.keys())[100]\n",
    "import dask.array as da\n",
    "with h5py.File(alnh5, 'r') as hf:\n",
    "    align_array = hf['MSA2array']\n",
    "    print(IDindex[selected_strain])\n",
    "    print(selected_strain)\n",
    "    #filter to columns without gaps\n",
    "    non_gap = np.where( align_array[: , IDindex[selected_strain]] != b'-')[0]\n",
    "    print(non_gap)\n",
    "    print(non_gap.shape)\n",
    "    sequence = align_array[ non_gap , IDindex[selected_strain]]\n",
    "    print(np.unique(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the amino acid chains in the structs\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "with open( alnfile + 'refgeno.fasta' , 'w' ) as fastout:\n",
    "    fastout.write('>'+selected_strain + '\\n')\n",
    "    fastout.write( ''.join( c.decode() for c in list(sequence) ) + '\\n' )\n",
    "qfile =  alnfile + 'refgeno.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blastpath = '/scratch/dmoi/software/ncbi-blast-2.11.0+-src/c++/ReleaseMT/bin/'\n",
    "print(modeldir + 'covid_structs.fasta')\n",
    "def runblastx( qseq , blastpath = blastpath , outannot = alnfile+'struct_blastout.txt' , db = modeldir + 'struct_chains.fasta' , outfmt = None ):\n",
    "    if outfmt is None:\n",
    "        outfmt = [ 'qseqid' , 'sseqid' , 'qlen' ,  'slen' , 'qstart' , 'qend' ,  'qframe' , 'evalue' ]\n",
    "        outfmt =  ' \"10 ' + ''.join([fmt+ ' ' for fmt in outfmt]) + ' \" '\n",
    "        print(outfmt)\n",
    "    args = blastpath+'blastx -query '+ qfile + ' -db '+db+' -outfmt' + outfmt + ' -out ' + outannot  \n",
    "    print(args)\n",
    "    p = subprocess.run( shlex.split(args) )\n",
    "    return p , outannot\n",
    "\n",
    "p,annot = runblastx(qfile)\n",
    "print(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pd.read_csv(  annot , header = None )\n",
    "annotation.columns = [ 'qseqid' , 'sseqid' , 'qlen' ,  'slen' , 'qstart' , 'qend' ,  'qframe' , 'evalue' ] \n",
    "annotation = annotation[ annotation['evalue'] < 10**-3 ]\n",
    "\n",
    "annotation['struct'] = annotation.sseqid.map( lambda x : x.split(':')[0].split('/')[-1].replace('.pdb','') )\n",
    "annotation['chain'] = annotation.sseqid.map( lambda x : x.split(':')[1] )\n",
    "annotation.to_csv( alnfile +'struct_annotation.csv'  )\n",
    "print(annotation)\n",
    "#make annotation for dengue orfs\n",
    "#find equivalent structures in multimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainlen = dict( zip ( annotation.struct + annotation.chain , annotation.slen ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_annot = False\n",
    "if filter_annot == True:\n",
    "    annotation = annotation[annotation.struct == '7DZW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab AA chain residues\n",
    "import itertools\n",
    "import warnings\n",
    "distmats = {}\n",
    "overwrite_connect = True\n",
    "@dask.delayed()\n",
    "def retmat_single(totalatoms, chain, slen= None):\n",
    "    ret = np.array( [[ a1['CA'] - a2['CA']  for i,a1 in enumerate(totalatoms[chain])  ] for j,a2 in enumerate(totalatoms[chain]) ] )\n",
    "    if slen:\n",
    "        ret = ret[:slen]\n",
    "        ret = ret[:,:slen]\n",
    "    return ret\n",
    "    \n",
    "@dask.delayed()\n",
    "def retmat_double(totalatoms, chain1, chain2 , slen1= None , slen2 = None):\n",
    "    ret = np.array( [[ a1['CA'] - a2['CA'] for i,a1 in enumerate(totalatoms[chain1])] for j,a2 in enumerate(totalatoms[chain2]) ] )\n",
    "    if slen1:\n",
    "        ret = ret[:slen1]\n",
    "    if slen2:\n",
    "        ret = ret[:,:slen2]\n",
    "    return ret\n",
    "\n",
    "@dask.delayed()\n",
    "def addT(arr):\n",
    "    arr += arr.T\n",
    "\n",
    "if overwrite_connect == True:\n",
    "    totalatoms = {}\n",
    "    for i,model in enumerate(annotation.sseqid.unique()):\n",
    "        with warnings.catch_warnings():\n",
    "            m = model.split('/')[-1].replace('.pdb' , '').split(':')[0]\n",
    "            print(m)\n",
    "            \n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            structure = parser.get_structure(m, model.split(':')[0])\n",
    "            io=PDBIO()\n",
    "            io.set_structure(structure)\n",
    "            totalatoms.update( { c.id : [ r for r in c.get_residues() if 'CA'  in r ] for c in structure.get_chains() } )\n",
    "    for chain in totalatoms:\n",
    "        print(structure)\n",
    "        if model not in distmats:\n",
    "            distmats[m]= {}\n",
    "            #if m+chain in chainlen:\n",
    "            #    l1 = chainlen[m+chain]    \n",
    "        l1 = None\n",
    "        distmats[m][chain] = retmat_single( totalatoms, chain , l1 )\n",
    "            #get interchain dists\n",
    "    combocount = 0\n",
    "    for chain1,chain2 in itertools.combinations(totalatoms,2):\n",
    "        if combocount < 2:\n",
    "            l1 = None\n",
    "            #if m+chain1 in chainlen:\n",
    "            #    l1 = chainlen[m+chain1]\n",
    "            l2 = None\n",
    "            #if m+chain2 in chainlen:\n",
    "            #    l2 = chainlen[m+chain2]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        combocount+=1\n",
    "        distmats[m][(chain1,chain2)] = retmat_double(totalatoms, chain1, chain2  ,l1 , l2)    \n",
    "        #compute vals\n",
    "        #    print( ' err ', m )\n",
    "    if m in distmats:\n",
    "        distmats[m] = dict( zip ( distmats[m].keys() , dask.compute( * list(distmats[m].values() ))))\n",
    "        #distmats[m] = dict( zip ( distmats[m].keys() , dask.compute( * [dropna(a) for a in list(distmats[m].values() )] ) ) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_distmats = False\n",
    "remove =[]\n",
    "if overwrite_connect == True:\n",
    "    if show_distmats == True:\n",
    "        for model in distmats:\n",
    "            print( model )\n",
    "            for i,chain in enumerate(distmats[model]):\n",
    "                if distmats[model][chain].shape[0]>0:\n",
    "                    if np.sum(distmats[model][chain])==0:\n",
    "                        remove.append((model,chain))\n",
    "                    else:\n",
    "                        plt.figure(figsize= (20,20))\n",
    "                        plt.title('distmat ' + model + '  '+  ''.join(chain))\n",
    "                        plt.imshow(distmats[model][chain])\n",
    "                        plt.show()\n",
    "    for model,chain in remove:\n",
    "        del distmats[model][chain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "angstrom_cutoff = 15\n",
    "show_contacts = True\n",
    "\n",
    "if overwrite_connect == True:\n",
    "    @dask.delayed( nout = 2)\n",
    "    def define_contacts(mat , angstrom_cutoff = 15 ):\n",
    "        subthresh = copy.deepcopy(mat)\n",
    "        subthresh[ subthresh < angstrom_cutoff ] = 1 \n",
    "        subthresh[ subthresh > angstrom_cutoff ] = 0 \n",
    "        np.fill_diagonal(subthresh , 0)\n",
    "        subthresh = subthresh[0:subthresh.shape[0]-20,0:subthresh.shape[1]-20]\n",
    "        connected = np.dot(subthresh,subthresh.T)\n",
    "        return subthresh,connected\n",
    "\n",
    "    subthresh_thresh ={}\n",
    "    subthresh_connected ={}\n",
    "\n",
    "    chain_equivalencies={}\n",
    "    for m in distmats:\n",
    "        for chain in distmats[m]:\n",
    "            if m not in subthresh_thresh:\n",
    "                subthresh_thresh[m]={}\n",
    "                subthresh_connected[m] ={}\n",
    "\n",
    "                for chain in distmats[m]:\n",
    "                    subthresh, connected = define_contacts(distmats[m][chain] , angstrom_cutoff  )\n",
    "                    subthresh_thresh[m][chain] = subthresh\n",
    "                    subthresh_connected[m][chain] = connected\n",
    "            subthresh_thresh[m] = dict( zip ( subthresh_thresh[m].keys() , dask.compute( * list(subthresh_thresh[m].values() ))))\n",
    "            subthresh_connected[m] = dict( zip ( subthresh_connected[m].keys() , dask.compute( * list(subthresh_connected[m].values() ))))\n",
    "\n",
    "            if show_contacts == True:\n",
    "                for chain in subthresh_thresh[m]:\n",
    "                    subthresh = subthresh_thresh[m][chain]\n",
    "\n",
    "                    plt.figure(figsize=(10,10) )\n",
    "                    plt.title(m +'contact mat '+ ''.join(chain) )\n",
    "                    plt.scatter( np.nonzero(subthresh)[0] ,np.nonzero(subthresh)[1]  , marker= 'o' , alpha = .1 ,  s = 20)\n",
    "                    plt.show()\n",
    "\n",
    "                for chain in subthresh_connected[m]:\n",
    "                    connected = subthresh_connected[m][chain]\n",
    "                    plt.figure(figsize=(10,10) )\n",
    "                    plt.title('contact mat '+ ''.join(chain) )\n",
    "                    plt.scatter( np.nonzero(connected)[0] ,np.nonzero(connected)[1]  , marker= 'o' , alpha = .1 ,  s = 20)\n",
    "                    plt.show()\n",
    "\n",
    "    with open(modeldir + 'contactmaps' , 'wb') as connectout:\n",
    "        connectout.write(pickle.dumps((subthresh_thresh , subthresh_connected)))\n",
    "else:\n",
    "    with open(modeldir + 'contactmaps' , 'rb') as connectout:\n",
    "        subthresh_thresh , subthresh_connected = pickle.loads(connectout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minhash all event cols with more than #nthresh events\n",
    "#search top 10 for each col\n",
    "allpairs = {}\n",
    "\n",
    "print(subthresh_thresh)\n",
    "print(subthresh_connected)\n",
    "\n",
    "for code in subthresh_thresh:\n",
    "    if code not in allpairs:\n",
    "        allpairs[code] = {}\n",
    "    for chainpair in subthresh_thresh[code]:\n",
    "        pairs = np.nonzero(subthresh_thresh[code][chainpair])\n",
    "        pairset = set([ ( pairs[0][i] , pairs[1][i] ) for i in range(pairs[0].shape[0]) ])\n",
    "        \n",
    "        allpairs[code][chainpair] = pairset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2Single_sparse_graph_updown(tree):\n",
    "    N = len(tree.nodes())\n",
    "    #mimic the fitch algo\n",
    "    #propagate up and down in separate graphs\n",
    "    index_up = np.vstack([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    index_down = np.vstack([ [c.matrow, n.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    connectmat_up = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_down = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_up[index_up[:,0],index_up[:,1]] = 1 \n",
    "    connectmat_down[index_down[:,0],index_down[:,1]] = 1 \n",
    "    diag = [[n,n] for n in range(N)]\n",
    "    connectmat_diag=scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_diag[diag,diag] = 1 \n",
    "    ntime = np.array([ n.distance_from_root() for n in tree.nodes()])\n",
    "    mtime = np.amax(ntime)\n",
    "    ntime/=mtime\n",
    "    levels = np.array([ n.level() for n in tree.nodes() ] , dtype='double')\n",
    "    levels /= np.amax(levels)\n",
    "    Norm_nchild= np.array( [ len(n.child_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    mchild =np.amax(Norm_nchild)\n",
    "    Norm_nchild/=mchild \n",
    "    Norm_nsister= np.array( [ len(n.sister_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    msis =np.amax(Norm_nsister)\n",
    "    Norm_nsister/=msis    \n",
    "    template_features = np.stack([ntime ,  Norm_nchild , Norm_nsister ]).T    \n",
    "    return connectmat_up, connectmat_down, connectmat_diag, template_features\n",
    "\n",
    "def sparse2pairs(sparsemat, matrows = None):\n",
    "    if matrows :\n",
    "        sparsemat = sparsemat[matrows,:]\n",
    "        sparsemat = sparsemat[:,matrows]\n",
    "    sparsemat = scipy.sparse.find(sparsemat)\n",
    "    return np.vstack([sparsemat[0],sparsemat[1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree sector based aggregation\n",
    "#label sectors\n",
    "\n",
    "def process_node_down(node, sector = 0, breakpt = 10 , total = 0 ):\n",
    "    node.sector = sector\n",
    "    if sector == 0 :\n",
    "        global count\n",
    "        count = 0\n",
    "    total += len(node.child_nodes())\n",
    "    for i,child in enumerate(node.child_nodes()):\n",
    "        if total > breakpt:\n",
    "            if len(child.child_nodes())>0:\n",
    "                #new sector w new total\n",
    "                count+=1\n",
    "                process_node_down(child, count , total = 0 , breakpt = breakpt)\n",
    "            else:\n",
    "                #leaf\n",
    "                process_node_down(child, count , total = 0 , breakpt = breakpt)\n",
    "        else:\n",
    "            process_node_down(child, count , total = total , breakpt = breakpt)\n",
    "\n",
    "    \n",
    "def get_sectors(tree, breakpt = 10):\n",
    "    process_node_down( tree.seed_node , sector = 0, breakpt = breakpt )\n",
    "    row = [n.matrow for n in tree.nodes()]\n",
    "    col = [n.sector for n in tree.nodes()]\n",
    "    data = np.ones((len(row)))\n",
    "    sectormat = scipy.sparse.csc_matrix( (data,(row,col)) )\n",
    "    return sectormat\n",
    "\n",
    "for i,l in enumerate(tree.nodes()):\n",
    "    l.sum_lengths = None\n",
    "for i,l in enumerate(tree.leaf_nodes()):\n",
    "    l.sum_lengths = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_templates = False\n",
    "if compile_templates == True:\n",
    "    #check correlation of graphnet score w jaccard\n",
    "    connectmat_up, connectmat_down, connectmat_diag, template_features = tree2Single_sparse_graph_updown(tree)\n",
    "    print(connectmat_up.shape)\n",
    "    print(template_features.shape)\n",
    "\n",
    "    sectormat = get_sectors(tree, breakpt = 600 )\n",
    "    print(sectormat.shape)\n",
    "    with open('template_features.pkl' , 'wb') as templateout:\n",
    "        templateout.write(pickle.dumps([connectmat_up, connectmat_down, connectmat_diag, template_features]))\n",
    "\n",
    "    with open('sectormat.pkl' , 'wb' ) as sectorout:\n",
    "        sectorout.write(pickle.dumps(sectormat))\n",
    "else:\n",
    "    with open('template_features.pkl' , 'rb') as template_in:\n",
    "        connectmat_up, connectmat_down, connectmat_diag, template_features = pickle.loads(template_in.read())\n",
    "    with open('sectormat.pkl' , 'rb') as sector_in:\n",
    "        sectormat = pickle.loads(sector_in.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( template_features )\n",
    "print( sectormat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def ret_pytorch_sample(subfeatures, connect_up , connect_down , sub_diag ,  overview , overview_rev , intra , toss ):\n",
    "    data = HeteroData()\n",
    "\n",
    "    #add input data\n",
    "\n",
    "    data['phylonodes_up'].x = torch.tensor( subfeatures )\n",
    "    data['phylonodes_down'].x =torch.tensor( subfeatures )\n",
    "    data['sectornode'].x =torch.tensor(  np.zeros((1,1)) )\n",
    "    #up down fitch net\n",
    "    data['phylonodes_up', 'phylolink_up', 'phylonodes_up'].edge_index = torch.tensor(connect_up ,  dtype=torch.long )\n",
    "    data['phylonodes_down', 'phylolink_down', 'phylonodes_down'].edge_index = torch.tensor(connect_down ,  dtype=torch.long )             \n",
    "    data['phylonodes_up', 'phylolink_up_down', 'phylonodes_down'].edge_index = torch.tensor( sub_diag ,  dtype=torch.long )\n",
    "    data['phylonodes_down', 'phylolink_down_up', 'phylonodes_up'].edge_index = torch.tensor( sub_diag ,  dtype=torch.long )\n",
    "    #pooling connections\n",
    "    data['phylonodes_down', 'informs', 'sectornode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "    data['phylonodes_up', 'informs', 'sectornode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "\n",
    "    #pooling connections\n",
    "    data['sectornode',  'informs', 'phylonodes_down' ].edge_index = torch.tensor(overview_rev,  dtype=torch.long )\n",
    "    data['sectornode',  'informs', 'phylonodes_up'].edge_index = torch.tensor(overview_rev ,  dtype=torch.long )\n",
    "    #categories are intra or interprotein contacts or nothing\n",
    "    if intra == True:\n",
    "        cateforical = np.array([0,1])\n",
    "    else:\n",
    "        cateforical = np.array([1,0])\n",
    "    data['phylonodes_down'].y =torch.tensor( np.ones((subfeatures.shape[0],1) ) *toss ,  dtype=torch.long )\n",
    "    data['phylonodes_up'].y =torch.tensor( np.ones((subfeatures.shape[0],1)) * toss ,  dtype=torch.long )\n",
    "    #todo change to categorical\n",
    "    data['sectornode'].y =torch.tensor(  np.ones((1,1))*toss  ,  dtype=torch.long )\n",
    "    data = T.AddSelfLoops()(data)\n",
    "    data = T.NormalizeFeatures()(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_data_updown_transitions(intra ,pairs, AA_tensor, NT_tensor, sectormat, connectmat_up, connectmat_down, connectmat_diag, \n",
    " template_features , posi_percent = .5 , nsamples = 10 , min_nodes = 100,  q = None , iolock= None,  verbose = True, loop= True , sectors_chunk = True ):\n",
    "    #upward and downward connected phylo nodes\n",
    "    Nsectors = 0\n",
    "    Nnodes = 0\n",
    "    allcols =list( np.arange(NT_tensor.shape[1]) )\n",
    "    while True:\n",
    "        toss = scipy.stats.bernoulli.rvs(posi_percent, loc=0, size=1, random_state=None)\n",
    "        label = np.ones((1,1))*toss\n",
    "    \n",
    "        if verbose == True:\n",
    "            print('posi/nega',toss)\n",
    "            print('posi/nega',label)\n",
    "\n",
    "        if toss == 0:\n",
    "            col1 = random.choice(allcols)\n",
    "            col2 = col1\n",
    "            while col1 == col2 and (col1,col2) not in pairs:\n",
    "                col2 = random.choice(allcols)\n",
    "            labels = np.zeros((template_features.shape[0],))           \n",
    "        else:\n",
    "            #positive sample\n",
    "            pairtuple = random.choice(pairs)\n",
    "            col1 = pairtuple[0]\n",
    "            col2 = pairtuple[1]\n",
    "        if verbose == True:\n",
    "            print(col1, col2)\n",
    "        nodeAAfeatures = sparse.stack( [AA_tensor[:,col1,:] ,AA_tensor[:,col2,:] ] , axis = 2  ).reshape((AA_tensor.shape[0],-1)).to_scipy_sparse()\n",
    "        nt_cols = []\n",
    "        for pos in [0,1,2]:\n",
    "            nodeNTfeatures = sparse.stack( [NT_tensor[:,col1,pos,:] ,NT_tensor[:, col2,pos,:] ] , axis = 2)\n",
    "            nt_cols.append(nodeNTfeatures.reshape((nodeNTfeatures.shape[0],-1)).to_scipy_sparse() )\n",
    "        \n",
    "        nodefeatures = scipy.sparse.hstack([nodeAAfeatures,scipy.sparse.coo_matrix(template_features)]+nt_cols)\n",
    "        #slice the features into sectors and yield sectors\n",
    "        if sectors_chunk == True:\n",
    "            nodefeatures = scipy.sparse.lil_matrix(nodefeatures)\n",
    "            count = 0\n",
    "            sectors = list(np.arange(sectormat.shape[1])) \n",
    "            for sample in range(nsamples):\n",
    "                sector = random.choice(sectors)\n",
    "                rows = scipy.sparse.find(sectormat[:,sector])[0]\n",
    "                if rows.shape[0]> min_nodes:\n",
    "                    count +=1\n",
    "                    if verbose == True:\n",
    "                        print('rows',rows.shape)\n",
    "\n",
    "                #node features\n",
    "                subfeatures = nodefeatures[rows,:].todense()\n",
    "                #phylonode connections\n",
    "                sub_connect_up = connectmat_up[rows,:]\n",
    "                sub_connect_up = sub_connect_up[:,rows]\n",
    "                connect_up = sparse2pairs(sub_connect_up)\n",
    "                sub_connect_down = connectmat_down[rows,:]\n",
    "                sub_connect_down = sub_connect_down[:,rows]\n",
    "                connect_down = sparse2pairs(sub_connect_down)\n",
    "                sub_diag = connectmat_diag[rows,:]\n",
    "                sub_diag = sub_diag[:,rows]\n",
    "                sub_diag = sparse2pairs(sub_diag)\n",
    "                #aggregator node\n",
    "                overview = scipy.sparse.lil_matrix( (subfeatures.shape[0], 2 ) )\n",
    "                overview[:,0] = 1\n",
    "                overview_rev = sparse2pairs(overview.T)\n",
    "                overview = sparse2pairs(overview)\n",
    "                data = ret_pytorch_sample(subfeatures, connect_up , connect_down , sub_diag ,  overview , overview_rev , intra , toss )\n",
    "                if q:\n",
    "                    q.put(data)\n",
    "                else:\n",
    "                    yield data\n",
    "        else:\n",
    "            #yield the entire graph\n",
    "            #not working with sparse yet...\n",
    "            #node features\n",
    "            #change to torch sparse\n",
    "            values = nodefeatures.data\n",
    "            indices = np.vstack((nodefeatures.row, nodefeatures.col))\n",
    "\n",
    "            i = torch.LongTensor(indices)\n",
    "            v = torch.FloatTensor(values)\n",
    "            shape = nodefeatures.shape\n",
    "            subfeatures = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "            subfeatures = subfeatures.coalesce()\n",
    "            #phylonode connections\n",
    "            sub_connect_up = connectmat_up\n",
    "            connect_up = sparse2pairs(sub_connect_up)\n",
    "            sub_connect_down = connectmat_down\n",
    "            connect_down = sparse2pairs(sub_connect_down)\n",
    "            sub_diag = connectmat_diag\n",
    "            sub_diag = sparse2pairs(sub_diag)\n",
    "            #aggregator node\n",
    "            overview = scipy.sparse.lil_matrix( (subfeatures.shape[0], 2 ) )\n",
    "            overview[:,0] = 1\n",
    "            overview_rev = sparse2pairs(overview.T)\n",
    "            overview = sparse2pairs(overview)\n",
    "            data = ret_pytorch_sample(subfeatures, connect_up , connect_down , sub_diag ,  overview , overview_rev , intra , toss )\n",
    "            if q:\n",
    "                q.put(data)\n",
    "            else:\n",
    "                yield data\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create reduced alphabet mapping\n",
    "\n",
    "\n",
    "#AA transitions\n",
    "murphy12 = [('L','V','I','M'), ('C'), ('A'), ('G'), ('S','T'), ('P'), ('F','Y'), ('W'), ('E','Q'), ('D','N'), ('K','R'), ('H') ]\n",
    "murphy12 = { c:i for i,cset in enumerate(murphy12) for c in cset   }\n",
    "print('murphy12',murphy12)\n",
    "\n",
    "ProteinAlphabet = [ 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y' ]\n",
    "allowed_AA_transitions = [ c1+c2 for c1 in ProteinAlphabet for c2 in ProteinAlphabet  if c1!= c2]\n",
    "\n",
    "print(len(set(allowed_AA_transitions)))\n",
    "new_transitions = [ (murphy12[c1],murphy12[c2]) for c1 in ProteinAlphabet for c2 in ProteinAlphabet  if c1!= c2  ]\n",
    "\n",
    "\n",
    "print(len(set(new_transitions)))\n",
    "\n",
    "print(allowed_AA_transitions[0:100] , '...etc...')\n",
    "transitiondict_AA = {  c : i  for i,c in enumerate( allowed_AA_transitions )  }\n",
    "\n",
    "\n",
    "rev_transitiondict_AA = dict( zip(transitiondict_AA.values(), transitiondict_AA.keys()))\n",
    "\n",
    "\n",
    "transitionmap = \n",
    "#\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    qseqid                                             sseqid   qlen  slen  \\\n",
      "0      EPI  /scratch/dmoi/datasets/covid_data/structs/7DZW...  29809  1121   \n",
      "1      EPI  /scratch/dmoi/datasets/covid_data/structs/7DZW...  29809  1121   \n",
      "2      EPI  /scratch/dmoi/datasets/covid_data/structs/7DZW...  29809  1121   \n",
      "3      EPI  /scratch/dmoi/datasets/covid_data/structs/7DZY...  29809  1121   \n",
      "4      EPI  /scratch/dmoi/datasets/covid_data/structs/7DZY...  29809  1121   \n",
      "..     ...                                                ...    ...   ...   \n",
      "495    EPI  /scratch/dmoi/datasets/covid_data/structs/7BYR...  29809   998   \n",
      "496    EPI  /scratch/dmoi/datasets/covid_data/structs/7L3N...  29809  1001   \n",
      "497    EPI  /scratch/dmoi/datasets/covid_data/structs/7LYN...  29809   996   \n",
      "498    EPI  /scratch/dmoi/datasets/covid_data/structs/7LYQ...  29809   996   \n",
      "499    EPI  /scratch/dmoi/datasets/covid_data/structs/7LYP...  29809   996   \n",
      "\n",
      "     qstart   qend  qframe  evalue struct chain  \n",
      "0     21604  24966       1     0.0   7DZW     C  \n",
      "1     21604  24966       1     0.0   7DZW     B  \n",
      "2     21604  24966       1     0.0   7DZW     A  \n",
      "3     21604  24966       1     0.0   7DZY     C  \n",
      "4     21604  24966       1     0.0   7DZY     B  \n",
      "..      ...    ...     ...     ...    ...   ...  \n",
      "495   21604  24963       1     0.0   7BYR     B  \n",
      "496   21604  24963       1     0.0   7L3N     B  \n",
      "497   21604  24966       1     0.0   7LYN     B  \n",
      "498   21604  24966       1     0.0   7LYQ     B  \n",
      "499   21604  24966       1     0.0   7LYP     B  \n",
      "\n",
      "[500 rows x 10 columns]\n",
      "<COO: shape=(997916, 29809, 12), dtype=float64, nnz=2075009, fill_value=0.0>\n",
      "<COO: shape=(997916, 29809, 380), dtype=float64, nnz=1338566, fill_value=0.0>\n",
      "21604\n",
      "done AA\n",
      "done NT\n",
      "21604\n",
      "21604\n",
      "21604\n",
      "21604\n",
      "21604\n",
      "21604\n",
      "21604\n",
      "21604\n",
      "21565\n",
      "done AA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/numba/core/serialize.py\", line 29, in _numba_unpickle\n",
      "    def _numba_unpickle(address, bytedata, hashed):\n",
      "KeyboardInterrupt\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/7191046/ipykernel_1262520/3417993238.py\", line 35, in <module>\n",
      "    structmats[row.struct][row.chain ] = retcodons(AA_mutation, nucleotide_mutation  , row.qstart , row.qend)\n",
      "  File \"/tmp/7191046/ipykernel_1262520/3417993238.py\", line 18, in retcodons\n",
      "    NTmat_sub = sparse.stack([  sparse.stack( [ NTmat[:, codon + frame , : ] for frame in [0,1,2] ] , axis = 1 )  for codon in range(qstart-1, qend-1 , 3 ) ] , axis = 1)\n",
      "  File \"/tmp/7191046/ipykernel_1262520/3417993238.py\", line 18, in <listcomp>\n",
      "    NTmat_sub = sparse.stack([  sparse.stack( [ NTmat[:, codon + frame , : ] for frame in [0,1,2] ] , axis = 1 )  for codon in range(qstart-1, qend-1 , 3 ) ] , axis = 1)\n",
      "  File \"/tmp/7191046/ipykernel_1262520/3417993238.py\", line 18, in <listcomp>\n",
      "    NTmat_sub = sparse.stack([  sparse.stack( [ NTmat[:, codon + frame , : ] for frame in [0,1,2] ] , axis = 1 )  for codon in range(qstart-1, qend-1 , 3 ) ] , axis = 1)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/sparse/_coo/indexing.py\", line 77, in getitem\n",
      "    mask, adv_idx = _mask(x.coords, index, x.shape)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/sparse/_coo/indexing.py\", line 180, in _mask\n",
      "    mask, is_slice = _compute_mask(coords, _ind_ar_from_indices(indices))\n",
      "SystemError: CPUDispatcher(<function _compute_mask at 0x7f408f17f1f0>) returned a result with an error set\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'SystemError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/inspect.py\", line 745, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/IPython/utils/shimmodule.py\", line 92, in __getattr__\n",
      "    return import_item(name)\n",
      "  File \"/users/dmoi/.local/lib/python3.8/site-packages/IPython/utils/importstring.py\", line 31, in import_item\n",
      "    module = __import__(package, fromlist=[obj])\n",
      "  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 914, in _find_spec\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numba/core/serialize.py\u001b[0m in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_numba_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytedata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \"\"\"Used by `numba_unpickle` from _helperlib.c\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/7191046/ipykernel_1262520/3417993238.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqstart\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstart_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mstructmats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretcodons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAA_mutation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnucleotide_mutation\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqstart\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mstart_stop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqstart\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructmats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstruct\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/7191046/ipykernel_1262520/3417993238.py\u001b[0m in \u001b[0;36mretcodons\u001b[0;34m(AAmat, NTmat, qstart, qend, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done AA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mNTmat_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m  \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mNTmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mcodon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqstart\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/7191046/ipykernel_1262520/3417993238.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done AA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mNTmat_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m  \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mNTmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mcodon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqstart\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/7191046/ipykernel_1262520/3417993238.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done AA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mNTmat_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m  \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mNTmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mcodon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqstart\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sparse/_coo/indexing.py\u001b[0m in \u001b[0;36mgetitem\u001b[0;34m(x, index)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Get the mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sparse/_coo/indexing.py\u001b[0m in \u001b[0;36m_mask\u001b[0;34m(coords, indices, shape)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ind_ar_from_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: CPUDispatcher(<function _compute_mask at 0x7f408f17f1f0>) returned a result with an error set",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2063\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SystemError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2064\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2067\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import sparse\n",
    "\n",
    "#reduce alphabet to reduce AA transition dimensionality\n",
    "def restrictAA_transitions(AAmat, mapping, transitions):\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###rewrite codon compilation for sparse\n",
    "def retcodons( AAmat , NTmat, qstart, qend, verbose = True ):\n",
    "    #aa mutations for each pos\n",
    "    AAmat_sub = sparse.stack(  [ AAmat[ : , codon:codon+2 , : ].sum(axis = 1) for codon in range(qstart-1, qend-1 , 3 )  ] , axis = 1 )    \n",
    "    #add the frames for each in a stack \n",
    "    if verbose == True:\n",
    "        print('done AA')\n",
    "    NTmat_sub = sparse.stack([  sparse.stack( [ NTmat[:, codon + frame , : ] for frame in [0,1,2] ] , axis = 1 )  for codon in range(qstart-1, qend-1 , 3 ) ] , axis = 1)\n",
    "    if verbose == True:\n",
    "        print('done NT')\n",
    "    return AAmat_sub , NTmat_sub\n",
    "\n",
    "print(annotation)\n",
    "structmats = {}\n",
    "\n",
    "start_stop ={}\n",
    "print(nucleotide_mutation)\n",
    "print(AA_mutation)\n",
    "\n",
    "for i,row  in annotation.iterrows():\n",
    "    print(row.qstart)\n",
    "    if row.struct not in structmats:\n",
    "        structmats[row.struct]={}    \n",
    "    if (row.qstart , row.qend) not in start_stop:\n",
    "        structmats[row.struct][row.chain ] = retcodons(AA_mutation, nucleotide_mutation  , row.qstart , row.qend)\n",
    "        start_stop[(row.qstart , row.qend)] = structmats[row.struct ][ row.chain ]\n",
    "    else:\n",
    "        structmats[row.struct][row.chain ] = start_stop[(row.qstart , row.qend)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subthresh_connected['7DZW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv , SAGEConv, Linear , ResGatedGraphConv , GATv2Conv , TransformerConv , MFConv , FiLMConv \n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#without sectornode\n",
    "class HeteroGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.lins =  torch.nn.ModuleList()\n",
    "        self.lins2 =  torch.nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('phylonodes_up', 'phylolink_up', 'phylonodes_up'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down', 'phylonodes_down'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down_up', 'phylonodes_up'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'phylolink_up_down', 'phylonodes_down'):TransformerConv((-1,-1),  int( hidden_channels) ), \n",
    "                ('phylonodes_down', 'informs', 'sectornode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'informs', 'sectornode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "            } , aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "            for vectype in  ['phylonodes_up', 'phylonodes_down' , 'sectornode'  ]:\n",
    "                lin1 = Linear(-1 , int( hidden_channels))\n",
    "                self.lins.append( lin1 )\n",
    "                \n",
    "            print( 'hidden units' , int( hidden_channels) )\n",
    "            print( 'layer' , i )\n",
    "\n",
    "        for vectype in ['phylonodes_up', 'phylonodes_down' , 'sectornode' ]:\n",
    "            lin2 = Linear(-1 , out_channels)\n",
    "            self.lins2.append( lin2 ) \n",
    "\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        lins = iter(self.lins)\n",
    "        for i,conv in enumerate(self.convs):\n",
    "            x_dict = conv(x_dict , edge_index_dict)\n",
    "            x_dict = {key: F.dropout(x , p = .75 , training = self.training ) for key, x in x_dict.items()}\n",
    "\n",
    "            for key, x in x_dict.items():\n",
    "                x_dict[key] = next(lins)(x)\n",
    "        lins2 = iter(self.lins2)\n",
    "        \n",
    "        for key, x in x_dict.items():\n",
    "            x_dict[key] =  next(lins2)(x)\n",
    "        \n",
    "        return {key: F.tanh(x) for key, x in x_dict.items()}\n",
    "        #return x_dict\n",
    "\n",
    "model = HeteroGCN(hidden_channels=100 , out_channels=1, num_layers=5)\n",
    "model = model.double()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chains in enumerate(subthresh_connected['7DZW']):\n",
    "    AA_tensor , NT_tensor = structmats['7DZW'][chains]\n",
    "    print(AA_tensor.shape)\n",
    "    print(NT_tensor.shape)\n",
    "    intra = len(chains)>1\n",
    "    pairs = list(allpairs['7DZW'][chains])\n",
    "    contact_gen = create_data_updown_transitions(intra ,pairs, AA_tensor, NT_tensor, sectormat, connectmat_up, connectmat_down, connectmat_diag, template_features , posi_percent = .5 ,  q = None , iolock= None,  verbose = True, loop= True  )\n",
    "    data = next(contact_gen)\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testgen = False\n",
    "traingen = False\n",
    "reload = True\n",
    "\n",
    "def gen_dataset(data_name , structmats , allpairs ,  reload = True  ):\n",
    "    pairsample = 100\n",
    "    print('writing dataset to' , data_name)\n",
    "    if reload == True:\n",
    "        with open(data_name, 'rb' ) as datain:\n",
    "            samples =pickle.loads( datain.read() )\n",
    "    else:\n",
    "        samples = []\n",
    "    for struct in subthresh_connected:\n",
    "        for i,chains in enumerate(subthresh_connected[struct]):\n",
    "            AA_tensor , NT_tensor = structmats[struct][chains]\n",
    "            intra = len(chains)>1\n",
    "            pairs = list(allpairs[struct][chains])\n",
    "            contact_gen = create_data_updown_transitions(intra ,pairs, AA_tensor, NT_tensor, sectormat, connectmat_up, connectmat_down, \n",
    "                    connectmat_diag, template_features , posi_percent = .5 ,  q = None , iolock= None,  verbose = False, loop= True  )\n",
    "            for data in contact_gen:\n",
    "                samples.append(data)\n",
    "                if len(samples) < 10:\n",
    "                    print(data)\n",
    "                if len(samples)%100==0 and len(samples)> 0:\n",
    "                    print(len(samples))\n",
    "                    with open(data_name, 'wb' ) as datain:\n",
    "                        datain.write(pickle.dumps(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if traingen == True:\n",
    "    trainfile = alnfile + 'train.pkl'\n",
    "    gen_dataset(trainfile , structmats , allpairs ,  reload  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testgen == True:\n",
    "    testfile = alnfile + 'test.pkl'\n",
    "    gen_dataset(testfile , structmats , allpairs ,  reload )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    data = data.to(device)\n",
    "    out = model(data.x_dict , data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(alnfile + 'train.pkl', 'rb') as trainin:\n",
    "    trainingdata = pickle.loads(trainin.read())\n",
    "\n",
    "#with open(alnfile + 'test.pkl') as trainin:\n",
    "#    testingdata = pickle.loads(trainin.read()\n",
    "\n",
    "trainloader = DataLoader( trainingdata , batch_size = 10 , shuffle=True)\n",
    "#testloader = DataLoader(testingdata , batch_size = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "lastauc = 0\n",
    "epochs = 1000\n",
    "#iterate over structs\n",
    "model = model.to(device)\n",
    "\n",
    "def calc_metrics(truths,preds,label = ''):\n",
    "    truth = np.hstack(truths)\n",
    "    predy = np.hstack(preds)\n",
    "    print(truth.shape, predy.shape)\n",
    "    fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "    auc_s = auc(fpr, tpr)\n",
    "    print('train node auc sector',auc_s)\n",
    "    truth_n = np.hstack(truths_n)\n",
    "    predy_n = np.hstack(preds_n)\n",
    "    fpr, tpr, _ = roc_curve(  truth_n  ,predy_n )\n",
    "    auc_n = auc(fpr, tpr)\n",
    "    print('train node auc',auc_n)\n",
    "        \n",
    "        \n",
    "with warnings.catch_warnings():\n",
    "    model.train()\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    #iterate over chain pairs\n",
    "    #create a chunk generator for a chain pair\n",
    "    #iterate over graph chunks\n",
    "    truths = []\n",
    "    preds = []\n",
    "    truths_n = []\n",
    "    preds_n = []\n",
    "\n",
    "    losses1 =[]\n",
    "    losses2 = []\n",
    "    losses3 = []\n",
    "    for e in range(epochs ) :\n",
    "        trainloader = DataLoader( trainingdata , batch_size = 10 , shuffle=True)\n",
    "\n",
    "        for i,data in enumerate(trainloader):\n",
    "            data = data.to(device)            \n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            optimizer.zero_grad()\n",
    "            loss1 =  F.mse_loss(out['phylonodes_up'], data['phylonodes_up'].y.double())\n",
    "            loss2 =  F.mse_loss(out['phylonodes_down'], data['phylonodes_down'].y.double())\n",
    "            loss3 =  F.mse_loss(out['sectornode'], data['sectornode'].y.double())\n",
    "            loss = loss1 + loss2+ loss3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses3.append(float(loss3.to('cpu')))\n",
    "            losses2.append(float(loss2.to('cpu')))\n",
    "            losses1.append(float(loss1.to('cpu')))\n",
    "\n",
    "            truth = data['sectornode']['y'][:].to('cpu').detach().numpy()\n",
    "            predy =  out['sectornode'][:].to('cpu').detach().numpy()\n",
    "            #truth_n = data['phylonodes_down']['y'][:,0].to('cpu').detach().numpy()\n",
    "            #pred_n =  out['phylonodes_down'][:,0].to('cpu').detach().numpy()\n",
    "            #truths_n.append(truth_n)\n",
    "            #preds_n.append(pred_n)\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)            \n",
    "            if i % 100 == 0:\n",
    "                print('losses', np.mean(losses1), np.mean(losses2), np.mean(losses3) )\n",
    "                print(i)\n",
    "                losses1 =[]\n",
    "                losses2 = []\n",
    "                losses3 = []\n",
    "\n",
    "        #calc_metrics(truths,preds,label = 'sectornode')\n",
    "            #calc_metrics(truths_n,preds_n,label = 'phylonodes')\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        truths = []\n",
    "        preds = []\n",
    "        losses3 = []\n",
    "        for i,data in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss3 =  F.smooth_l1_loss(out['godnode'].double(), data['godnode'].y.double())\n",
    "            loss = loss3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses3.append(float(loss3.to('cpu')))\n",
    "            truth = data['sectornode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  out['sectornode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_n = auc(fpr, tpr)\n",
    "        print('losses', np.mean(losses1), np.mean(losses2), np.mean(losses3) )\n",
    "        print('train auc', auc_n)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        \n",
    "        truths = []\n",
    "        preds = []\n",
    "\n",
    "        truths_n = []\n",
    "        preds_n = []\n",
    "        \n",
    "        \n",
    "        for i,testdata in enumerate(testloader):\n",
    "            testdata = testdata.to(device)\n",
    "            pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "            truth = testdata['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  pred['godnode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "            truth_n = testdata['phylonodes_down']['y'][:,0].to('cpu').detach().numpy()\n",
    "            pred_n =  pred['phylonodes_down'][:,0].to('cpu').detach().numpy()\n",
    "\n",
    "            truths_n.append(truth_n)\n",
    "            preds_n.append(pred_n)\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_gn = auc(fpr, tpr)\n",
    "        print('test auc',auc_gn)\n",
    "        truth_n = np.hstack(truths_n)\n",
    "        predy_n = np.hstack(preds_n)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(  truth_n  ,predy_n )\n",
    "        auc_n = auc(fpr, tpr)        \n",
    "        print('test node auc',auc_n)\n",
    "        if auc_gn > lastauc:\n",
    "            lastauc = auc_gn\n",
    "            print('saving')\n",
    "            torch.save(model, './phylographnet_job_final50.torch')\n",
    "            print('done')\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #load subgraphs\n",
    "        #train_input_nodes = #######\n",
    "        #train_loader = HGTLoader(data , num_samples=[1024] * 4, shuffle=True, input_nodes=train_input_nodes, **kwargs)\n",
    "        #for trainsample in train_loader:\n",
    "            #train here\n",
    "            \n",
    "            \n",
    "            \n",
    "    #for labels in [ aglo_l]:\n",
    "    #    l,c = np.unique(labels, return_counts= True)\n",
    "        print(i)\n",
    "        print(chain)\n",
    "        #categorical-> direct, indirect or no contact\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if i == 0 :\n",
    "            total_thresh = subthresh_thresh['7DZW'][chain]\n",
    "            total_connect = subthresh_connected['7DZW'][chain]\n",
    "\n",
    "            #green, oranges, reds = struct_hits( labels, threshmat , connectmat , struct = '7DZW' , chain=chain , l=l , c=c , verbose = False , radius = 10)\n",
    "        else:\n",
    "            total_thresh += subthresh_thresh['7DZW'][chain]\n",
    "            total_connect += subthresh_connected['7DZW'][chain]\n",
    "            \n",
    "        # find nonzero\n",
    "        # zeroed entries are the complement\n",
    "        #sample each fraction randomly\n",
    "        ### testing loop ####\n",
    "        if epoch % 100 == 0:\n",
    "            ROC_curve_single(y_test, y_pred_grd)\n",
    "        \n",
    "        ### save based on performance\n",
    "        \n",
    "        \n",
    "            \n",
    "proj_greens, proj_oranges, proj_reds = struct_hits_filter( AATF, labels, total_thresh , total_connect , struct = '7DZW' , chain=chain , l=l , c=c , verbose = False , radius = 10 )\n",
    "#proj_greens, proj_oranges, proj_reds = struct_hits(  labels, total_thresh , total_connect , struct = '7DZW' , chain=chain , l=l , c=c , verbose = False , radius = 10 )\n",
    "\n",
    "print(proj_greens, proj_oranges, proj_reds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for lsh encoding\n",
    "def retcodon_mats(annotation , AAmat , plt = False):\n",
    "    positions = set([])\n",
    "    AAmat = scipy.sparse.csc_matrix(AAmat)\n",
    "    print('converted')\n",
    "    \n",
    "    slices = []\n",
    "    for i,r in annotation.iterrows():\n",
    "        if (r.qstart-1, r.qend-1) not in slices:        \n",
    "            slices.append((r.qstart-1, r.qend-1))\n",
    "    count = 0\n",
    "    for start,end in slices:\n",
    "        codonmat = np.zeros(( AAmat.shape[0] , int((end - start + 10 ) /3) ) )\n",
    "        for j,codon in enumerate(range(start-1, end-1 , 3 )):\n",
    "            cols = np.sum( AAmat[:, codon:codon+2] , axis = 1 )\n",
    "            codonmat[:,count] = cols.ravel()\n",
    "            count+=1\n",
    "            if count%500 == 0 and count >0:\n",
    "                print(count/codonmat.shape[1])\n",
    "        yield codonmat \n",
    "        \n",
    "        if plt == True:\n",
    "            print(np.sum(codonmat))\n",
    "            plt.figure(figsize=(20,20))\n",
    "            plt.spy(codonmat, markersize= .5)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHashLSHForest, WeightedMinHash\n",
    "#Ntaxa known. avoid allvsall\n",
    "from datasketch import WeightedMinHashGenerator\n",
    "\n",
    "# WeightedMinHashGenerator requires dimension as the first argument\n",
    "wmg = WeightedMinHashGenerator(connectmat.shape[0] , sample_size=512 , seed=0)\n",
    "\n",
    "#parallelise the sig generation for columns\n",
    "#compile db\n",
    "#iteratively search to make a net\n",
    "\n",
    "#verify matches with CGN\n",
    "#compute bloom filters for protein pairs\n",
    "def blur_cols( col , niter = 10 , blurmat ):\n",
    "    for i in range(niter):\n",
    "        col = blurmat.dot(col)\n",
    "    #you need a redis server running on your cluster for this to work. change your ip, port and db number accordingly\n",
    "    return col\n",
    "\n",
    "def hash_cols( col , wmg, nhashes = 512 ):\n",
    "    mh = wmg.minhash(col)\n",
    "    #you need a redis server running on your cluster for this to work. change your ip, port and db number accordingly\n",
    "    return mh.digest()\n",
    "\n",
    "#transform dask array\n",
    "AAmat_dask = da.from_array(AAmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur each col w graph laplacian\n",
    "AAmat_dask = AAmat_dask.blur_cols(func1d, axis=1 )\n",
    "\n",
    "#return the hash sig of each col\n",
    "hashsigs = AAmat_dask.hash_cols(func1d, axis=1  )\n",
    "\n",
    "\n",
    "#input each sig into the lsh forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
