{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import wget\n",
    "import requests\n",
    "import glob\n",
    "import time\n",
    "import dask\n",
    "import h5py\n",
    "\n",
    "from Bio import SeqIO\n",
    "import pickle\n",
    "import sys\n",
    "sys.setrecursionlimit( 10 **9 )\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics import roc_curve , precision_recall_curve , auc\n",
    "\n",
    "import scipy\n",
    "import copy\n",
    "from numpy import linalg as LA\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "import itertools\n",
    "import dendropy\n",
    "import seaborn as sns\n",
    "\n",
    "overwrite = False\n",
    "jk_iterations = 5\n",
    "os.environ['MKL_ENABLE_INSTRUCTIONS'] = 'AVX2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#create graphs on the fly to represent pairs of profiles\n",
    "device = torch.device('cuda' )\n",
    "#device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import  to_hetero\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import HeteroData ,InMemoryDataset\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = '/work/FAC/FBM/DBC/cdessim2/default/dmoi/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_02021-09-27T12:05:34.021732small_test0.001_BS_coevmats.pkl', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_02021-09-27T12:05:34.021732small_test0.1_BS_coevmats.pkl', '/work/FAC/FBM/DBC/cdessim2/default/dmoi/datasets/covid_data/msa_0730/msa_0730.fasta_02021-09-27T12:05:34.021732small_test0.01_BS_coevmats.pkl']\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob( filedir + 'datasets/covid_data/msa_0730/msa_0730.fasta*02021-09-27T12:05:34*pkl')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treefile = '../validation_data/covid19/gisaid_hcov-2020_08_25.QC.NSoutlier.filter.deMaiomask.aln.EPIID.treefile'\n",
    "#alnfile = '../validation_data/covid19/gisaid_hcov-2020_08_25.QC.NSoutlier.filter.deMaiomask.EPIID.aln'\n",
    "alnfile = filedir + 'datasets/covid_data/msa_0730/msa_0730.fasta'\n",
    "treefile = filedir + 'datasets/covid_data/msa_0730/global.tree'\n",
    "\n",
    "alnh5 = alnfile+'.h5'\n",
    "#ts = '2021-08-08T11:16:34.358764'\n",
    "ts = '2021-08-08T14:37:59.736512'\n",
    "events = alnfile+'*'+ts+'*'\n",
    "eventmats = glob.glob(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = dendropy.Tree.get( path=treefile, schema='newick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the internal nodes for the fitch algo\n",
    "for i,l in enumerate(tree.nodes()):\n",
    "    l.event = {}\n",
    "    l.scores = {}\n",
    "    l.symbols = None\n",
    "    l.char= None\n",
    "    l.matrow = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import colour\n",
    "#quick and dirty check of trees\n",
    "\n",
    "def tree_circle(node, start , sliver  ):\n",
    "    if start == True :\n",
    "        global count\n",
    "        count = 0\n",
    "    for i,child in enumerate(node.child_nodes()):\n",
    "        if child.is_leaf() == True:\n",
    "            child.radians = count*sliver\n",
    "            count+=1\n",
    "        if child.radians is None and child.is_leaf() == False :\n",
    "            tree_circle( child , start = False , sliver = sliver  )\n",
    "    radians = np.mean([ child.radians if child.radians else 0 for child in node.child_nodes() ])\n",
    "    if node.radians is None:\n",
    "        node.radians = radians\n",
    "\n",
    "def phylograph(treein,labels , title = None):\n",
    "    N = len(treein.nodes())\n",
    "    tree = copy.deepcopy(treein)\n",
    "    pdm = tree.phylogenetic_distance_matrix()\n",
    "    sliver = 2*np.pi / len(tree.leaf_nodes())\n",
    "    \n",
    "    root = tree.seed_node\n",
    "    radii = [ n.distance_from_root() for n in tree.nodes()]\n",
    "    for n in tree.nodes():\n",
    "        n.radians = None\n",
    "    tree_circle(tree.seed_node, start=True , sliver = sliver)\n",
    "    thetas = [n.radians for n in tree.nodes() ]\n",
    "    pos = { i: [ np.sin(thetas[i])*radii[i] , np.cos(thetas[i])*radii[i]] for i in range(len(thetas)) }\n",
    "    index = np.vstack([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    connectmat = np.zeros(( N ,  N ) )\n",
    "    connectmat[index[:,0],index[:,1]] = 1 \n",
    "    connectmat += connectmat.T\n",
    "    G = nx.from_numpy_array(connectmat)\n",
    "    red = colour.Color(\"red\")\n",
    "    blue = colour.Color(\"blue\")\n",
    "    crange = dict( zip ( list(set(labels)),  [ c.hex_l for c in list(red.range_to(blue, len(set(labels)) ) ) ] ) )\n",
    "    colors = [crange[n] for n in labels ]\n",
    "    #color according to downstream node\n",
    "    edge_colors= [crange[labels[v]] for u,v in G.edges() ]\n",
    "    #node size inversly proportional to number in graph\n",
    "    plt.figure(figsize= (20,20) )\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    nx.draw_networkx(G, pos = pos,  node_color = colors , node_size = 5, width = .5 , edge_color = edge_colors , with_labels=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(y_data, label = None):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    for l in y_data:\n",
    "        print(l)\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        fpr, tpr, _ = roc_curve(   y_pred_grd ,y_test)\n",
    "        plt.plot(fpr, tpr, label=l + 'auc'+ str(auc(fpr, tpr) ))\n",
    "        plt.xlabel('False positive rate')\n",
    "        plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_ROC.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    for l in y_data:\n",
    "        y_pred_grd = y_data[l]['Ytrue']\n",
    "        y_test = y_data[l]['Ypred']\n",
    "        precision, recall, thresholds = precision_recall_curve( y_pred_grd, y_test)\n",
    "        plt.plot( recall, precision , label= l )\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlabel('Recall')\n",
    "    \n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    if label:\n",
    "        plt.savefig( label +'_PR.svg' )\n",
    "    plt.show()\n",
    "    \n",
    "def ROC_curve_single(y_test, y_pred_grd):\n",
    "    fpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_grd)\n",
    "    plt.plot(fpr, tpr, label='single')\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(  y_test , y_pred_grd)\n",
    "    plt.plot(precision, recall , label='single')\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "\n",
    "    plt.title('PR curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( eventmats )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists( alnfile + '_IDs.pkl'):\n",
    "    with open( alnfile + '_IDs.pkl' , 'rb') as idxin:\n",
    "        IDindex = pickle.loads(idxin.read())\n",
    "    IDs = dict(zip( IDindex.values() , IDindex.keys() ) )\n",
    "else:\n",
    "    \n",
    "    msa = SeqIO.parse(alnfile , format = 'fasta')\n",
    "    def clipID(ID):\n",
    "        return ''.join( [ s +'|' for s in str(ID).split('|')[:-1] ])[:-1].replace('_',' ') \n",
    "    IDs = {i:rec.id for i,rec in enumerate(msa)}\n",
    "    IDindex = dict(zip( IDs.values() , IDs.keys() ) )\n",
    "    print( [(t,IDindex[t]) for t in list(IDindex.keys())[0:10]] )\n",
    "    with open( alnfile + '_IDs.pkl' , 'wb') as idxout:\n",
    "        idxout.write(pickle.dumps(IDindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(IDindex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = '7DZW, 6vxx,6zxn,5x58,6cs0,6sc1,6nb6, 2jw8, 2xab, 4aud1, 1yo4, 2acf, 2wct, 3vc8, 2gt7, 3ee7, 2g9t, 3ee7, 2g9t, 6jyt, 1ysy, 6nur, 2g9t, 5c8u, 2g9t, 2xyq, 4mm3, 6cs2, 6acg, 6acj, 6ack, 2dd8, 2ghw, 6nb6, 6nb7'\n",
    "#models = '6vxx,6zxn'\n",
    "models = models.split(',')\n",
    "\n",
    "from Bio.PDB import *\n",
    "dl_url = 'http://files.rcsb.org/download/'\n",
    "dl_url_err = 'http://files.rcsb.org/download/'\n",
    "\n",
    "structs = {}\n",
    "already = glob.glob( modeldir+'/*.pdb' )\n",
    "print(already)\n",
    "#pull complexes\n",
    "for m in models:\n",
    "    structfile = modeldir+m.upper().strip()+'.pdb'\n",
    "    if structfile not in already:\n",
    "        print(m)\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            wget.download(url = dl_url + m.strip() +'.pdb' , out =structfile)\n",
    "            structs[m] = structfile\n",
    "        except:\n",
    "            try:\n",
    "                wget.download(url = dl_url + m.strip() +'.pdb' , out =structfile)\n",
    "                structs[m] = structfile\n",
    "            except:\n",
    "                print('err', m )\n",
    "    else:\n",
    "        structs[m.strip()] = structfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldir = filedir+'datasets/covid_data/structs/'\n",
    "modelfiles = modeldir + '*.pdb'\n",
    "modelfiles = glob.glob( modelfiles )\n",
    "print(modelfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import MMCIFParser, PDBParser , PDBIO\n",
    "from Bio.PDB.mmcifio import MMCIFIO\n",
    "parser = PDBParser()\n",
    "import warnings\n",
    "from Bio import SeqUtils\n",
    "\n",
    "###compile all pdbs to fasta \n",
    "converter = SeqUtils.IUPACData.protein_letters_3to1\n",
    "converter = { res.upper():converter[res] for res in converter}\n",
    "chain_sequences={}\n",
    "for model in modelfiles:\n",
    "    print(model)\n",
    "    with warnings.catch_warnings():\n",
    "        try:\n",
    "            m = model.split('/')[-1].replace('pdb' , '') \n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            structure = parser.get_structure(m, model)\n",
    "            io=PDBIO()\n",
    "            io.set_structure(structure)\n",
    "            chain_sequences[model]= { c.id : ''.join(  [ converter[ r.get_resname()] for r in c.get_residues() if 'CA' in r ] ) for c in structure.get_chains() }\n",
    "            chain_sequences[model]={ c: chain_sequences[model][c] for c in chain_sequences[model] if len(chain_sequences[model][c])> 0 }\n",
    "        except:\n",
    "            print('err', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( modeldir + 'struct_chains.fasta', 'w') as fastout:\n",
    "    for model in chain_sequences:\n",
    "        for c in chain_sequences[model]:\n",
    "            fastout.write( '> '+model+ ':'+c + '\\n' + chain_sequences[model][c] + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask.distributed import fire_and_forget\n",
    "from dask.distributed import Client, Variable , Queue , Lock ,LocalCluster\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import  utils_perf\n",
    "import dask\n",
    "import dask.bag as db\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "from dask import delayed, compute\n",
    "\n",
    "distributed_computation = False\n",
    "\n",
    "print('flashing up a dask cluster')\n",
    "if distributed_computation == True:\n",
    "    \n",
    "    NCORE = 10\n",
    "    njobs = 20\n",
    "    print('deploying cluster')\n",
    "    cluster = SLURMCluster(\n",
    "        walltime='24:00:00',\n",
    "        n_workers = NCORE,\n",
    "        cores=NCORE,\n",
    "        processes = NCORE,\n",
    "        interface='ib0',\n",
    "        memory=\"150GB\",\n",
    "        env_extra=[\n",
    "        'source /scratch/dmoi/miniconda/etc/profile.d/conda.sh',\n",
    "        'conda activate ML'\n",
    "        ],\n",
    "        scheduler_options={'interface': 'ens2f0' }\n",
    "    )\n",
    "    print(cluster.job_script())\n",
    "    #cluster.adapt(minimum=10, maximum=30)\n",
    "    cluster.scale(jobs=20)\n",
    "    time.sleep(5)\n",
    "\n",
    "    print(cluster)\n",
    "    print(cluster.dashboard_link)\n",
    "    client = Client(cluster , timeout='450s' , set_as_default=True )\n",
    "else:\n",
    "    if __name__ == '__main__':\n",
    "        NCORE = 5\n",
    "        njobs = 1\n",
    "        print('testing')\n",
    "        cluster = LocalCluster(n_workers = NCORE )    \n",
    "        print(cluster.dashboard_link)\n",
    "        client = Client(cluster)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nucleotide_mutation = None\n",
    "AA_mutation = None\n",
    "for mat in eventmats:\n",
    "    with open( mat , 'rb') as pklin:\n",
    "        mats = pickle.loads(pklin.read())\n",
    "        print(mats)\n",
    "        if AA_mutation is None:\n",
    "            nucleotide_mutation = mats[1]\n",
    "            AA_mutation = mats[0]\n",
    "        else:\n",
    "            nucleotide_mutation += mats[1]\n",
    "            AA_mutation += mats[0]\n",
    "print(nucleotide_mutation)\n",
    "print(AA_mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_cleanup = True\n",
    "from scipy.stats import gamma\n",
    "\n",
    "if mat_cleanup == True:\n",
    "    for transition in range(AA_mutation.shape[2]):\n",
    "        if transition % 10 == 0 :\n",
    "            print(transition)\n",
    "        coevmat = AA_mutation[:,:,transition]\n",
    "        sumv = coevmat.sum(axis = 0).todense()\n",
    "\n",
    "        posi = np.log(sumv[sumv>0])\n",
    "        if len(posi)>0 and np.sum(posi)>10:\n",
    "            \n",
    "            try:\n",
    "                a,loc, scale =gamma.fit(posi)\n",
    "                probas = gamma.cdf( posi , a , loc , scale)\n",
    "\n",
    "                if np.amax(probas)>.999:\n",
    "                    thresh = np.amin(np.exp(posi[probas>.999]))\n",
    "                #remove odd looking transition columns\n",
    "            except:\n",
    "                thresh  = 0\n",
    "                \n",
    "                if transition == 0:\n",
    "                    AAmat = AA_mutation[:,:,transition].to_scipy_sparse()\n",
    "                    AAmat[:,sumv>thresh] = 0\n",
    "                else:\n",
    "                    AAmat_ =  scipy.sparse.csc_matrix(AA_mutation[:,:,transition].to_scipy_sparse())\n",
    "                    AAmat_[:,sumv>thresh] = 0\n",
    "                    AAmat += scipy.sparse.coo_matrix(AAmat_)        \n",
    "            cdf = np.array( np.cumsum(sumv) / np.sum(sumv))        \n",
    "        else:\n",
    "            if transition == 0:\n",
    "                AAmat =  AA_mutation[:,:,transition].to_scipy_sparse()\n",
    "            else:\n",
    "                AAmat +=  AA_mutation[:,:,transition].to_scipy_sparse()\n",
    "                \n",
    "    with open( alnfile + 'AAmat_sum.pkl' , 'wb')as AAmatout:\n",
    "        print(AAmat.shape)\n",
    "        AAmatout.write(pickle.dumps(AAmat))\n",
    "else:\n",
    "    with open( alnfile + 'AAmat_sum.pkl' , 'rb')as AAmatout:\n",
    "        AAmat = pickle.loads(AAmatout.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = [ b'A', b'C', b'G' , b'T' ]\n",
    "allowed_transitions = [ c1+c2 for c1 in allowed_symbols for c2 in allowed_symbols  if c1!= c2]\n",
    "print('allowed transitions',allowed_transitions)\n",
    "\n",
    "transition_dict = {  c : i  for i,c in enumerate( allowed_transitions )  }\n",
    "rev_transition_dict= dict( zip(transition_dict.values(), transition_dict.keys()))\n",
    "allowed_symbols = set(allowed_symbols)\n",
    "\n",
    "print('transition dict', transition_dict)\n",
    "ProteinAlphabet = [ 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y' ]\n",
    "allowed_AA_transitions = [ c1+c2 for c1 in ProteinAlphabet for c2 in ProteinAlphabet  if c1!= c2]\n",
    "transitiondict_AA = {  c : i  for i,c in enumerate( allowed_AA_transitions )  }\n",
    "rev_transitiondict_AA = dict( zip(transitiondict_AA.values(), transitiondict_AA.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,n in enumerate(tree.nodes()):\n",
    "    n.matrow = i\n",
    "    n.symbols = None\n",
    "    n.scores = None\n",
    "    n.event = None\n",
    "    n.char = None\n",
    "\n",
    "matsize = len(tree.nodes())\n",
    "print(matsize)\n",
    "print('nodes')\n",
    "#blur w connectivity mat\n",
    "blurfactor =  .25\n",
    "connectmat = scipy.sparse.csc_matrix((len(tree.nodes()), len(tree.nodes() ) ) )\n",
    "index = np.array([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "lengths = np.array([ c.edge_length for n in tree.nodes() for c in n.child_nodes()])\n",
    "total_len = np.sum(lengths)\n",
    "#fill diagonal\n",
    "#index = np.vstack( [index , np.array([ [n.matrow, n.matrow ] for n in tree.nodes() ]) ] )\n",
    "\n",
    "connectmat[index[:,0],index[:,1]] = 1\n",
    "connectmat[index[:,1],index[:,0]] = 1\n",
    "\n",
    "\n",
    "#connectmat = connectmat.todense()\n",
    "diag = [ i for i in range(connectmat.shape[0])]\n",
    "connectmat[diag,diag] = 1\n",
    "#connectmat = connectmat.todense()\n",
    "#connectmat = scipy.sparse.csc_matrix(connectmat)\n",
    "#np.fill_diagonal(connectmat , 1)\n",
    "connectmat = scipy.sparse.coo_matrix(connectmat)\n",
    "plt.figure( figsize=(10,10))\n",
    "plt.title( 'phylo tree connectivity matrix ' )\n",
    "plt.spy(connectmat, markersize= 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####define the aln row and structure here, for dengue they should all be the same\n",
    "selected_strain = list(IDindex.keys())[100]\n",
    "import dask.array as da\n",
    "with h5py.File(alnh5, 'r') as hf:\n",
    "    align_array = hf['MSA2array']\n",
    "    print(IDindex[selected_strain])\n",
    "    print(selected_strain)\n",
    "    #filter to columns without gaps\n",
    "    non_gap = np.where( align_array[: , IDindex[selected_strain]] != b'-')[0]\n",
    "    print(non_gap)\n",
    "    print(non_gap.shape)\n",
    "    sequence = align_array[ non_gap , IDindex[selected_strain]]\n",
    "    print(np.unique(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the amino acid chains in the structs\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "with open( alnfile + 'refgeno.fasta' , 'w' ) as fastout:\n",
    "    fastout.write('>'+selected_strain + '\\n')\n",
    "    fastout.write( ''.join( c.decode() for c in list(sequence) ) + '\\n' )\n",
    "qfile =  alnfile + 'refgeno.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blastpath = filedir + 'software/ncbi-blast-2.11.0+-src/c++/ReleaseMT/bin/'\n",
    "\n",
    "\n",
    "print(modeldir + 'covid_structs.fasta')\n",
    "def runblastx( qseq , blastpath = blastpath , outannot = alnfile+'struct_blastout.txt' , db = modeldir + 'struct_chains.fasta' , outfmt = None ):\n",
    "    if outfmt is None:\n",
    "        outfmt = [ 'qseqid' , 'sseqid' , 'qlen' ,  'slen' , 'qstart' , 'qend' ,  'qframe' , 'evalue' ]\n",
    "        outfmt =  ' \"10 ' + ''.join([fmt+ ' ' for fmt in outfmt]) + ' \" '\n",
    "        print(outfmt)\n",
    "    args = blastpath+'blastx -query '+ qfile + ' -db '+db+' -outfmt' + outfmt + ' -out ' + outannot  \n",
    "    print(args)\n",
    "    p = subprocess.run( shlex.split(args) )\n",
    "    return p , outannot\n",
    "\n",
    "p,annot = runblastx(qfile)\n",
    "print(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = pd.read_csv(  annot , header = None )\n",
    "annotation.columns = [ 'qseqid' , 'sseqid' , 'qlen' ,  'slen' , 'qstart' , 'qend' ,  'qframe' , 'evalue' ] \n",
    "annotation = annotation[ annotation['evalue'] < 10**-3 ]\n",
    "\n",
    "annotation['struct'] = annotation.sseqid.map( lambda x : x.split(':')[0].split('/')[-1].replace('.pdb','') )\n",
    "annotation['chain'] = annotation.sseqid.map( lambda x : x.split(':')[1] )\n",
    "annotation.to_csv( alnfile +'struct_annotation.csv'  )\n",
    "print(annotation)\n",
    "#make annotation for dengue orfs\n",
    "#find equivalent structures in multimers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainlen = dict( zip ( annotation.struct + annotation.chain , annotation.slen ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_annot = True\n",
    "if filter_annot == True:\n",
    "    annotation = annotation[annotation.struct == '7DZW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab AA chain residues\n",
    "import itertools\n",
    "import warnings\n",
    "distmats = {}\n",
    "overwrite_connect = True\n",
    "@dask.delayed()\n",
    "def retmat_single(totalatoms, chain, slen= None):\n",
    "    ret = np.array( [[ a1['CA'] - a2['CA']  for i,a1 in enumerate(totalatoms[chain])  ] for j,a2 in enumerate(totalatoms[chain]) ] )\n",
    "    if slen:\n",
    "        ret = ret[:slen]\n",
    "        ret = ret[:,:slen]\n",
    "    return ret\n",
    "    \n",
    "@dask.delayed()\n",
    "def retmat_double(totalatoms, chain1, chain2 , slen1= None , slen2 = None):\n",
    "    ret = np.array( [[ a1['CA'] - a2['CA'] for i,a1 in enumerate(totalatoms[chain1])] for j,a2 in enumerate(totalatoms[chain2]) ] )\n",
    "    if slen1:\n",
    "        ret = ret[:slen1]\n",
    "    if slen2:\n",
    "        ret = ret[:,:slen2]\n",
    "    return ret\n",
    "\n",
    "@dask.delayed()\n",
    "def addT(arr):\n",
    "    arr += arr.T\n",
    "\n",
    "if overwrite_connect == True:\n",
    "    totalatoms = {}\n",
    "    for i,model in enumerate(annotation.sseqid.unique()):\n",
    "        with warnings.catch_warnings():\n",
    "            m = model.split('/')[-1].replace('.pdb' , '').split(':')[0]\n",
    "            print(m)\n",
    "            \n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            structure = parser.get_structure(m, model.split(':')[0])\n",
    "            io=PDBIO()\n",
    "            io.set_structure(structure)\n",
    "            totalatoms.update( { c.id : [ r for r in c.get_residues() if 'CA'  in r ] for c in structure.get_chains() } )\n",
    "    for chain in totalatoms:\n",
    "        print(structure)\n",
    "        if model not in distmats:\n",
    "            distmats[m]= {}\n",
    "            #if m+chain in chainlen:\n",
    "            #    l1 = chainlen[m+chain]    \n",
    "        l1 = None\n",
    "        distmats[m][chain] = retmat_single( totalatoms, chain , l1 )\n",
    "            #get interchain dists\n",
    "    combocount = 0\n",
    "    for chain1,chain2 in itertools.combinations(totalatoms,2):\n",
    "        if combocount < 2:\n",
    "            l1 = None\n",
    "            #if m+chain1 in chainlen:\n",
    "            #    l1 = chainlen[m+chain1]\n",
    "            l2 = None\n",
    "            #if m+chain2 in chainlen:\n",
    "            #    l2 = chainlen[m+chain2]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        combocount+=1\n",
    "        distmats[m][(chain1,chain2)] = retmat_double(totalatoms, chain1, chain2  ,l1 , l2)    \n",
    "        #compute vals\n",
    "        #    print( ' err ', m )\n",
    "    if m in distmats:\n",
    "        distmats[m] = dict( zip ( distmats[m].keys() , dask.compute( * list(distmats[m].values() ))))\n",
    "        #distmats[m] = dict( zip ( distmats[m].keys() , dask.compute( * [dropna(a) for a in list(distmats[m].values() )] ) ) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_distmats = False\n",
    "remove =[]\n",
    "if overwrite_connect == True:\n",
    "    if show_distmats == True:\n",
    "        for model in distmats:\n",
    "            print( model )\n",
    "            for i,chain in enumerate(distmats[model]):\n",
    "                if distmats[model][chain].shape[0]>0:\n",
    "                    if np.sum(distmats[model][chain])==0:\n",
    "                        remove.append((model,chain))\n",
    "                    else:\n",
    "                        plt.figure(figsize= (20,20))\n",
    "                        plt.title('distmat ' + model + '  '+  ''.join(chain))\n",
    "                        plt.imshow(distmats[model][chain])\n",
    "                        plt.show()\n",
    "    for model,chain in remove:\n",
    "        del distmats[model][chain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "angstrom_cutoff = 15\n",
    "show_contacts = True\n",
    "\n",
    "if overwrite_connect == True:\n",
    "    @dask.delayed( nout = 2)\n",
    "    def define_contacts(mat , angstrom_cutoff = 15 ):\n",
    "        subthresh = copy.deepcopy(mat)\n",
    "        subthresh[ subthresh < angstrom_cutoff ] = 1 \n",
    "        subthresh[ subthresh > angstrom_cutoff ] = 0 \n",
    "        np.fill_diagonal(subthresh , 0)\n",
    "        subthresh = subthresh[0:subthresh.shape[0]-20,0:subthresh.shape[1]-20]\n",
    "        connected = np.dot(subthresh,subthresh.T)\n",
    "        return subthresh,connected\n",
    "\n",
    "    subthresh_thresh ={}\n",
    "    subthresh_connected ={}\n",
    "\n",
    "    chain_equivalencies={}\n",
    "    for m in distmats:\n",
    "        for chain in distmats[m]:\n",
    "            if m not in subthresh_thresh:\n",
    "                subthresh_thresh[m]={}\n",
    "                subthresh_connected[m] ={}\n",
    "\n",
    "                for chain in distmats[m]:\n",
    "                    subthresh, connected = define_contacts(distmats[m][chain] , angstrom_cutoff  )\n",
    "                    subthresh_thresh[m][chain] = subthresh\n",
    "                    subthresh_connected[m][chain] = connected\n",
    "            subthresh_thresh[m] = dict( zip ( subthresh_thresh[m].keys() , dask.compute( * list(subthresh_thresh[m].values() ))))\n",
    "            subthresh_connected[m] = dict( zip ( subthresh_connected[m].keys() , dask.compute( * list(subthresh_connected[m].values() ))))\n",
    "\n",
    "            if show_contacts == True:\n",
    "                for chain in subthresh_thresh[m]:\n",
    "                    subthresh = subthresh_thresh[m][chain]\n",
    "\n",
    "                    plt.figure(figsize=(10,10) )\n",
    "                    plt.title(m +'contact mat '+ ''.join(chain) )\n",
    "                    plt.scatter( np.nonzero(subthresh)[0] ,np.nonzero(subthresh)[1]  , marker= 'o' , alpha = .1 ,  s = 20)\n",
    "                    plt.show()\n",
    "\n",
    "                for chain in subthresh_connected[m]:\n",
    "                    connected = subthresh_connected[m][chain]\n",
    "                    plt.figure(figsize=(10,10) )\n",
    "                    plt.title('contact mat '+ ''.join(chain) )\n",
    "                    plt.scatter( np.nonzero(connected)[0] ,np.nonzero(connected)[1]  , marker= 'o' , alpha = .1 ,  s = 20)\n",
    "                    plt.show()\n",
    "\n",
    "    with open(modeldir + 'contactmaps' , 'wb') as connectout:\n",
    "        connectout.write(pickle.dumps((subthresh_thresh , subthresh_connected)))\n",
    "else:\n",
    "    with open(modeldir + 'contactmaps' , 'rb') as connectout:\n",
    "        subthresh_thresh , subthresh_connected = pickle.loads(connectout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minhash all event cols with more than #nthresh events\n",
    "#search top 10 for each col\n",
    "allpairs = {}\n",
    "\n",
    "print(subthresh_thresh)\n",
    "print(subthresh_connected)\n",
    "\n",
    "for code in subthresh_thresh:\n",
    "    if code not in allpairs:\n",
    "        allpairs[code] = {}\n",
    "    for chainpair in subthresh_thresh[code]:\n",
    "        pairs = np.nonzero(subthresh_thresh[code][chainpair])\n",
    "        pairset = set([ ( pairs[0][i] , pairs[1][i] ) for i in range(pairs[0].shape[0]) ])\n",
    "        \n",
    "        allpairs[code][chainpair] = pairset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree2Single_sparse_graph_updown(tree):\n",
    "    N = len(tree.nodes())\n",
    "    #mimic the fitch algo\n",
    "    #propagate up and down in separate graphs\n",
    "    index_up = np.vstack([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    index_down = np.vstack([ [c.matrow, n.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "    connectmat_up = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_down = scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_up[index_up[:,0],index_up[:,1]] = 1 \n",
    "    connectmat_down[index_down[:,0],index_down[:,1]] = 1 \n",
    "    diag = [[n,n] for n in range(N)]\n",
    "    connectmat_diag=scipy.sparse.lil_matrix(( N ,  N ) )\n",
    "    connectmat_diag[diag,diag] = 1 \n",
    "    ntime = np.array([ n.distance_from_root() for n in tree.nodes()])\n",
    "    mtime = np.amax(ntime)\n",
    "    ntime/=mtime\n",
    "    levels = np.array([ n.level() for n in tree.nodes() ] , dtype='double')\n",
    "    levels /= np.amax(levels)\n",
    "    Norm_nchild= np.array( [ len(n.child_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    mchild =np.amax(Norm_nchild)\n",
    "    Norm_nchild/=mchild \n",
    "    Norm_nsister= np.array( [ len(n.sister_nodes()) for n in tree.nodes() ] ,dtype='double' )\n",
    "    msis =np.amax(Norm_nsister)\n",
    "    Norm_nsister/=msis    \n",
    "    template_features = np.stack([ntime ,  Norm_nchild , Norm_nsister ]).T    \n",
    "    return connectmat_up, connectmat_down, connectmat_diag, template_features\n",
    "\n",
    "def sparse2pairs(sparsemat, matrows = None):\n",
    "    if matrows :\n",
    "        sparsemat = sparsemat[matrows,:]\n",
    "        sparsemat = sparsemat[:,matrows]\n",
    "    sparsemat = scipy.sparse.find(sparsemat)\n",
    "    return np.vstack([sparsemat[0],sparsemat[1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree sector based aggregation\n",
    "#label sectors\n",
    "\n",
    "def process_node_down(node, sector = 0, breakpt = 10 , total = 0 ):\n",
    "    node.sector = sector\n",
    "    if sector == 0 :\n",
    "        global count\n",
    "        count = 0\n",
    "    total += len(node.child_nodes())\n",
    "    for i,child in enumerate(node.child_nodes()):\n",
    "        if total > breakpt:\n",
    "            if len(child.child_nodes())>0:\n",
    "                #new sector w new total\n",
    "                count+=1\n",
    "                process_node_down(child, count , total = 0 , breakpt = breakpt)\n",
    "            else:\n",
    "                #leaf\n",
    "                process_node_down(child, count , total = 0 , breakpt = breakpt)\n",
    "        else:\n",
    "            process_node_down(child, count , total = total , breakpt = breakpt)\n",
    "\n",
    "    \n",
    "def get_sectors(tree, breakpt = 10):\n",
    "    process_node_down( tree.seed_node , sector = 0, breakpt = breakpt )\n",
    "    row = [n.matrow for n in tree.nodes()]\n",
    "    col = [n.sector for n in tree.nodes()]\n",
    "    data = np.ones((len(row)))\n",
    "    sectormat = scipy.sparse.csc_matrix( (data,(row,col)) )\n",
    "    return sectormat\n",
    "\n",
    "for i,l in enumerate(tree.nodes()):\n",
    "    l.sum_lengths = None\n",
    "for i,l in enumerate(tree.leaf_nodes()):\n",
    "    l.sum_lengths = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_templates = False\n",
    "if compile_templates == True:\n",
    "    #check correlation of graphnet score w jaccard\n",
    "    connectmat_up, connectmat_down, connectmat_diag, template_features = tree2Single_sparse_graph_updown(tree)\n",
    "    print(connectmat_up.shape)\n",
    "    print(template_features.shape)\n",
    "\n",
    "    sectormat = get_sectors(tree, breakpt = 600 )\n",
    "    print(sectormat.shape)\n",
    "    with open('template_features.pkl' , 'wb') as templateout:\n",
    "        templateout.write(pickle.dumps([connectmat_up, connectmat_down, connectmat_diag, template_features]))\n",
    "\n",
    "    with open('sectormat.pkl' , 'wb' ) as sectorout:\n",
    "        sectorout.write(pickle.dumps(sectormat))\n",
    "else:\n",
    "    with open('template_features.pkl' , 'rb') as template_in:\n",
    "        connectmat_up, connectmat_down, connectmat_diag, template_features = pickle.loads(template_in.read())\n",
    "    with open('sectormat.pkl' , 'rb') as sector_in:\n",
    "        sectormat = pickle.loads(sector_in.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.01439458 0.        ]\n",
      " [0.         0.         0.01355932]\n",
      " [0.         0.09822185 0.01355932]\n",
      " ...\n",
      " [0.         0.         0.01355932]\n",
      " [0.         0.         0.01355932]\n",
      " [0.         0.         0.01355932]]\n",
      "(997916, 1067)\n"
     ]
    }
   ],
   "source": [
    "print( template_features )\n",
    "print( sectormat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "\n",
    "def ret_pytorch_sample(subfeatures, connect_up , connect_down , sub_diag ,  overview , overview_rev , intra , toss ):\n",
    "    data = HeteroData()\n",
    "\n",
    "    #add input data\n",
    "\n",
    "    data['phylonodes_up'].x = torch.tensor( subfeatures )\n",
    "    data['phylonodes_down'].x =torch.tensor( subfeatures )\n",
    "    data['sectornode'].x =torch.tensor(  np.zeros((1,1)) )\n",
    "    #up down fitch net\n",
    "    data['phylonodes_up', 'phylolink_up', 'phylonodes_up'].edge_index = torch.tensor(connect_up ,  dtype=torch.long )\n",
    "    data['phylonodes_down', 'phylolink_down', 'phylonodes_down'].edge_index = torch.tensor(connect_down ,  dtype=torch.long )             \n",
    "    data['phylonodes_up', 'phylolink_up_down', 'phylonodes_down'].edge_index = torch.tensor( sub_diag ,  dtype=torch.long )\n",
    "    data['phylonodes_down', 'phylolink_down_up', 'phylonodes_up'].edge_index = torch.tensor( sub_diag ,  dtype=torch.long )\n",
    "    #pooling connections\n",
    "    data['phylonodes_down', 'informs', 'sectornode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "    data['phylonodes_up', 'informs', 'sectornode'].edge_index = torch.tensor(overview ,  dtype=torch.long )\n",
    "\n",
    "    #pooling connections\n",
    "    data['sectornode',  'informs', 'phylonodes_down' ].edge_index = torch.tensor(overview_rev,  dtype=torch.long )\n",
    "    data['sectornode',  'informs', 'phylonodes_up'].edge_index = torch.tensor(overview_rev ,  dtype=torch.long )\n",
    "    #categories are intra or interprotein contacts or nothing\n",
    "    if intra == True:\n",
    "        cateforical = np.array([0,1])\n",
    "    else:\n",
    "        cateforical = np.array([1,0])\n",
    "    data['phylonodes_down'].y =torch.tensor( np.ones((subfeatures.shape[0],1) ) *toss ,  dtype=torch.long )\n",
    "    data['phylonodes_up'].y =torch.tensor( np.ones((subfeatures.shape[0],1)) * toss ,  dtype=torch.long )\n",
    "    #todo change to categorical\n",
    "    data['sectornode'].y =torch.tensor(  np.ones((1,1))*toss  ,  dtype=torch.long )\n",
    "    data = T.AddSelfLoops()(data)\n",
    "    data = T.NormalizeFeatures()(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_data_updown_transitions(intra ,pairs, AA_tensor, NT_tensor, sectormat, connectmat_up, connectmat_down, connectmat_diag, \n",
    " template_features , posi_percent = .5 , nsamples = 10 , min_nodes = 100,  q = None , iolock= None,  verbose = True, loop= True , sectors_chunk = True ):\n",
    "    #upward and downward connected phylo nodes\n",
    "    Nsectors = 0\n",
    "    Nnodes = 0\n",
    "    allcols =list( np.arange(NT_tensor.shape[1]) )\n",
    "    while True:\n",
    "        toss = scipy.stats.bernoulli.rvs(posi_percent, loc=0, size=1, random_state=None)\n",
    "        label = np.ones((1,1))*toss\n",
    "    \n",
    "        if verbose == True:\n",
    "            print('posi/nega',toss)\n",
    "            print('posi/nega',label)\n",
    "\n",
    "        if toss == 0:\n",
    "            col1 = random.choice(allcols)\n",
    "            col2 = col1\n",
    "            while col1 == col2 and (col1,col2) not in pairs:\n",
    "                col2 = random.choice(allcols)\n",
    "            labels = np.zeros((template_features.shape[0],))           \n",
    "        else:\n",
    "            #positive sample\n",
    "            pairtuple = random.choice(pairs)\n",
    "            col1 = pairtuple[0]\n",
    "            col2 = pairtuple[1]\n",
    "        if verbose == True:\n",
    "            print(col1, col2)\n",
    "        nodeAAfeatures = sparse.stack( [AA_tensor[:,col1,:] ,AA_tensor[:,col2,:] ] , axis = 2  ).reshape((AA_tensor.shape[0],-1)).to_scipy_sparse()\n",
    "        nt_cols = []\n",
    "        for pos in [0,1,2]:\n",
    "            nodeNTfeatures = sparse.stack( [NT_tensor[:,col1,pos,:] ,NT_tensor[:, col2,pos,:] ] , axis = 2)\n",
    "            nt_cols.append(nodeNTfeatures.reshape((nodeNTfeatures.shape[0],-1)).to_scipy_sparse() )\n",
    "        \n",
    "        nodefeatures = scipy.sparse.hstack([nodeAAfeatures,scipy.sparse.coo_matrix(template_features)]+nt_cols)\n",
    "        #slice the features into sectors and yield sectors\n",
    "        if sectors_chunk == True:\n",
    "            nodefeatures = scipy.sparse.lil_matrix(nodefeatures)\n",
    "            count = 0\n",
    "            sectors = list(np.arange(sectormat.shape[1])) \n",
    "            for sample in range(nsamples):\n",
    "                sector = random.choice(sectors)\n",
    "                rows = scipy.sparse.find(sectormat[:,sector])[0]\n",
    "                if rows.shape[0]> min_nodes:\n",
    "                    count +=1\n",
    "                    if verbose == True:\n",
    "                        print('rows',rows.shape)\n",
    "\n",
    "                #node features\n",
    "                subfeatures = nodefeatures[rows,:].todense()\n",
    "                #phylonode connections\n",
    "                sub_connect_up = connectmat_up[rows,:]\n",
    "                sub_connect_up = sub_connect_up[:,rows]\n",
    "                connect_up = sparse2pairs(sub_connect_up)\n",
    "                sub_connect_down = connectmat_down[rows,:]\n",
    "                sub_connect_down = sub_connect_down[:,rows]\n",
    "                connect_down = sparse2pairs(sub_connect_down)\n",
    "                sub_diag = connectmat_diag[rows,:]\n",
    "                sub_diag = sub_diag[:,rows]\n",
    "                sub_diag = sparse2pairs(sub_diag)\n",
    "                #aggregator node\n",
    "                overview = scipy.sparse.lil_matrix( (subfeatures.shape[0], 2 ) )\n",
    "                overview[:,0] = 1\n",
    "                overview_rev = sparse2pairs(overview.T)\n",
    "                overview = sparse2pairs(overview)\n",
    "                data = ret_pytorch_sample(subfeatures, connect_up , connect_down , sub_diag ,  overview , overview_rev , intra , toss )\n",
    "                if q:\n",
    "                    q.put(data)\n",
    "                else:\n",
    "                    yield data\n",
    "        else:\n",
    "            #yield the entire graph\n",
    "            #not working with sparse yet...\n",
    "            #node features\n",
    "            #change to torch sparse\n",
    "            values = nodefeatures.data\n",
    "            indices = np.vstack((nodefeatures.row, nodefeatures.col))\n",
    "\n",
    "            i = torch.LongTensor(indices)\n",
    "            v = torch.FloatTensor(values)\n",
    "            shape = nodefeatures.shape\n",
    "            subfeatures = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "            subfeatures = subfeatures.coalesce()\n",
    "            #phylonode connections\n",
    "            sub_connect_up = connectmat_up\n",
    "            connect_up = sparse2pairs(sub_connect_up)\n",
    "            sub_connect_down = connectmat_down\n",
    "            connect_down = sparse2pairs(sub_connect_down)\n",
    "            sub_diag = connectmat_diag\n",
    "            sub_diag = sparse2pairs(sub_diag)\n",
    "            #aggregator node\n",
    "            overview = scipy.sparse.lil_matrix( (subfeatures.shape[0], 2 ) )\n",
    "            overview[:,0] = 1\n",
    "            overview_rev = sparse2pairs(overview.T)\n",
    "            overview = sparse2pairs(overview)\n",
    "            data = ret_pytorch_sample(subfeatures, connect_up , connect_down , sub_diag ,  overview , overview_rev , intra , toss )\n",
    "            if q:\n",
    "                q.put(data)\n",
    "            else:\n",
    "                yield data\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "murphy12 {'L': 0, 'V': 0, 'I': 0, 'M': 0, 'C': 1, 'A': 2, 'G': 3, 'S': 4, 'T': 4, 'P': 5, 'F': 6, 'Y': 6, 'W': 7, 'E': 8, 'Q': 8, 'D': 9, 'N': 9, 'K': 10, 'R': 10, 'H': 11}\n",
      "380\n",
      "138\n",
      "['AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AK', 'AL', 'AM', 'AN', 'AP', 'AQ', 'AR', 'AS', 'AT', 'AV', 'AW', 'AY', 'CA', 'CD', 'CE', 'CF', 'CG', 'CH', 'CI', 'CK', 'CL', 'CM', 'CN', 'CP', 'CQ', 'CR', 'CS', 'CT', 'CV', 'CW', 'CY', 'DA', 'DC', 'DE', 'DF', 'DG', 'DH', 'DI', 'DK', 'DL', 'DM', 'DN', 'DP', 'DQ', 'DR', 'DS', 'DT', 'DV', 'DW', 'DY', 'EA', 'EC', 'ED', 'EF', 'EG', 'EH', 'EI', 'EK', 'EL', 'EM', 'EN', 'EP', 'EQ', 'ER', 'ES', 'ET', 'EV', 'EW', 'EY', 'FA', 'FC', 'FD', 'FE', 'FG', 'FH', 'FI', 'FK', 'FL', 'FM', 'FN', 'FP', 'FQ', 'FR', 'FS', 'FT', 'FV', 'FW', 'FY', 'GA', 'GC', 'GD', 'GE', 'GF'] ...etc...\n"
     ]
    }
   ],
   "source": [
    "#create reduced alphabet mapping\n",
    "#AA transitions\n",
    "\n",
    "murphy12 = [('L','V','I','M'), ('C'), ('A'), ('G'), ('S','T'), ('P'), ('F','Y'), ('W'), ('E','Q'), ('D','N'), ('K','R'), ('H') ]\n",
    "murphy12 = { c:i for i,cset in enumerate(murphy12) for c in cset   }\n",
    "print('murphy12',murphy12)\n",
    "\n",
    "ProteinAlphabet = [ 'A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y' ]\n",
    "allowed_AA_transitions = [ c1+c2 for c1 in ProteinAlphabet for c2 in ProteinAlphabet  if c1!= c2]\n",
    "\n",
    "print(len(set(allowed_AA_transitions)))\n",
    "new_transitions = [ (murphy12[c1],murphy12[c2]) for c1 in ProteinAlphabet for c2 in ProteinAlphabet  if c1!= c2  ]\n",
    "new_transitions = { tup:i for i,tup in enumerate(new_transitions) }\n",
    "\n",
    "print(len(set(new_transitions)))\n",
    "print(allowed_AA_transitions[0:100] , '...etc...')\n",
    "transitiondict_AA = {  c : i  for i,c in enumerate( allowed_AA_transitions )  }\n",
    "rev_transitiondict_AA = dict( zip(transitiondict_AA.values(), transitiondict_AA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reduce alphabet to reduce AA transition dimensionality\n",
    "def restrictAA_transitions(AAmat, rev_transitiondict_AA, murphy12 , new_transitions , restricted_transitions = 138 ):\n",
    "    restricted = None\n",
    "    for transition in AAmat.shape[2]:\n",
    "        transition = rev_transition_dict[col]\n",
    "        new_transition = new_transitions[(murphy12[transition[0]] , murphy12[transition[1]] )]\n",
    "        data = AAmat[:,:,transition].data\n",
    "        if restricted is not None:\n",
    "            coords =  AAmat[:,:,transition].coords\n",
    "            coords = np.hstack( [coords, np.ones(coords.shape[0])*new_transtion])\n",
    "            restricted  += sparseND.COO( coords =  coords , data = data\n",
    "                                        , shape = (AAmat.shape[0] , AAmat.shape[1] , restricted_transitions )  )\n",
    "        else:\n",
    "            restricted  =  sparseND.COO( coords = coords , data = data  \n",
    "                                        , shape = (AAmat.shape[0] , AAmat.shape[1] , restricted_transitions )  )\n",
    "    return restricted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  qseqid                                             sseqid   qlen  slen  \\\n",
      "0    EPI  /work/FAC/FBM/DBC/cdessim2/default/dmoi/datase...  29809  1121   \n",
      "1    EPI  /work/FAC/FBM/DBC/cdessim2/default/dmoi/datase...  29809  1121   \n",
      "2    EPI  /work/FAC/FBM/DBC/cdessim2/default/dmoi/datase...  29809  1121   \n",
      "\n",
      "   qstart   qend  qframe  evalue struct chain  \n",
      "0   21604  24966       1     0.0   7DZW     C  \n",
      "1   21604  24966       1     0.0   7DZW     B  \n",
      "2   21604  24966       1     0.0   7DZW     A  \n",
      "<COO: shape=(997916, 29809, 12), dtype=float64, nnz=2075009, fill_value=0.0>\n",
      "<COO: shape=(997916, 29809, 380), dtype=float64, nnz=1338566, fill_value=0.0>\n",
      "21604\n",
      "done AA\n",
      "done NT\n",
      "21604\n",
      "21604\n"
     ]
    }
   ],
   "source": [
    "###rewrite codon compilation for sparse\n",
    "import sparse\n",
    "\n",
    "def retcodons( AAmat , NTmat, qstart, qend, verbose = True ):\n",
    "    #aa mutations for each pos\n",
    "    AAmat_sub = sparse.stack(  [ AAmat[ : , codon:codon+2 , : ].sum(axis = 1) for codon in range(qstart-1, qend-1 , 3 )  ] , axis = 1 )    \n",
    "    #add the frames for each in a stack \n",
    "    if verbose == True:\n",
    "        print('done AA')\n",
    "    NTmat_sub = sparse.stack([  sparse.stack( [ NTmat[:, codon + frame , : ] for frame in [0,1,2] ] , axis = 1 )  for codon in range(qstart-1, qend-1 , 3 ) ] , axis = 1)\n",
    "    if verbose == True:\n",
    "        print('done NT')\n",
    "    return AAmat_sub , NTmat_sub\n",
    "\n",
    "print(annotation)\n",
    "structmats = {}\n",
    "\n",
    "start_stop ={}\n",
    "print(nucleotide_mutation)\n",
    "print(AA_mutation)\n",
    "\n",
    "for i,row  in annotation.iterrows():\n",
    "    print(row.qstart)\n",
    "    if row.struct not in structmats:\n",
    "        structmats[row.struct]={}    \n",
    "    if (row.qstart , row.qend) not in start_stop:\n",
    "        structmats[row.struct][row.chain ] = retcodons(AA_mutation, nucleotide_mutation  , row.qstart , row.qend)\n",
    "        start_stop[(row.qstart , row.qend)] = structmats[row.struct ][ row.chain ]\n",
    "    else:\n",
    "        structmats[row.struct][row.chain ] = start_stop[(row.qstart , row.qend)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'7DZW': {'C': (<COO: shape=(997916, 1121, 380), dtype=float64, nnz=371401, fill_value=0.0>, <COO: shape=(997916, 1121, 3, 12), dtype=float64, nnz=401917, fill_value=0.0>), 'B': (<COO: shape=(997916, 1121, 380), dtype=float64, nnz=371401, fill_value=0.0>, <COO: shape=(997916, 1121, 3, 12), dtype=float64, nnz=401917, fill_value=0.0>), 'A': (<COO: shape=(997916, 1121, 380), dtype=float64, nnz=371401, fill_value=0.0>, <COO: shape=(997916, 1121, 3, 12), dtype=float64, nnz=401917, fill_value=0.0>)}}\n"
     ]
    }
   ],
   "source": [
    "print(structmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': array([[28., 19., 19., ...,  0.,  0.,  0.],\n",
      "       [19., 34., 31., ...,  0.,  0.,  0.],\n",
      "       [19., 31., 39., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ..., 19., 18., 16.],\n",
      "       [ 0.,  0.,  0., ..., 18., 20., 16.],\n",
      "       [ 0.,  0.,  0., ..., 16., 16., 17.]], dtype=float32), ('A', 'B'): array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), ('A', 'C'): array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ...,\n",
      "       [ 0.,  0.,  0., ...,  9.,  8.,  7.],\n",
      "       [ 0.,  0.,  0., ...,  8., 10.,  9.],\n",
      "       [ 0.,  0.,  0., ...,  7.,  9., 11.]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(subthresh_connected['7DZW'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden units 100\n",
      "layer 0\n",
      "hidden units 100\n",
      "layer 1\n",
      "hidden units 100\n",
      "layer 2\n",
      "hidden units 100\n",
      "layer 3\n",
      "hidden units 100\n",
      "layer 4\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/7217717/ipykernel_1269384/4083212057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHeteroGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 897\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         raise ValueError(\n\u001b[1;32m    122\u001b[0m             \u001b[0;34m'Attempted to use an uninitialized parameter in {}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/FAC/FBM/DBC/cdessim2/default/dmoi/condaenvs/envs/ML2/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import HeteroConv , SAGEConv, Linear , ResGatedGraphConv , GATv2Conv , TransformerConv , MFConv , FiLMConv \n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "#without sectornode\n",
    "class HeteroGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.lins =  torch.nn.ModuleList()\n",
    "        self.lins2 =  torch.nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                ('phylonodes_up', 'phylolink_up', 'phylonodes_up'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down', 'phylonodes_down'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_down', 'phylolink_down_up', 'phylonodes_up'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'phylolink_up_down', 'phylonodes_down'):TransformerConv((-1,-1),  int( hidden_channels) ), \n",
    "                ('phylonodes_down', 'informs', 'sectornode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "                ('phylonodes_up', 'informs', 'sectornode'):TransformerConv((-1,-1),  int( hidden_channels) ),\n",
    "            } , aggr='sum')\n",
    "            self.convs.append(conv)\n",
    "            for vectype in  ['phylonodes_up', 'phylonodes_down' , 'sectornode'  ]:\n",
    "                lin1 = Linear(-1 , int( hidden_channels))\n",
    "                self.lins.append( lin1 )\n",
    "                \n",
    "            print( 'hidden units' , int( hidden_channels) )\n",
    "            print( 'layer' , i )\n",
    "\n",
    "        for vectype in ['phylonodes_up', 'phylonodes_down' , 'sectornode' ]:\n",
    "            lin2 = Linear(-1 , out_channels)\n",
    "            self.lins2.append( lin2 ) \n",
    "\n",
    "    \n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        lins = iter(self.lins)\n",
    "        for i,conv in enumerate(self.convs):\n",
    "            x_dict = conv(x_dict , edge_index_dict)\n",
    "            x_dict = {key: F.dropout(x , p = .75 , training = self.training ) for key, x in x_dict.items()}\n",
    "\n",
    "            for key, x in x_dict.items():\n",
    "                x_dict[key] = next(lins)(x)\n",
    "        lins2 = iter(self.lins2)\n",
    "        \n",
    "        for key, x in x_dict.items():\n",
    "            x_dict[key] =  next(lins2)(x)\n",
    "        \n",
    "        return {key: F.tanh(x) for key, x in x_dict.items()}\n",
    "        #return x_dict\n",
    "\n",
    "model = HeteroGCN(hidden_channels=100 , out_channels=1, num_layers=5)\n",
    "model = model.double()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,chains in enumerate(subthresh_connected['7DZW']):\n",
    "    AA_tensor , NT_tensor = structmats['7DZW'][chains]\n",
    "    print(AA_tensor.shape)\n",
    "    print(NT_tensor.shape)\n",
    "    intra = len(chains)>1\n",
    "    pairs = list(allpairs['7DZW'][chains])\n",
    "    contact_gen = create_data_updown_transitions(intra ,pairs, AA_tensor, NT_tensor, sectormat, connectmat_up, connectmat_down, connectmat_diag, template_features , posi_percent = .5 ,  q = None , iolock= None,  verbose = True, loop= True  )\n",
    "    data = next(contact_gen)\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testgen = False\n",
    "traingen = False\n",
    "reload = True\n",
    "\n",
    "def gen_dataset(data_name , structmats , allpairs ,  reload = True  ):\n",
    "    pairsample = 100\n",
    "    print('writing dataset to' , data_name)\n",
    "    if reload == True:\n",
    "        with open(data_name, 'rb' ) as datain:\n",
    "            samples =pickle.loads( datain.read() )\n",
    "    else:\n",
    "        samples = []\n",
    "    for struct in subthresh_connected:\n",
    "        for i,chains in enumerate(subthresh_connected[struct]):\n",
    "            AA_tensor , NT_tensor = structmats[struct][chains]\n",
    "            intra = len(chains)>1\n",
    "            pairs = list(allpairs[struct][chains])\n",
    "            contact_gen = create_data_updown_transitions(intra ,pairs, AA_tensor, NT_tensor, sectormat, connectmat_up, connectmat_down, \n",
    "                    connectmat_diag, template_features , posi_percent = .5 ,  q = None , iolock= None,  verbose = False, loop= True  )\n",
    "            for data in contact_gen:\n",
    "                samples.append(data)\n",
    "                if len(samples) < 10:\n",
    "                    print(data)\n",
    "                if len(samples)%100==0 and len(samples)> 0:\n",
    "                    print(len(samples))\n",
    "                    with open(data_name, 'wb' ) as datain:\n",
    "                        datain.write(pickle.dumps(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if traingen == True:\n",
    "    trainfile = alnfile + 'train.pkl'\n",
    "    gen_dataset(trainfile , structmats , allpairs ,  reload  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testgen == True:\n",
    "    testfile = alnfile + 'test.pkl'\n",
    "    gen_dataset(testfile , structmats , allpairs ,  reload )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    data = data.to(device)\n",
    "    out = model(data.x_dict , data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(alnfile + 'train.pkl', 'rb') as trainin:\n",
    "    trainingdata = pickle.loads(trainin.read())\n",
    "\n",
    "#with open(alnfile + 'test.pkl') as trainin:\n",
    "#    testingdata = pickle.loads(trainin.read()\n",
    "\n",
    "trainloader = DataLoader( trainingdata , batch_size = 10 , shuffle=True)\n",
    "#testloader = DataLoader(testingdata , batch_size = 50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "lastauc = 0\n",
    "epochs = 1000\n",
    "#iterate over structs\n",
    "model = model.to(device)\n",
    "\n",
    "def calc_metrics(truths,preds,label = ''):\n",
    "    truth = np.hstack(truths)\n",
    "    predy = np.hstack(preds)\n",
    "    print(truth.shape, predy.shape)\n",
    "    fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "    auc_s = auc(fpr, tpr)\n",
    "    print('train node auc sector',auc_s)\n",
    "    truth_n = np.hstack(truths_n)\n",
    "    predy_n = np.hstack(preds_n)\n",
    "    fpr, tpr, _ = roc_curve(  truth_n  ,predy_n )\n",
    "    auc_n = auc(fpr, tpr)\n",
    "    print('train node auc',auc_n)\n",
    "        \n",
    "        \n",
    "with warnings.catch_warnings():\n",
    "    model.train()\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    #iterate over chain pairs\n",
    "    #create a chunk generator for a chain pair\n",
    "    #iterate over graph chunks\n",
    "    truths = []\n",
    "    preds = []\n",
    "    truths_n = []\n",
    "    preds_n = []\n",
    "\n",
    "    losses1 =[]\n",
    "    losses2 = []\n",
    "    losses3 = []\n",
    "    for e in range(epochs ) :\n",
    "        trainloader = DataLoader( trainingdata , batch_size = 10 , shuffle=True)\n",
    "\n",
    "        for i,data in enumerate(trainloader):\n",
    "            data = data.to(device)            \n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            optimizer.zero_grad()\n",
    "            loss1 =  F.mse_loss(out['phylonodes_up'], data['phylonodes_up'].y.double())\n",
    "            loss2 =  F.mse_loss(out['phylonodes_down'], data['phylonodes_down'].y.double())\n",
    "            loss3 =  F.mse_loss(out['sectornode'], data['sectornode'].y.double())\n",
    "            loss = loss1 + loss2+ loss3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses3.append(float(loss3.to('cpu')))\n",
    "            losses2.append(float(loss2.to('cpu')))\n",
    "            losses1.append(float(loss1.to('cpu')))\n",
    "\n",
    "            truth = data['sectornode']['y'][:].to('cpu').detach().numpy()\n",
    "            predy =  out['sectornode'][:].to('cpu').detach().numpy()\n",
    "            #truth_n = data['phylonodes_down']['y'][:,0].to('cpu').detach().numpy()\n",
    "            #pred_n =  out['phylonodes_down'][:,0].to('cpu').detach().numpy()\n",
    "            #truths_n.append(truth_n)\n",
    "            #preds_n.append(pred_n)\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)            \n",
    "            if i % 100 == 0:\n",
    "                print('losses', np.mean(losses1), np.mean(losses2), np.mean(losses3) )\n",
    "                print(i)\n",
    "                losses1 =[]\n",
    "                losses2 = []\n",
    "                losses3 = []\n",
    "\n",
    "        #calc_metrics(truths,preds,label = 'sectornode')\n",
    "            #calc_metrics(truths_n,preds_n,label = 'phylonodes')\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        truths = []\n",
    "        preds = []\n",
    "        losses3 = []\n",
    "        for i,data in enumerate(trainloader):\n",
    "            data = data.to(device)\n",
    "            out = model(data.x_dict ,data.edge_index_dict)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss3 =  F.smooth_l1_loss(out['godnode'].double(), data['godnode'].y.double())\n",
    "            loss = loss3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses3.append(float(loss3.to('cpu')))\n",
    "            truth = data['sectornode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  out['sectornode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_n = auc(fpr, tpr)\n",
    "        print('losses', np.mean(losses1), np.mean(losses2), np.mean(losses3) )\n",
    "        print('train auc', auc_n)\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        \n",
    "        truths = []\n",
    "        preds = []\n",
    "\n",
    "        truths_n = []\n",
    "        preds_n = []\n",
    "        \n",
    "        \n",
    "        for i,testdata in enumerate(testloader):\n",
    "            testdata = testdata.to(device)\n",
    "            pred = model(testdata.x_dict ,testdata.edge_index_dict)\n",
    "            truth = testdata['godnode']['y'][:,0].to('cpu').detach().numpy()\n",
    "            predy =  pred['godnode'][:,0].to('cpu').detach().numpy()\n",
    "            truths.append(truth)\n",
    "            preds.append(predy)\n",
    "\n",
    "            truth_n = testdata['phylonodes_down']['y'][:,0].to('cpu').detach().numpy()\n",
    "            pred_n =  pred['phylonodes_down'][:,0].to('cpu').detach().numpy()\n",
    "\n",
    "            truths_n.append(truth_n)\n",
    "            preds_n.append(pred_n)\n",
    "\n",
    "        truth = np.hstack(truths)\n",
    "        predy = np.hstack(preds)\n",
    "        fpr, tpr, _ = roc_curve(  truth  ,predy )\n",
    "        auc_gn = auc(fpr, tpr)\n",
    "        print('test auc',auc_gn)\n",
    "        truth_n = np.hstack(truths_n)\n",
    "        predy_n = np.hstack(preds_n)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(  truth_n  ,predy_n )\n",
    "        auc_n = auc(fpr, tpr)        \n",
    "        print('test node auc',auc_n)\n",
    "        if auc_gn > lastauc:\n",
    "            lastauc = auc_gn\n",
    "            print('saving')\n",
    "            torch.save(model, './phylographnet_job_final50.torch')\n",
    "            print('done')\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #load subgraphs\n",
    "        #train_input_nodes = #######\n",
    "        #train_loader = HGTLoader(data , num_samples=[1024] * 4, shuffle=True, input_nodes=train_input_nodes, **kwargs)\n",
    "        #for trainsample in train_loader:\n",
    "            #train here\n",
    "            \n",
    "            \n",
    "            \n",
    "    #for labels in [ aglo_l]:\n",
    "    #    l,c = np.unique(labels, return_counts= True)\n",
    "        print(i)\n",
    "        print(chain)\n",
    "        #categorical-> direct, indirect or no contact\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if i == 0 :\n",
    "            total_thresh = subthresh_thresh['7DZW'][chain]\n",
    "            total_connect = subthresh_connected['7DZW'][chain]\n",
    "\n",
    "            #green, oranges, reds = struct_hits( labels, threshmat , connectmat , struct = '7DZW' , chain=chain , l=l , c=c , verbose = False , radius = 10)\n",
    "        else:\n",
    "            total_thresh += subthresh_thresh['7DZW'][chain]\n",
    "            total_connect += subthresh_connected['7DZW'][chain]\n",
    "            \n",
    "        # find nonzero\n",
    "        # zeroed entries are the complement\n",
    "        #sample each fraction randomly\n",
    "        ### testing loop ####\n",
    "        if epoch % 100 == 0:\n",
    "            ROC_curve_single(y_test, y_pred_grd)\n",
    "        \n",
    "        ### save based on performance\n",
    "        \n",
    "        \n",
    "            \n",
    "proj_greens, proj_oranges, proj_reds = struct_hits_filter( AATF, labels, total_thresh , total_connect , struct = '7DZW' , chain=chain , l=l , c=c , verbose = False , radius = 10 )\n",
    "#proj_greens, proj_oranges, proj_reds = struct_hits(  labels, total_thresh , total_connect , struct = '7DZW' , chain=chain , l=l , c=c , verbose = False , radius = 10 )\n",
    "\n",
    "print(proj_greens, proj_oranges, proj_reds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for lsh encoding\n",
    "def retcodon_mats(annotation , AAmat , plt = False):\n",
    "    positions = set([])\n",
    "    AAmat = scipy.sparse.csc_matrix(AAmat)\n",
    "    print('converted')\n",
    "    \n",
    "    slices = []\n",
    "    for i,r in annotation.iterrows():\n",
    "        if (r.qstart-1, r.qend-1) not in slices:        \n",
    "            slices.append((r.qstart-1, r.qend-1))\n",
    "    count = 0\n",
    "    for start,end in slices:\n",
    "        codonmat = np.zeros(( AAmat.shape[0] , int((end - start + 10 ) /3) ) )\n",
    "        for j,codon in enumerate(range(start-1, end-1 , 3 )):\n",
    "            cols = np.sum( AAmat[:, codon:codon+2] , axis = 1 )\n",
    "            codonmat[:,count] = cols.ravel()\n",
    "            count+=1\n",
    "            if count%500 == 0 and count >0:\n",
    "                print(count/codonmat.shape[1])\n",
    "        yield codonmat \n",
    "        \n",
    "        if plt == True:\n",
    "            print(np.sum(codonmat))\n",
    "            plt.figure(figsize=(20,20))\n",
    "            plt.spy(codonmat, markersize= .5)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHashLSHForest, WeightedMinHash\n",
    "#Ntaxa known. avoid allvsall\n",
    "from datasketch import WeightedMinHashGenerator\n",
    "\n",
    "# WeightedMinHashGenerator requires dimension as the first argument\n",
    "wmg = WeightedMinHashGenerator(connectmat.shape[0] , sample_size=512 , seed=0)\n",
    "\n",
    "#parallelise the sig generation for columns\n",
    "#compile db\n",
    "#iteratively search to make a net\n",
    "\n",
    "#verify matches with CGN\n",
    "#compute bloom filters for protein pairs\n",
    "def blur_cols( col , niter = 10 , blurmat ):\n",
    "    for i in range(niter):\n",
    "        col = blurmat.dot(col)\n",
    "    #you need a redis server running on your cluster for this to work. change your ip, port and db number accordingly\n",
    "    return col\n",
    "\n",
    "def hash_cols( col , wmg, nhashes = 512 ):\n",
    "    mh = wmg.minhash(col)\n",
    "    #you need a redis server running on your cluster for this to work. change your ip, port and db number accordingly\n",
    "    return mh.digest()\n",
    "\n",
    "#transform dask array\n",
    "AAmat_dask = da.from_array(AAmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur each col w graph laplacian\n",
    "AAmat_dask = AAmat_dask.blur_cols(func1d, axis=1 )\n",
    "\n",
    "#return the hash sig of each col\n",
    "hashsigs = AAmat_dask.hash_cols(func1d, axis=1  )\n",
    "\n",
    "\n",
    "#input each sig into the lsh forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
