{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import *\n",
    "import scipy\n",
    "import copy\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from matplotlib import pyplot as plt\n",
    "from Bio import AlignIO\n",
    "import random\n",
    "import pickle\n",
    "import h5py\n",
    "import itertools\n",
    "import dendropy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import seaborn as sns\n",
    "jk_iterations = 50\n",
    "mapping = {'A':0 , 'T':1 , 'C':2 ,'G':3 }\n",
    "os.environ['MKL_ENABLE_INSTRUCTIONS'] = 'AVX2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16s rna dataset from 'Detecting the Coevolution of Biosequencesâ€”An Example of RNA Interaction Prediction' \n",
    "import glob\n",
    "#treefile = './16s/16s_wstruct.aln.fasta.treefile'\n",
    "#alnfile = './16s/16s_wstruct.aln.fasta'\n",
    "\n",
    "treefile = './16s/16s_salaminWstruct_aln.fasta.treefile'\n",
    "\n",
    "alnfile = './16s/16s_salaminWstruct_aln.fasta'\n",
    "\n",
    "events = './16s/16s_salaminWstruct_aln.fastasparsemat_AAtransitionbootstrap_runbootstrap_run_transition_*_coevmat.pkl'\n",
    "eventmats = glob.glob(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(eventmats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3783, 141)\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(alnfile +'.h5'):\n",
    "    with h5py.File(alnfile +'.h5', 'r') as hf:\n",
    "        align_array = hf['MSA2array'][:]\n",
    "print(align_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes 282\n",
      "length 49.026590402199986\n"
     ]
    }
   ],
   "source": [
    "tree = dendropy.Tree.get(\n",
    "    path=treefile,\n",
    "    schema='newick')\n",
    "treelen = tree.length()\n",
    "treenodes = len(tree.nodes())\n",
    "print('nodes',treenodes)\n",
    "print('length',treelen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.1 0.1 0.1]\n",
      " [0.1 0.7 0.1 0.1]\n",
      " [0.1 0.1 0.7 0.1]\n",
      " [0.1 0.1 0.1 0.7]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAULklEQVR4nO3df/BldX3f8eeLhYWSxY0GSwzLzwijRByQLdjmh1gwhSQDMdoWbCs61u20ITZN64SMHY1kmo5p1LEtrVkNptoxpDFNsq1bsYkSTaq4ayOWRSHrYmQh/gyBIAbY7/fdP+5duH5nv/fc737vPffc830+mDN77vnxOW/OMG8++zmfH6kqJEntOGbeAUjSRmLSlaQWmXQlqUUmXUlqkUlXklpk0pWkFpl0JWkVSW5O8tUkd65yPkn+fZL9ST6b5AVNZZp0JWl1vwZcMeb8lcA5w20H8J+bCjTpStIqqupjwJ+PueRq4L018EngO5M8a1yZx04zwCM+YPOpDnmTxvjWAx+fdwidcdzJZ2e9ZTzx9QMT55zNz/zef8KghnrYzqrauYbHnQrcN/L74PDYn612w8yTriS1anlp4kuHCXYtSXbdTLqS+qWW23za/cBpI7+3DY+tyjZdSf2yvDz5tn67gFcOezG8EHioqlZtWgBrupJ6pqZY003y68ClwMlJDgJvAo4bPKfeCewGfgTYDzwKvLqpTJOupH5ZOjS1oqrq2obzBfzkWso06UrqlzV8SJsHk66kfmn3Q9qamXQl9ct0PpDNjElXUq9M80PaLDQm3STPYTDU7dThofuBXVX1uVkGJklHpeM13bH9dJP8LHALEOBTwy3Arye5Ycx9O5LsTbJ3efmb04xXksZbemLybQ4ybjXgJPcA31dVT6w4vhnYV1XnND3AuRek8Zx74SnTmHvhsc99dOKcc/xzX7zu561VU/PCMvA9wJ+uOP6s4TlJ6paONy80Jd2fBn4/yZ/w1Ew6pwPPBq6fYVySdHQW+UNaVX0oybnAxXz7h7Q9VdXtHsiSNqYFr+lSg/4Xn2whFklat1qezweySdlPV1K/LHpNV5IWyiK36UrSwnHCG0lqkTVdSWrRRm/TPXvr2NWIpQ3vsV/86XmH0BnHvW3X+guZ4iTms2BNV1K/bPSariS1qevjtky6kvrFmq4ktcjeC5LUImu6ktQiey9IUotsXpCkFtm8IEktMulKUos63rwwdjXgcZK8esy5J1cDfuivvna0j5CktVs6NPk2B0eddIE3r3aiqnZW1faq2r71hGeu4xGStEbLy5NvczC2eSHJZ1c7BZwy/XAkaZ063rzQ1KZ7CvB3gAdXHA/wf2YSkSStx4J/SPufwJaq+szKE0lum0VAkrQui5x0q+o1Y869YvrhSNI6Vc07grHsMiapXw45DFiS2tPxD2nr6TImSd0zxS5jSa5IcneS/UluOML505N8NMkfJ/lskh9pKtOkK6lfqibfxkiyCbgJuBI4D7g2yXkrLvvXwH+rqguBa4D/1BSezQuS+mV6vRcuBvZX1QGAJLcAVwN3jVxTwNOG+1uBB5oKnXnSPWXz1lk/Qlpo9/52t7+2t+n8t02hkDUk3SQ7gB0jh3ZW1c7h/qnAfSPnDgKXrCji54EPJ/kp4DuAy5ueaU1XUq/U0uQLUw4T7M7GC1d3LfBrVfXWJH8TeF+S51Wt/jXPpCupX6bXvHA/cNrI723DY6NeA1wBUFWfSHICcDLw1dUK9UOapH6p5cm38fYA5yQ5K8lmBh/Kdq245kvAZQBJngucAIydWtGarqR+WZ5OG3lVHUpyPXArsAm4uar2JbkR2FtVu4B/Cbwryb9g8FHtVVXju0WYdCX1yxTnXqiq3cDuFcfeOLJ/F/D9aynTpCupX9bwIW0eTLqS+mWRZxmTpIUzpTbdWTHpSuqXRZ/wJslzklyWZMuK41fMLixJOkrLNfk2B2OTbpLXAb8L/BRwZ5KrR07/4pj7nlwN+MvfXNmXWJJmp5aXJ97moal54bXARVX1SJIzgQ8kObOq3sFgnbQjGh1a94OnXtbtBhZJ/bLgvReOqapHAKrqi0kuZZB4z2BM0pWkuen4h7SmNt2vJLng8I9hAv4xBmOLz59hXJJ0dKY4ifksNNV0Xwl824JDVXUIeGWSX5lZVJJ0tDpe021aDfjgmHN/NP1wJGmdOt5lzH66kvplkWu6krRo6tBi916QpMViTVeSWmSbriS1aKPXdLduOmHWj5AW2p2PPa35og1iGp3/a6MnXUlqlR/SJKlF1nQlqUUmXUlqT8NivHNn0pXUL9Z0JalFJl1Jak8dcnCEJLWn2zm3OekmuRioqtqT5DzgCuDzVbV75tFJ0hot9OCIJG8CrgSOTfK/gUuAjwI3JLmwqv7NKvftAHYAnP/053H6ltOnG7UkrWaRky7wcuAC4Hjgy8C2qno4yS8DtwNHTLqjC1P+2Ok/2u03IKlfFrx54VBVLQGPJvlCVT0MUFXfStLxfzVJG9FCNy8Ajyc5saoeBS46fDDJVjr//xNJG1EdWuyk+0NV9RhA1bdNUnkccN3MopKko9Xx6mDTwpSPrXL868DXZxKRJK1Dx+cwt5+upJ4x6UpSe7pe0z1m3gFI0jTVocm3JkmuSHJ3kv1Jbljlmr+X5K4k+5K8v6lMa7qSemVaNd0km4CbgJcAB4E9SXZV1V0j15wD/Bzw/VX1YJK/3lSuNV1JvVLLk28NLgb2V9WBqnocuAW4esU1rwVuqqoHAarqq02Fzryme0I2zfoR0kK777jMO4R+qcnf5+iUBUM7hyNqAU4F7hs5d5DBVAijzh2W80fAJuDnq+pD455p84KkXllL88LolAVH6VjgHOBSYBvwsSTnV9VfjLtBknqjlqf2N4f7gdNGfm8bHht1ELi9qp4A7k1yD4MkvGe1Qm3TldQry0uZeGuwBzgnyVlJNgPXALtWXPM7DGq5JDmZQXPDgXGFWtOV1CvT6r1QVYeSXA/cyqC99uaq2pfkRmBvVe0anvvhJHcBS8Drq+ob48o16UrqlSk2LzBcrGH3imNvHNkv4GeG20RMupJ6peMrsJt0JfXLNGu6s2DSldQrE3wgmyuTrqRe6XpNd81dxpK8dxaBSNI0VGXibR6aVgNe2SctwIuTfCdAVV21yn1PDq278BnP5+wtZ6w/UkmaQNendmxqXtgG3AW8GygGSXc78NZxN40OrXv5GVd1/FuipD5ZnlMNdlJNzQvbgU8DbwAeqqrbgG9V1R9U1R/MOjhJWquFbl4YLkb59iS/OfzzK033SNI89aL3QlUdBP5ukh8FHp5tSJJ09Lree2FNtdaq+iDwwRnFIknr1vU2XZsKJPXKvNpqJ2XSldQrzr0gSS2yeUGSWrTcpw9pktR1G76muxlXA5bGeTAdH7e6YPyQJkkt2vA1XUlqU8c7L5h0JfXL0nK3Fzk36Urqla63kJt0JfVKYZuuJLVmueONuiZdSb2ybE1Xktpj84IktWipT0k3yQ8AFwN3VtWHZxOSJB29rvdeGNuhLcmnRvZfC/xH4CTgTUluGHPfjiR7k+zd/8gXpxWrJDVaXsM2D029iI8b2d8BvKSq3gz8MPAPVrupqnZW1faq2v7sLWeuP0pJmlCRibd5aGpeOCbJ0xkk51TV1wCq6ptJDs08Oklao47P7NiYdLcyWII9QCV5VlX9WZItw2OS1CkL3WWsqs5c5dQy8NKpRyNJ67Q07wAaHFWXsap6FLh3yrFI0rotZ4FrupK0aDo+CtikK6lfFrqfriQtmuVMvjVJckWSu5Psbxib8LIklWR7U5nWdCX1yrSGASfZBNwEvAQ4COxJsquq7lpx3UnAPwdun6Rca7qSemWKNd2Lgf1VdaCqHgduAa4+wnW/ALwF+KtJ4pt5TXdTx/vMSfP2aOdbIRfLWt5mkh0MRtsetrOqdg73TwXuGzl3ELhkxf0vAE6rqg8mef0kz7R5QVKvrKX3wjDB7my88AiSHAO8DXjVWu4z6UrqlSkOA74fOG3k97bhscNOAp4H3JZB3+DvBnYluaqq9q5WqElXUq9MsbFmD3BOkrMYJNtrgFccPllVDwEnH/6d5DbgX41LuGDSldQzS1Oq6VbVoSTXA7cCm4Cbq2pfkhuBvVW162jKNelK6pVpfpasqt3A7hXH3rjKtZdOUqZJV1KvdL0viElXUq8494IktWjRJzGXpIXS9eaFpoUpL0nytOH+X0vy5iT/I8lbkmxtJ0RJmtzSGrZ5aJp74Wbg0eH+Oxgs3/OW4bH3rHbT6GrAf/KIc51Las80ZxmbhcaFKavq8AKU26vqBcP9P0zymdVuGh1a94/O+Imut2tL6pGFbl4A7kzy6uH+HYfnikxyLvDETCOTpKNQa9jmoSnp/mPgRUm+AJwHfCLJAeBdw3OS1CnL1MTbPDStBvwQ8Krhx7SzhtcfrKqvtBGcJK1VL1YDrqqHgTtmHIskrVvX23TtpyupVxwcIUktmldb7aRMupJ6pdsp16QrqWds05WkFi11vK4786R7TDreqi3N2ROdr5stlq6/TWu6knrFD2mS1KJup1yTrqSesXlBklq04T+kSVKbbNOVpBZ1O+WadCX1jDVdSWpR1z+kNS1M+bokp7UVjCStV63hn3loWjniF4Dbk3w8yT9L8sxJCh1dmPKev3RhSkntWaIm3uahKekeALYxSL4XAXcl+VCS65KctNpNVbWzqrZX1fZzTzpriuFK0njLa9jmoalNt6pqGfgw8OEkxwFXAtcCvwxMVPOVpLYs12J/SPu22Wqq6glgF7AryYkzi0qSjlK3U25z0v37q52oqkenHIskrdtCdxmrqnvaCkSSpmFevRImZT9dSb1yyKQrSe3pek23qcuYJC2UaXYZS3JFkruT7E9ywxHO/0ySu5J8NsnvJzmjqUyTrqReqaqJt3GSbAJuYtBN9jzg2iTnrbjsj4HtVfV84APALzXFZ9KV1CvL1MRbg4uB/VV1oKoeB24Brh69oKo+OtKT65MMBpONZZuuNGddn3R70azlfSbZAewYObSzqnYO908F7hs5dxC4ZExxrwH+V9MzTbqSemUt/XSHCXZn44UNkvxDYDvwoqZrTbqSeqWprXYN7gdGZ1ncNjz2bZJcDrwBeFFVPdZUqG26knplir0X9gDnJDkryWbgGgbTIDwpyYXArwBXVdVXJ4nPmq6kXplWP92qOpTkeuBWYBNwc1XtS3IjsLeqdgH/DtgC/GYSgC9V1VXjyjXpSuqVac69UFW7gd0rjr1xZP/ytZZp0pXUK0vV7QV7TLqSeqXrw4BNupJ6ZdEnMZekhdLtlNuQdEe6STxQVb+X5BXA3wI+x2DkxhMtxChJE1voScyB9wyvOTHJdQy6Rvx34DIG45KvO9JNo0PrXviMC3FxSkltWfSke35VPT/JsQxGYnxPVS0l+a/AHavdNDq07rozX9btNyCpVxa998IxwyaG7wBOBLYCfw4cDxw349gkac0WvffCrwKfZzAa4w0MRl0cAF7IYJozSeqUKc69MBNNC1O+PclvDPcfSPJe4HLgXVX1qTYClKS1WPQ2XarqgZH9v2AwO7okddJC13QladEsTbT62fyYdCX1iiPSJKlFi957QZIWijVdSWrRhq/puh6QNJ6rAU+XNV1JatGiDwOWpIWy4ZsXJKlNZU1Xktqz8MOAJWmROAxYklpkTVeSWrS0bJuuJLXG3guS1KKFb9NNcjbwE8BpwBJwD/D+qnp4xrFJ0pp1vU137CjdJK8D3gmcAPwNBmujnQZ8MsmlY+7bkWRvkr13/+W904tWkhpU1cTbPGTcg5P8P+CC4QrAJwK7q+rSJKcDv1tVFzY94NWuBiyNtYnMO4TOePcXP7Dul/H0Lc+eOOc8+Mj+1l/+JG26xzJoVjge2AJQVV9K4mrAkjqn680LTUn33cCeJLcDPwi8BSDJMxksxS5JnbLQH9Kq6h1Jfg94LvDWqvr88PjXgB9qIT5JWpOFn9qxqvYB+1qIRZLWzX66ktSiha/pStIiWe741I6upiOpV6bZTzfJFUnuTrI/yQ1HOH98kt8Ynr89yZlNZZp0JfXKtJJukk3ATcCVwHnAtUnOW3HZa4AHq+rZwNsZ9vAax6QrqVdqDVuDi4H9VXWgqh4HbgGuXnHN1cB/Ge5/ALgsydgBFzNv033PF3+rE8Ntkuyoqp3zjqMLfBdP8V08pS/v4tDj90+cc5LsAHaMHNo58g5OBe4bOXcQuGRFEU9eU1WHkjwEfBfw9dWeuZFqujuaL9kwfBdP8V08ZcO9i6raWVXbR7aZ/09nIyVdSVqL+xlM8HXYtuGxI16T5FhgK/CNcYWadCXpyPYA5yQ5K8lm4Bpg14prdgHXDfdfDnykGr7QbaR+ugvfVjVFvoun+C6e4rsYMWyjvR64FdgE3FxV+5LcCOytql3ArwLvS7KfwXw01zSVO3ZqR0nSdNm8IEktMulKUot6n3ST/HiSSvKceccyT0m+K8lnhtuXk9w/8nvzvONrW5LvTnJLki8k+XSS3UnOnXdcbUtySpL3JzkwfA+fSPLSecfVZ71PusC1wB8O/9ywquobVXVBVV3AYN27tx/+PRxts2EMRwz9NnBbVX1vVV0E/Bxwynwja9fwPfwO8LGqOnv4Hq5h0DVKM9LrpJtkC/ADDMZHN35V1IbxYuCJqnrn4QNVdUdVfXyOMc3D3wYeX/Ee/rSq/sMcY+q9XiddBuOiP1RV9wDfSHLRvANSJzwP+PS8g+iA7wP+77yD2Gj6nnSvZTBJBcM/N3QTgzROkpuS3JFkz7xj6bPeDo5I8gwGf306P0kx6NxcSV7fNGJEvbePweihjW4f8LLDP6rqJ5OcDOydX0j91+ea7suB91XVGVV1ZlWdBtzLYFVjbWwfAY4fzjAFQJLnJ9lo/218BDghyT8dOXbivILZKPqcdK9l8IV61G9hE8OGN/ybzkuBy4ddxvYB/xb48nwja9fwPfw48KIk9yb5FIO5YX92roH1nMOAJalFfa7pSlLnmHQlqUUmXUlqkUlXklpk0pWkFpl0JalFJl1JatH/B7Vf2dmA4G0wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#toy jukes cantor model\n",
    "state0 = np.array([0,0,0,1])\n",
    "r = .1\n",
    "jk = r*np.ones((4,4))\n",
    "np.fill_diagonal(jk,0)\n",
    "for i in range(jk.shape[0]):\n",
    "    jk[i,i] = 1-np.sum(jk[i,:])\n",
    "\n",
    "sns.heatmap(jk , xticklabels= mapping.keys( ), yticklabels= mapping.keys( ))\n",
    "plt.show()\n",
    "probas = []\n",
    "for i in range(10):\n",
    "    probas.append(state0)\n",
    "    state0 = np.matmul(state0,jk)\n",
    "sns.heatmap(np.vstack(probas),xticklabels= mapping.keys( ))\n",
    "\n",
    "print(jk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.1 0.1 0.1]\n",
      " [0.1 0.7 0.1 0.1]\n",
      " [0.1 0.1 0.7 0.1]\n",
      " [0.1 0.1 0.1 0.7]]\n",
      "[[ 0.35667494 -0.11889165 -0.11889165 -0.11889165]\n",
      " [-0.11889165  0.35667494 -0.11889165 -0.11889165]\n",
      " [-0.11889165 -0.11889165  0.35667494 -0.11889165]\n",
      " [-0.11889165 -0.11889165 -0.11889165  0.35667494]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAD4CAYAAADfPUyRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPGklEQVR4nO3df4xlZX3H8fdnF1AKjf1hQ2F3Kxi3NVs1GOnapAEbxbjYxvUPTYBUaUM6adJtNf5TEhsaSdpYTWz6B3+4URJtqlt/NHFjthJKaYpt0d1aqi5I2W5aWfBHqFpEUZi53/4xF/YymZ25s/cM57ln3q/NCfeec+ac594Mn3nyPc95TqoKSVI7tvXdAEnSsxnMktQYg1mSGmMwS1JjDGZJasw5m32Cpx496bCPsfMvubLvJkhNW3zy4cx6jI1kzrkvfPHM59sMmx7MkvScGi313YKZGcyShqVGfbdgZgazpGEZGcyS1JSyxyxJjVla7LsFMzOYJQ2LF/8kqTGWMiSpMV78k6S2ePFPklpjj1mSGrP0VN8tmJnBLGlYLGVIUmMsZUhSY+wxS1Jj7DFLUltq5MU/SWqLPWZJaow1ZklqjJMYSVJj7DFLUmOsMUtSYwYwUf62vhsgSZ0ajaZf1pFkX5IHkpxIctMq238vyVeS3Jvk80n2dPERDGZJg1K1NPWyliTbgVuBa4A9wHWrBO/HqurlVXU58D7gA118BksZkoaluxrzXuBEVZ0ESHII2A/c9/QOVfXYxP4XANXFiQ1mScOygVEZSRaAhYlVB6vq4Pj1DuChiW2ngFevcozfB94FnAe8dqPNXc26wZzkpSz/ldgxXvUwcLiq7u+iAZLUqQ30mMchfHDdHdc+xq3ArUmuB/4YuGGW48E6NeYkfwQcAgJ8cbwE+PhqhfCJn1tIcizJsQ999OOztlGSpre0OP2ytoeBXRPvd47Xnckh4M2zNX7Zej3mG4FfrqpnzQqS5APAceC9q/3Q5F+hpx492UnNRZKm0t0NJkeB3UkuYzmQrwWun9whye6qenD89jeAB+nAesE8Ai4B/mfF+ovH2ySpLR1d/KuqxSQHgNuB7cBtVXU8yS3Asao6DBxIcjXwFPBdOihjwPrB/E7gziQPcroI/gvAS4ADXTRAkjrV4Z1/VXUEOLJi3c0Tr9/R2ckmrBnMVfW5JL/I8rCRyYt/R2u9QYCS1IetMFdGVY2Ae56DtkjS7AZwS7bjmCUNi5MYSVJjtkIpQ5Lmij1mSWqMwSxJjan5v6fNYJY0LIuOypCktnjxT5IaY41ZkhpjjVmSGmOPWZIaYzBLUltqaf7nVzOYJQ2LPWZJaozD5SSpMSNHZUhSWyxlSFJjvPgnSY2xxyxJjbHGLEmNcVSGJDXGHvP6zr/kys0+xdx44pG7+25CM/y90GYpa8yS1BhHZUhSYyxlSFJjLGVIUmPsMUtSYxwuJ0mNsccsSW2pRUdlSFJb7DFLUmOsMUtSY+wxS1JbymCWpMZ48U+SGjOAHvO2vhsgSZ0a1fTLOpLsS/JAkhNJblpl+7uS3Jfky0nuTPKiLj6CwSxpUKpq6mUtSbYDtwLXAHuA65LsWbHbvwNXVNUrgE8B7+viMxjMkoalux7zXuBEVZ2sqieBQ8D+yR2q6q6q+uH47T3Azi4+gsEsaVg2EMxJFpIcm1gWJo60A3ho4v2p8bozuRH4uy4+ghf/JA1KLU5/g0lVHQQOznrOJL8FXAG8ZtZjgcEsaWi6u/HvYWDXxPud43XPkuRq4N3Aa6rqx12c2GCWNCgd3mByFNid5DKWA/la4PrJHZK8EvggsK+qvt3ViQ1mScPSUTBX1WKSA8DtwHbgtqo6nuQW4FhVHQbeD1wIfDIJwNer6k2znttgljQsHc5hVFVHgCMr1t088frq7s52msEsaVCcK0OSGlOLBrMktWX+p2M2mCUNywDmyT/7O/+S/M4a2565m2Y0+sHZnkKSNm60gaVRs9yS/Z4zbaiqg1V1RVVdsW3bBTOcQpI2pkbTL61as5SR5Mtn2gRc1H1zJGk2tdh3C2a3Xo35IuANwHdXrA/wL5vSIkmaQcs94WmtF8yfBS6sqntXbkjyj5vRIEmaxeCDuapuXGPb9WfaJkm9qfTdgpk5XE7SoAy+xyxJ86ZG9pglqSmjJYNZkppiKUOSGmMpQ5IaU/M/uZzBLGlY7DFLUmO8+CdJjbHHLEmNKe/8k6S2OFxOkhozsscsSW2xlCFJjXFUhiQ1xlEZktQYa8yS1BhrzJLUGOfKkKTGWMqQpMaMvPgnSW2xx6wNOf+SK/tuQjOeeOTuvpvQDH8vuuXFP0lqjD1mSWrMAAZlGMyShmVptK3vJszMYJY0KAOY9dNgljQsxfzXmOe/zy9JE0Y1/bKeJPuSPJDkRJKbVtl+VZIvJVlM8pauPoPBLGlQRmTqZS1JtgO3AtcAe4DrkuxZsdvXgd8GPtblZ7CUIWlQOixl7AVOVNVJgCSHgP3Afc+cq+q/x9s6LW3bY5Y0KEtk6iXJQpJjE8vCxKF2AA9NvD81Xrfp7DFLGpSNdF2r6iBwcLPacrYMZkmD0mFN4WFg18T7neN1m85ShqRBKTL1so6jwO4klyU5D7gWOLzpHwCDWdLAjDL9spaqWgQOALcD9wOfqKrjSW5J8iaAJL+S5BTwVuCDSY538RksZUgalPWGwW1EVR0BjqxYd/PE66Mslzg6ZTBLGpSlvhvQAYNZ0qCMMv+3ZBvMkgbFaT8lqTHOLidJjRnAs1gNZknDsjSAaT8NZkmDYo9ZkhpjjVmSGuOoDElqjKUMSWrMEEoZ605ilOSlSV6X5MIV6/dtXrMk6ewsZfqlVWsGc5I/BD4D/AHw1ST7Jzb/2Ro/98xTAUajH3TTUkmawmgDS6vWK2X8LvCqqno8yaXAp5JcWlV/CWceLDj5VIBzztsxhFq8pDnRcuBOa71g3lZVj8PyQweT/DrL4fwi1ghmSerLEHqC69WYv5Xk8qffjEP6N4EXAi/fxHZJ0lnpaqL8Pq0XzG8Hvjm5oqoWq+rtwFWb1ipJOkuDrzFX1ak1tv1z982RpNk4Ub4kNablEsW0DGZJg9JyiWJaBrOkQRnCqAyDWdKgjAYQzQazpEHx4p8kNcYasyQ1xlEZktQYa8yS1Jj5j2WDWdLAWGOWpMYsDaDPbDBLGhR7zJLUGC/+SVJj5j+WDWZJA2MpQ5Ia48U/SWqMNWZJasz8x/L6z/yTpLkyoqZe1pNkX5IHkpxIctMq25+X5G/G27+Q5NIuPoPBLGlQunoYa5LtwK3ANcAe4Loke1bsdiPw3ap6CfAXwJ938RkMZkmDUhv4t469wImqOllVTwKHgP0r9tkPfGT8+lPA65LMPL+dNWb14vxLruy7Cc144pG7+27CoGxkVEaSBWBhYtXBqjo4fr0DeGhi2yng1SsO8cw+VbWY5P+AnwUe3WCzn8VgljQoGxnHPA7hg+vu+BwzmCUNyqg6G5fxMLBr4v3O8brV9jmV5BzgBcD/znpia8ySBqU2sKzjKLA7yWVJzgOuBQ6v2OcwcMP49VuAf6ia/S+DPWZJg9LVDSbjmvEB4HZgO3BbVR1PcgtwrKoOAx8G/irJCeA7LIf3zAxmSYMyxWiL6Y9VdQQ4smLdzROvfwS8tbMTjhnMkgZlcQD3/hnMkgalyx5zXwxmSYPitJ+S1JgOBkX0zmCWNChO+ylJjXGifElqjD1mSWqMNWZJaoyjMiSpMY5jlqTGWGOWpMYs1fwXMwxmSYNiKUOSGtPhRPm9MZglDcr8x7LBLGlgvPgnSY0xmCWpMY7KkKTGOCpDkhrjXBmS1JgtUWNOsheoqjqaZA+wD/ja+OmxktSUwfeYk/wJcA1wTpI7gFcDdwE3JXllVf3pGX5uAVgAyPYXsG3bBd22WpLOYGkA88tlrb8uSb4CXA48D/gmsLOqHktyPvCFqnrFeic457wd8//nS9pETzxyd99NaMa5L3xxZj3Gyy761akz56vfumfm822G9UoZi1W1BPwwyX9V1WMAVfVEkvn/syRpcLbCqIwnk/xEVf0QeNXTK5O8gGHMRy1pYLbCXBlXVdWPAaqeNWr7XOCGTWuVJJ2lwfeYnw7lVdY/Cjy6KS2SpBlshR6zJM0Vb8mWpMYMvpQhSfOm7DFLUlu2xC3ZkjRPBn9LtiTNG3vMktSYpZE1ZklqiqMyJKkxQ6gxb+u7AZLUpRE19TKLJD+T5I4kD47/+9Nn2O9zSb6X5LPTHttgljQoVTX1MqObgDurajdw5/j9at4PvG0jBzaYJQ3K0mg09TKj/cBHxq8/Arx5tZ2q6k7g+xs5sMEsaVA2UspIspDk2MSysIFTXVRV3xi//iZwUVefwYt/kgZlIyWKqjoIHDzT9iR/D/z8KpveveI4laSzq44Gs6RB6XLaz6q6+kzbknwrycVV9Y0kFwPf7uq8ljIkDUpt4N+MDnP6gSE3AJ+Z9YBPM5glDcqoauplRu8FXp/kQeDq8XuSXJHkQ0/vlORu4JPA65KcSvKG9Q685lOyu+BTsqW1+ZTs07p4Svbznr9r6sz58Y8emsunZEvSXBnCnX8Gs6RBMZglqTHzH8vPQY25FUkWxmMWtzy/i9P8Lk7zu2jHVhqVsZE7eobO7+I0v4vT/C4asZWCWZLmgsEsSY3ZSsFs7ew0v4vT/C5O87toxJa5+CdJ82Ir9ZglaS4YzJLUmMEHc5J9SR5IciLJmR79siUkuS3Jt5N8te+29CnJriR3JbkvyfEk7+i7TX1J8vwkX0zyH+Pv4j19t0kDrzEn2Q78J/B64BRwFLiuqu7rtWE9SXIV8Djw0ap6Wd/t6ct47tyLq+pLSX4S+DfgzVvx9yJJgAuq6vEk5wKfB95RVff03LQtbeg95r3Aiao6WVVPAodYfk7XllRV/wR8p+929K2qvlFVXxq//j5wP7Cj31b1o5Y9Pn577ngZbm9tTgw9mHcAD028P8UW/R9Qq0tyKfBK4As9N6U3SbYnuZflJ3DcUVVb9rtoxdCDWTqjJBcCnwbeWVWP9d2evlTVUlVdDuwE9ibZsmWuVgw9mB8Gdk283zlepy1uXE/9NPDXVfW3fbenBVX1PeAuYF/PTdnyhh7MR4HdSS5Lch5wLcvP6dIWNr7g9WHg/qr6QN/t6VOSn0vyU+PX57N8ofxrvTZKww7mqloEDgC3s3yB5xNVdbzfVvUnyceBfwV+afzssRv7blNPfg14G/DaJPeOlzf23aieXAzcleTLLHdk7qiqz/bcpi1v0MPlJGkeDbrHLEnzyGCWpMYYzJLUGINZkhpjMEtSYwxmSWqMwSxJjfl/OFGnsazMJu0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#define toy continuous jk model\n",
    "print(jk)\n",
    "jump_mat = copy.deepcopy(jk)\n",
    "#define diag as exponential process\n",
    "Qmat = np.zeros(jump_mat.shape)\n",
    "#exponential\n",
    "diag = []\n",
    "for i in range(jump_mat.shape[0]):\n",
    "    Qmat[i,i] = -np.log(jump_mat[i,i])\n",
    "for i in range(jump_mat.shape[0]):\n",
    "    for j in range(jump_mat.shape[1]):\n",
    "        if i != j:\n",
    "            Qmat[i,j] = -Qmat[i,i]*jump_mat[i,j]/(np.sum( jump_mat[i,:] )-jump_mat[i,i] ) \n",
    "print(Qmat)\n",
    "sns.heatmap(Qmat)\n",
    "\n",
    "plt.show()\n",
    "w, v = LA.eig(Qmat)\n",
    "v_inv = LA.inv(v) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "[ 4.75566592e-01 -4.16333634e-17  4.75566592e-01  4.75566592e-01]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Pdt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/4055274/ipykernel_1862560/601769235.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pdt' is not defined"
     ]
    }
   ],
   "source": [
    "state0 = np.array([0,0,0,1])\n",
    "probas = []\n",
    "dt=.10\n",
    "print(v.shape)\n",
    "D = np.zeros((v.shape))\n",
    "\n",
    "np.fill_diagonal(D,w)\n",
    "\n",
    "print(w)\n",
    "\n",
    "probas.append(state0)\n",
    "print(Pdt)\n",
    "t= 0\n",
    "for i in range(20):\n",
    "    t+=dt\n",
    "    probas.append( np.matmul( state0 ,  Pt( Qmat ,dt )  ) )\n",
    "\n",
    "sns.heatmap(np.vstack(probas),xticklabels= mapping.keys( ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#align Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transitionmat = np.zeros( (4,4))\n",
    "\n",
    "#the number of actual events recorded should be number of transitions recorded / jk_iterations\n",
    "coevmats_raw = {}\n",
    "\n",
    "for outsankof in eventmats:\n",
    "    print(outsankof)\n",
    "    transition_type = outsankof.replace('./16s/16s_salaminWstruct_aln.fastasparsemat_AAtransitionbootstrap_runbootstrap_run_transition_'  , '')\n",
    "    transition_type = transition_type.replace( '_coevmat.pkl' , '')\n",
    "    transition_type = transition_type.replace(\"'\" , '')\n",
    "    \n",
    "    with open( outsankof , 'rb') as matin:\n",
    "        coevmat = pickle.loads(matin.read())[1]\n",
    "    \n",
    "    print(np.sum(coevmat))\n",
    "    row = mapping[ transition_type[0] ]\n",
    "    col = mapping[ transition_type[1] ]\n",
    "    transitionmat[row,col] = coevmat.sum()\n",
    "    transitionmat[row,col]/= jk_iterations\n",
    "    coevmat/=jk_iterations\n",
    "    coevmats_raw[transition_type] = coevmat.todense()\n",
    "\n",
    "sns.heatmap(transitionmat, xticklabels= mapping.keys( ) , yticklabels=mapping.keys())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionmat/=align_array.shape[1]*treelen\n",
    "for i in range(transitionmat.shape[1]):\n",
    "    transitionmat[i,i] = 1- np.sum(transitionmat[:,i])\n",
    "    #markov transition mat\n",
    "\n",
    "print(transitionmat)\n",
    "sns.heatmap(transitionmat, xticklabels= mapping.keys( ) , yticklabels=mapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the state changes over all branches \n",
    "#eig(transition_mat) to get p(t) = c ( exp(eig) ) c^-1\n",
    "from numpy import linalg as LA\n",
    "w, v = LA.eig(transitionmat)\n",
    "\n",
    "print(w)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_inv = LA.inv(v)\n",
    "print(v_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eigenval  mat with time = 1\n",
    "Q = np.array([ [ np.exp( w[i] ) if i==j else 0 for j in range(transitionmat.shape[1]) ] for i in range( transitionmat.shape[1])] ) \n",
    "print( Q )\n",
    "#p(t) = v * Q^t * v_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get edge length distribution\n",
    "edgelengths = []\n",
    "for node in tree.nodes():\n",
    "    if node.edge_length:\n",
    "        edgelengths.append(node.edge_length)\n",
    "plt.hist(edgelengths,15)\n",
    "plt.show()\n",
    "time_counts,time_bins = np.histogram( edgelengths , bins= 15 ) \n",
    "\n",
    "print(time_bins)\n",
    "print(time_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = []\n",
    "coevmats_raw = {}\n",
    "thresh_mats = .4\n",
    "for i,outsankof in enumerate(eventmats):\n",
    "    with open( outsankof , 'rb') as matin:\n",
    "        coevmat = pickle.loads(matin.read())[1]\n",
    "    coevmat = coevmat.todense()\n",
    "    coevmat/= jk_iterations \n",
    "    if i == 0:\n",
    "        globalmat_thresh = coevmat\n",
    "        globalmat_thresh[thresh_mats > globalmat_thresh ] = 0        \n",
    "    else:\n",
    "        coevmat[thresh_mats > coevmat] = 0\n",
    "        globalmat_thresh+= coevmat\n",
    "\n",
    "\n",
    "for i,outsankof in enumerate(eventmats):\n",
    "    with open( outsankof , 'rb') as matin:\n",
    "        coevmat = pickle.loads(matin.read())[1]\n",
    "    tensor.append( coevmat )\n",
    "    coevmat = coevmat.todense()\n",
    "    coevmat/= jk_iterations \n",
    "    if i == 0:\n",
    "        globalmat = coevmat\n",
    "    else:\n",
    "        globalmat+= coevmat\n",
    "    \n",
    "    transition_type = outsankof.replace('./16s/16s_salaminWstruct_aln.fastasparsemat_AAtransitionbootstrap_runbootstrap_run_transition_'  , '')\n",
    "    transition_type = transition_type.replace( '_coevmat.pkl' , '')\n",
    "    transition_type = transition_type.replace(\"'\" , '')\n",
    "    coevmats_raw[transition_type] = coevmat\n",
    "    \n",
    "    \n",
    "    plt.hist( coevmat[coevmat>0].flat , bins = 40 )\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.spy(coevmat, markersize= .5)\n",
    "    plt.title(outsankof)\n",
    "    plt.show()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('all mutations')\n",
    "plt.spy(globalmat,markersize= .5)\n",
    "plt.show()\n",
    "\n",
    "bigstack = np.vstack(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tree to blur eventmat\n",
    "from Bio import AlignIO\n",
    "tree = dendropy.Tree.get(\n",
    "    path=treefile,\n",
    "    schema='newick')\n",
    "col = 525\n",
    "aln_col = align_array[:,col]\n",
    "def clipID(ID):\n",
    "    return ID.replace('/',' ')\n",
    "\n",
    "msa = AlignIO.read(alnfile , format = 'fasta')    \n",
    "IDs = {i:clipID(rec.id) for i,rec in enumerate(msa)}\n",
    "#IDs = {i:rec.id for i,rec in enumerate(msa)}\n",
    "IDindex = dict(zip( IDs.values() , IDs.keys() ) )\n",
    "matsize = len(tree.nodes())\n",
    "#add chars to leaves\n",
    "for i,l in enumerate(tree.leaf_nodes()):\n",
    "    if l.taxon.label in IDindex:\n",
    "        l.taxon.label += ' '+ aln_col[IDindex[l.taxon.label]].decode()\n",
    "\n",
    "#add events\n",
    "for i,n in enumerate(tree.nodes()):\n",
    "    if globalmat[i,col]>0:\n",
    "        n.label = str( globalmat[i,col] )\n",
    "    else:\n",
    "        n.label = ''\n",
    "#vis w event proba \n",
    "tree.print_plot( show_internal_node_labels = True , width = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,n in enumerate(tree.nodes()):\n",
    "    n.matrow = i\n",
    "    n.symbols = None\n",
    "    n.scores = None\n",
    "    n.event = None\n",
    "    n.char = None\n",
    "\n",
    "matsize = len(tree.nodes())\n",
    "print(matsize)\n",
    "print('nodes')\n",
    "\n",
    "#blur w connectivity mat\n",
    "blurfactor =  .25\n",
    "bluriter = 2\n",
    "\n",
    "connectmat = scipy.sparse.csc_matrix((len(tree.nodes()), len(tree.nodes() ) ) )\n",
    "index = np.array([ [n.matrow, c.matrow ] for n in tree.nodes() for c in n.child_nodes()])\n",
    "lengths = np.array([ c.edge_length for n in tree.nodes() for c in n.child_nodes()])\n",
    "total_len = np.sum(lengths)\n",
    "#fill diagonal\n",
    "#index = np.vstack( [index , np.array([ [n.matrow, n.matrow ] for n in tree.nodes() ]) ] )\n",
    "\n",
    "connectmat[index[:,0],index[:,1]] = 1\n",
    "connectmat[index[:,1],index[:,0]] = 1\n",
    "connectmat = connectmat.todense()\n",
    "np.fill_diagonal(connectmat , 1)\n",
    "\n",
    "plt.figure( figsize=(10,10))\n",
    "plt.title( 'phylo tree connectivity matrix ' )\n",
    "plt.spy(connectmat, markersize= 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur the eventmats and find distribution of blurred jackknife\n",
    "\n",
    "#use rbm ising model to capture features\n",
    "\n",
    "vectors = []\n",
    "\n",
    "#include the scaling factor and distribution params\n",
    "jkparetto_transitions_vs_timebins = {}\n",
    "jkparetto ={}\n",
    "coevmats = {}\n",
    "\n",
    "#get pareto distribution of jackknife by event type and time\n",
    "for i,outsankof in enumerate(eventmats):\n",
    "    with open( outsankof , 'rb') as matin:\n",
    "        coevmat = pickle.loads(matin.read())[1]\n",
    "    \n",
    "    \n",
    " \n",
    "    coevmat = coevmat.todense()\n",
    "    for k in range( bluriter):\n",
    "        print(np.sum(coevmat))\n",
    "        coevmat += np.dot(blurfactor*connectmat, coevmat )\n",
    "    print(np.sum(coevmat))\n",
    "\n",
    "    if i == 0:\n",
    "        globalmat_blur = coevmat\n",
    "    else:\n",
    "        globalmat_blur += coevmat\n",
    "    \n",
    "    transition_type = outsankof.replace('./16s/16s_wstruct.aln.fastasparsemat_AAtransitionbootstrap_runbootstrap_run_transition_'  , '')\n",
    "    transition_type = transition_type.replace( '_coevmat.pkl' , '')\n",
    "    transition_type = transition_type.replace(\"'\" , '')\n",
    "    coevmats[transition_type] = coevmat\n",
    "    print(np.sum(coevmat))\n",
    "    '''coevmats[transition_type]=coevmat\n",
    "    #generate a vector for each time bin\n",
    "    previous_time = 0\n",
    "    for t,max_time in enumerate(list(time_bins)):\n",
    "        if max_time not in jkparetto_transitions_vs_timebins:\n",
    "            jkparetto_transitions_vs_timebins[max_time] = {}\n",
    "        #print('max time : ', max_time)\n",
    "        node_inidices = [  node.matrow  for node in tree.nodes() if  node.edge_length and node.edge_length > previous_time and  node.edge_length >= max_time  ]\n",
    "        #print('n branches : ', len(node_inidices))\n",
    "        submat = coevmat[node_inidices , : ]\n",
    "        previous_time = max_time\n",
    "        submat = submat[submat>0].flatten()\n",
    "        sum_events = np.sum(submat)\n",
    "        submat /= sum_events\n",
    "        \n",
    "        #print('n events : ' , submat.shape)\n",
    "        \n",
    "        df, loc, scale = scipy.stats.genpareto.fit( submat.flat )\n",
    "        \n",
    "        #keep paretto parameters for this time bin and transition type\n",
    "        jkparetto_transitions_vs_timebins[max_time][transition_type] = {'max_time':max_time, 'scaling': sum_events, 'df': df, 'loc':loc, 'scale':scale }\n",
    "        x = np.linspace(scipy.stats.genpareto.ppf(.3, df, loc=loc , scale=scale ),\n",
    "                scipy.stats.genpareto.ppf(.99, df  , loc=loc , scale=scale ), 100)\n",
    "        if t < 3:\n",
    "            plt.plot(x, scipy.stats.genpareto.pdf(x, df ,  loc=loc , scale=scale), '-', lw=2, alpha=0.6, label='pareto pdf ' + str(max_time))'''\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    vector =  coevmat[coevmat>0].flatten()\n",
    "    vectors.append(vector)\n",
    "    \n",
    "    #vector =  coevmat.flatten()\n",
    "    vector /= np.sum(vector)\n",
    "    plt.hist( vector.flat , bins = 40 )\n",
    "    df, loc, scale = scipy.stats.genpareto.fit( vector.flat )\n",
    "    x = np.linspace(scipy.stats.genpareto.ppf(.4, df, loc=loc , scale=scale ),\n",
    "                scipy.stats.genpareto.ppf(.999, df  , loc=loc , scale=scale ), 100)\n",
    "    plt.plot(x, scipy.stats.genpareto.pdf(x, df ,  loc=loc , scale=scale), 'r-', lw=5, alpha=0.6, label='pareto pdf')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.spy(coevmat, markersize= .5)\n",
    "    plt.title(outsankof)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('calculating pareto for total event scores')\n",
    "vectors = np.hstack(vectors)\n",
    "sum_events =  np.sum(vectors)\n",
    "vectors /= np.sum(vectors)\n",
    "plt.hist( vectors.flat , bins = 40 )\n",
    "\n",
    "df, loc, scale = scipy.stats.genpareto.fit( vectors.flat )\n",
    "x = np.linspace(scipy.stats.genpareto.ppf(.25, df, loc=loc , scale=scale ),\n",
    "                scipy.stats.genpareto.ppf(.999, df  , loc=loc , scale=scale ), 100)\n",
    "\n",
    "plt.plot(x, scipy.stats.genpareto.pdf(x, df ,  loc=loc , scale=scale), 'r-', lw=5, alpha=0.6, label='pareto pdf')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "jkpareto = { 'scaling': sum_events, 'df': df, 'loc':loc, 'scale':scale }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tree to blur eventmat\n",
    "from Bio import AlignIO\n",
    "tree = dendropy.Tree.get(\n",
    "    path=treefile,\n",
    "    schema='newick')\n",
    "col = 521\n",
    "aln_col = align_array[:,col]\n",
    "def clipID(ID):\n",
    "    return ID.replace('/',' ')\n",
    "\n",
    "msa = AlignIO.read(alnfile , format = 'fasta')    \n",
    "IDs = {i:clipID(rec.id) for i,rec in enumerate(msa)}\n",
    "#IDs = {i:rec.id for i,rec in enumerate(msa)}\n",
    "IDindex = dict(zip( IDs.values() , IDs.keys() ) )\n",
    "matsize = len(tree.nodes())\n",
    "#add chars to leaves\n",
    "for i,l in enumerate(tree.leaf_nodes()):\n",
    "    if l.taxon.label in IDindex:\n",
    "        l.taxon.label += ' '+ aln_col[IDindex[l.taxon.label]].decode()\n",
    "\n",
    "#add events\n",
    "for mat in coevmats:\n",
    "    print(mat)\n",
    "    for i,n in enumerate( tree.nodes() ):\n",
    "        if coevmats[mat][i,col] > 0:\n",
    "            n.label = str( coevmats[mat][i,col] )\n",
    "        else:\n",
    "            n.label = ''\n",
    "    #vis w event proba \n",
    "    tree.print_plot( show_internal_node_labels = True , width = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "#define the jack knife / blur score proba functions\n",
    "\n",
    "def discretize(t, timebins):\n",
    "    #return a discvretized time to call a bin of the hist of jackknife score\n",
    "    prevtime = 0\n",
    "    for time in timebins:\n",
    "        if t > prevtime and t<= time:\n",
    "            return time\n",
    "    else:\n",
    "        return max(timebins)\n",
    "\n",
    "def p_jacknife(jacknife , pdf_jk):\n",
    "    #given a jacknife score, what is its probability over all scores\n",
    "    jacknife /=pdf_jk['scaling']\n",
    "    \n",
    "    return scipy.stats.genpareto.cdf(jacknife , pdf_jk['df']  , loc=pdf_jk['loc'] , scale= pdf_jk['scale'] )\n",
    "\n",
    "def p_jacknife_given_s1s2t( jacknife, s1,s2, t , jk_pdfs  ):\n",
    "    #given that a transition was detected over t, what is the probability of this jackknife/blur score\n",
    "    #derive a distribution of jack_knife/blur score for each transition type and time\n",
    "    #get the pvalue of a jackknife score sampling the normalized 2d hist ( distribution of scores over time bins )\n",
    "    timebin = discretize(t)\n",
    "    jacknife/=jk_pdfs[timebin][s1+s2]['scaling']\n",
    "    return scipy.stats.genpareto.cdf(jacknife , jk_pdfs[timebin][s1+s2]['df']  , loc=jk_pdfs[timebin][s1+s2]['loc'] , scale= jk_pdfs[timebin][s1+s2]['scale'] )\n",
    "\n",
    "p_jacknife = partial(p_jacknife,  pdf_jk = jkpareto  )\n",
    "p_jacknife_given_s1s2t = partial(p_jacknife_given_s1s2t,  jk_pdfs = jkparetto_transitions_vs_timebins )\n",
    "discretize = partial(discretize , timebins = list(time_bins)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p( t , v, v_inv, w):\n",
    "    #define transition probability given two states and time elapsed using mutation rates for markov process\n",
    "    Q = np.array([ [ np.exp( w[i]*t ) if i==j else 0 for j in range(w.shape[0]) ] for i in range( w.shape[0])] ) \n",
    "    probas_over_t = v * Q * v_inv\n",
    "    return probas_over_t\n",
    "\n",
    "def p_dual( t , v, v_inv, w):\n",
    "    #define transition probability given two states and time elapsed using mutation rates for markov process\n",
    "    Q = np.array([ [ np.exp( w[i]*t ) if i==j else 0 for j in range(w.shape[0]) ] for i in range( w.shape[0] ) ] )\n",
    "    probas_over_t = v*Q*v_inv\n",
    "    return probas_over_t\n",
    "\n",
    "def bayes_jk( s1,s2, t , jacknife ):\n",
    "    #give the score of this transition, what is the proba it occured here\n",
    "    #depends on blurred jackknife distribution and markov transition proba\n",
    "    #sum jacknife prob, transition mat colum and p jackknife all = 1\n",
    "    \n",
    "    conditioning = p_jacknife_given_s1s2t(jacknife, s1,s2, t ) / p_jacknife(jacknife)\n",
    "    return conditioning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######important \n",
    "mapping = {'A':0 , 'T':1 , 'C':2 ,'G':3 }\n",
    "transitions = [ char1+char2  for char1 in mapping.keys() for char2 in mapping.keys()]\n",
    "transitionmap = { t:i for i,t in enumerate(transitions) }\n",
    "print(transitionmap)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ancestral_states(tree, cols):\n",
    "    #use the aln and tree w no jacknife to make a tree of ancestral states for one or two cols\n",
    "    \n",
    "    \n",
    "def get_transitino_mat_p(node, mapping , codons =False , verbose = True):\n",
    "    #parse tree and aln and find the number of transitions to derive transition mat using coevmat\n",
    "    \n",
    "    #this should work for a tree instantiatiated w one or two codons or one or two columns ( 6,3 or 2,1 columns )\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def process_node_smallpars_1(node):\n",
    "    #go from leaves up and generate character sets\n",
    "    if node.symbols is None:\n",
    "        for child in node.child_nodes():\n",
    "            if child.symbols is None:\n",
    "                process_node_smallpars_1(child)\n",
    "        node.symbols = { }\n",
    "        node.scores = { }\n",
    "        for pos in range(len(child.symbols)):\n",
    "            symbols = set.intersection( * [ child.symbols[pos] for child in node.child_nodes( ) ] )\n",
    "            if len(symbols) == 0:\n",
    "                symbols = set.union( * [ child.symbols[pos] for child in node.child_nodes( ) ] )\n",
    "            node.symbols[pos] = symbols\n",
    "            node.scores[pos] = { }\n",
    "            for c in allowed_symbols:\n",
    "                if c not in node.symbols[pos]:\n",
    "                    #add trnasition mat here if needed\n",
    "                    score = min(  [ child.scores[pos][c] for child in node.child_nodes()])+1\n",
    "                else:\n",
    "                    score = min(  [ child.scores[pos][c] for child in node.child_nodes() ] )\n",
    "                node.scores[pos][c] = score\n",
    "\n",
    "def process_node_smallpars_2(node , eventmat , codons = False, verbose = False):\n",
    "    #assign the most parsimonious char from children\n",
    "    if node.char is None:\n",
    "        if node.parent_node:\n",
    "            #node has parent\n",
    "            node.char = {}\n",
    "            node.event = {}\n",
    "            node.eventype= {}\n",
    "            node.AAevent = 0\n",
    "            for pos in range(len(child.symbols)):\n",
    "                node.char[pos] = min(node.scores[pos], key=node.scores[pos].get)\n",
    "                if node.parent_node.char[pos] == node.char[pos]:\n",
    "                    node.event[pos] = 0\n",
    "                else:\n",
    "                    if node.scores[pos][node.parent_node.char[pos]] == node.scores[pos][node.char[pos]] :\n",
    "                        node.char[pos] = node.parent_node.char[pos]\n",
    "                        node.event[pos] = 0\n",
    "                    else:\n",
    "                        node.event[pos] = 1\n",
    "                        node.eventype[pos] = transition_dict[node.parent_node.char[pos]+node.char[pos]]\n",
    "            ##### change this for the possibility of tracking two codons at once\n",
    "            if codons == True:\n",
    "                node.AA = str(Seq.Seq(b''.join([ node.char[pos] for pos in [0,1,2] ]).decode() ).translate())\n",
    "                if node.AA != node.parent_node.AA and nucleotides_only == False:\n",
    "                    node.AAevent = transitiondict_AA[node.parent_node.AA+node.AA]\n",
    "                    if verbose == True:\n",
    "                        print( node.parent_node.AA , ' -> ' ,  node.AA)\n",
    "        else:\n",
    "            #root node\n",
    "            node.char = {}\n",
    "            node.event= {}\n",
    "            node.eventype = {}\n",
    "            node.AAevent = 0\n",
    "            for pos in [0,1,2]:\n",
    "                node.char[pos] = min(node.scores[pos], key=node.scores[pos].get)\n",
    "                node.event[pos] = 0\n",
    "            if codons == True:\n",
    "                node.AA = Seq.Seq(b''.join([ node.char[pos] for pos in [0,1,2] ]).decode() ).translate()\n",
    "            else:\n",
    "                node.AA = 'G'\n",
    "        #down one level\n",
    "        for child in node.child_nodes():\n",
    "            if child.char is None:\n",
    "                process_node_smallpars_2(child)\n",
    "\n",
    "def calculate_small_parsimony( t, aln_columns , row_index , verbose  = False ):\n",
    "    missing = 0\n",
    "    #assign leaf values\n",
    "    for pos,col in enumerate(aln_columns):\n",
    "        for l in t.leaf_nodes():\n",
    "            if hasattr(col , 'decode'):\n",
    "                #column has no events\n",
    "                l.calc[pos] = False\n",
    "                char = col\n",
    "                l.event[pos] = 0\n",
    "                l.scores[pos] = { c:10**10 for c in allowed_symbols }\n",
    "                if char.upper() in allowed_symbols:\n",
    "                    l.symbols[pos] = { char }\n",
    "                    l.scores[pos][char] = 0\n",
    "                else:\n",
    "                    #ambiguous leaf\n",
    "                    l.symbols[pos] = allowed_symbols\n",
    "            else:\n",
    "                #setup for small_pars1\n",
    "                l.calc[pos] = True\n",
    "                l.event[pos] =0\n",
    "                l.scores[pos] = { c:10**10 for c in allowed_symbols }\n",
    "\n",
    "                if str(l.taxon).replace(\"'\", '') in row_index:\n",
    "\n",
    "                    char = col[ row_index[str(l.taxon).replace(\"'\", '')]  ]\n",
    "                    if char.upper() in allowed_symbols:\n",
    "                        l.symbols[pos] = { char }\n",
    "                        l.scores[pos][char] = 0\n",
    "                    else:\n",
    "                        #ambiguous leaf\n",
    "                        l.symbols[pos] =  allowed_symbols\n",
    "                else:\n",
    "                    missing += 1\n",
    "                    char = None\n",
    "                    l.symbols[pos] =  allowed_symbols\n",
    "                    if verbose == True:\n",
    "                        print( 'err ! alncol: ', l.taxon , aln_column  )\n",
    "                l.char[pos] = min(l.scores[pos], key=l.scores[pos].get)\n",
    "    #up\n",
    "    process_node_smallpars_1(t.seed_node)\n",
    "    #down\n",
    "    \n",
    "    #generate the eventmat\n",
    "    process_node_smallpars_2(t.seed_node ,eventmat)\n",
    "    eventdict = {}\n",
    "    for pos in [0,1,2]:\n",
    "        eventindex = [ n.matrow for n in t.nodes() if n.event[pos] > 0 ]\n",
    "        eventtypes = [ n.eventype[pos] for n in t.nodes() if n.event[pos] > 0 ]\n",
    "        eventdict[pos] = { 'type': eventtypes , 'index' : eventindex }\n",
    "    AAeventindex = [ n.matrow for n in t.nodes() if n.AAevent > 0 ]\n",
    "    AAeventypes = [ transition_dictAA[n.AAevent] for n in t.nodes() if n.AAevent > 0 ]\n",
    "    if verbose == True:\n",
    "        with iolock:\n",
    "            print('smallpars done')\n",
    "            print(eventdict)\n",
    "            print(AAeventindex)\n",
    "    return (eventdict , AAeventindex , AAeventypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use two colums to derive the dual model\n",
    "def doublemat_Pdt(coevmats ,  col1 , col2 , verbose = True ):\n",
    "    jump_mat = copy.deepcopy(transitionmat)\n",
    "    Qmat = np.zeros(jump_mat.shape)\n",
    "    #exponential\n",
    "    for i in range(jump_mat.shape[0]):\n",
    "        Qmat[i,i] = -np.log(jump_mat[i,i])\n",
    "    for i in range(jump_mat.shape[0]):\n",
    "        for j in range(jump_mat.shape[1]):\n",
    "            if i != j:\n",
    "                Qmat[i,j] = -Qmat[i,i]*jump_mat[i,j]/(np.sum( jump_mat[i,:] )-jump_mat[i,i] )\n",
    "    w, v = LA.eig(Qmat)\n",
    "    v_inv = LA.inv(v)  \n",
    "    if verbose == True:\n",
    "        return v,v_inv, w ,transitionmat\n",
    "    else:\n",
    "        return v,v_inv, w \n",
    "\n",
    "#return the Pmat over a give edge len\n",
    "def p( edge_length, v, v_inv, w):\n",
    "    return v_inv * np.exp(edge_length*w) * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 119\n",
    "#markov transition mat for 1 col\n",
    "v, vinv , w , transitionmat , Qmat  = singlemat_Pdt( coevmats_raw , col , verbose = True)\n",
    "print(w)\n",
    "sns.heatmap(transitionmat, xticklabels= mapping.keys( ) , yticklabels=mapping.keys())\n",
    "plt.show()\n",
    "print(Qmat)\n",
    "sns.heatmap(Qmat, xticklabels= mapping.keys( ) , yticklabels=mapping.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = 119 \n",
    "col2 = 120\n",
    "\n",
    "#markov transition mat for 2 cols\n",
    "v, vinv , w , transitionmat = doublemat_Pdt(coevmats_raw ,  col1 , col2 )\n",
    "sns.heatmap(transitionmat,  xticklabels=transitionmap.keys() , yticklabels=transitionmap.keys())\n",
    "\n",
    "print(w)\n",
    "print( np.sum(transitionmat, axis = 1))\n",
    "\n",
    "print( np.sum(transitionmat, axis = 0))\n",
    "\n",
    "plt.show()\n",
    "Qmat = treelen*transitionmat\n",
    "np.fill_diagonal(Qmat,0)\n",
    "for i in range(Qmat.shape[0]):\n",
    "    Qmat[i,i] = -np.sum(Qmat[i,:])\n",
    "sns.heatmap(Qmat, xticklabels= transitionmap.keys( ) , yticklabels=transitionmap.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( align_array[:, col2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the probability of a transition tree\n",
    "#proba over each branch of the transition or no change for all internal states\n",
    "#use priors derived from jackknife and blur and sankoff do not explore all possibilities\n",
    "def get_levels_p(node , coevmats , P , col , verbose = True):\n",
    "    #using the felstein pruning algo calculate the proba only the relevant tree levels\n",
    "    #go from leaves up\n",
    "    if node.L is None:\n",
    "        for child in node.child_nodes():\n",
    "            if child.L is None:\n",
    "                get_levels_p(child , coevmats,P,col )\n",
    "        v,v_inv, w = P\n",
    "        #calculate the conditional proba as a function of all states possible in each child node and parent\n",
    "        node.L={}\n",
    "        for s in mapping:\n",
    "            node.L[s]=1\n",
    "        \n",
    "        for child in node.child_nodes():\n",
    "            L={}\n",
    "            for s in mapping:\n",
    "                L[s]=0\n",
    "            #find transition probas for branch len\n",
    "            Pmat = p( child.edge_length, v, v_inv, w)\n",
    "            \n",
    "            for s1 in mapping:\n",
    "                #get the max likelihood here for every state\n",
    "                for s2 in mapping:\n",
    "                    if s1+s2 in coevmats:\n",
    "                        if coevmats[s1+s2][child.matrow,col] > 0 :\n",
    "                            #there may have been a change here.\n",
    "                            L[s1] += Pmat[mapping[s1],mapping[s2]] * bayes_jk( s1,s2, child.edge_length , coevmats[s1+s2][child.matrow,col] ) * child.L[s2]\n",
    "                            print( bayes_jk( s1,s2, child.edge_length , coevmats[s1+s2][child.matrow,col] ))\n",
    "                    else:\n",
    "                        #no change\n",
    "                        if Pmat[mapping[s1],mapping[s2]] > 0:\n",
    "                            L[s1] += Pmat[mapping[s1],mapping[s2]] * child.L[s2]\n",
    "                            \n",
    "                        else:\n",
    "                            L[s1] += child.L[s2] * 10**-6 \n",
    "        #could be an error here due to alnchar:\n",
    "            \n",
    "            for s in mapping:\n",
    "                node.L[s] *= L[s]\n",
    "            \n",
    "        if sum(node.L.values()) ==0:\n",
    "            print(Pmat)\n",
    "            for child in node.child_nodes():\n",
    "                print(child.taxon)\n",
    "                print(child.L)\n",
    "\n",
    "\n",
    "        \n",
    "        node.Ltotal = sum(node.L.values())\n",
    "        if verbose == True:\n",
    "            print( 'L', L)\n",
    "        if verbose == True:\n",
    "            print(node.L)\n",
    "\n",
    "def get_levels_p_dual(node , coevmats , P , col1  , col2 ):\n",
    "    #using the felstein pruning algo calculate the proba only the relevant tree levels\n",
    "    #go from leaves up\n",
    "    if node.L1 is None:\n",
    "        for child in node.child_nodes():\n",
    "            if child.L1 is None:\n",
    "                get_levels_p_dual(node , coevmats , P , col1  , col2 )\n",
    "        v,v_inv, w = P\n",
    "        #calculate the conditional proba as a function of all states possible in each child node and parent        \n",
    "        for child in node.child_nodes():\n",
    "            if child.L is None:\n",
    "                get_levels_p(child , coevmats,P,col )\n",
    "        v,v_inv, w = P\n",
    "        #calculate the conditional proba as a function of all states possible in each child node and parent\n",
    "        node.L1={}\n",
    "        node.L2={}\n",
    "        for s in mapping:\n",
    "            node.L[s]=1\n",
    "        for child in node.child_nodes():\n",
    "            L={}\n",
    "            for s in mapping:\n",
    "                L[s]=0\n",
    "            \n",
    "            #find transition probas for branch len\n",
    "            Pmat = p( child.edge_length, v, v_inv, w)\n",
    "            for t1 in transitionmap:\n",
    "                #get the max likelihood here for every state\n",
    "                for t2 in transitionmap:\n",
    "                    s1_1 = t1[0]\n",
    "                    s1_2 = t1[1]\n",
    "                    s2_1 = t2[0]\n",
    "                    s2_2 = t2[1]        \n",
    "                    p = Pmat[transitionmap[t1],transitionmap[t2]] * child.L[s1_2]   \n",
    "                    for t in [t1, t2]:\n",
    "                        if t in coevmats:\n",
    "                            p *= bayes_jk( s2_1,s2_2  , child.edge_length , coevmats[t2][child.matrow,col] )\n",
    "                    L1[s1_1] += p\n",
    "                \n",
    "                node.L1[s1_1]*= L[s1_1]\n",
    "                node.L2[s2_1]*= L[s2_1]\n",
    "\n",
    "def L_single(tree , coevmats , P , col , alncol , verbose = True ):\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('calc p for single site')\n",
    "    #assign the states to leaves\n",
    "    for i,n in enumerate(tree.nodes()):\n",
    "        n.matrow = i\n",
    "        n.L = None\n",
    "    for l in tree.leaf_nodes():\n",
    "        #leaf in aln\n",
    "        l.L= {}\n",
    "        if l.taxon.label in IDindex:\n",
    "            if alncol[IDindex[l.taxon.label]].decode() in mapping:\n",
    "            #set state to aln char\n",
    "                l.char = alncol[IDindex[l.taxon.label]].decode()\n",
    "                for c in mapping:\n",
    "                    l.L[c] = 0\n",
    "                l.L[l.char] = 1\n",
    "            elif alncol[IDindex[l.taxon.label]].decode() == '-':\n",
    "                for c in mapping:\n",
    "                    l.L[c] = 1/len(mapping)\n",
    "            else:\n",
    "                l.char = None\n",
    "                for c in mapping:\n",
    "                    l.L[c] = 1/len(mapping)\n",
    "        else:\n",
    "            l.char = None\n",
    "            for c in mapping:\n",
    "                l.L[c] = 1/len(mapping)\n",
    "        \n",
    "    L = get_levels_p(tree.seed_node, coevmats , P , col )\n",
    "\n",
    "def L_double(tree , coevmats , P , col1, col2 , alncol1, alncol2 , verbose = True ):\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('calc p for single site')\n",
    "        \n",
    "    #assign the states to leaves\n",
    "    for i,n in enumerate(tree.nodes()):\n",
    "        n.matrow = i\n",
    "        n.L = None\n",
    "    for l in tree.leaf_nodes():\n",
    "        #leaf in aln\n",
    "        l.L1= {}\n",
    "        l.L2= {}\n",
    "        for alncol,L in [ (alncol1,l.L1) , (alncol2 , l.L2)]:\n",
    "            if l.taxon.label in IDindex:\n",
    "                if alncol[IDindex[l.taxon.label]].decode() in mapping:\n",
    "                #set state to aln char\n",
    "                    l.char = alncol[IDindex[l.taxon.label]].decode()\n",
    "                    for c in mapping:\n",
    "                        L[c] = 0\n",
    "                    L[l.char] = 1\n",
    "                elif alncol[IDindex[l.taxon.label]].decode() == '-':\n",
    "                    for c in mapping:\n",
    "                        L[c] = 1/len(mapping)\n",
    "                else:\n",
    "                    l.char = None\n",
    "                    for c in mapping:\n",
    "                        L[c] = 1/len(mapping)\n",
    "            else:\n",
    "                l.char = None\n",
    "                for c in mapping:\n",
    "                    L[c] = 1/len(mapping)\n",
    "    L = get_levels_p_dual(tree.seed_node, coevmats , P , col )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = dendropy.Tree.get(\n",
    "    path=treefile,\n",
    "    schema='newick')\n",
    "P  = singlemat_Pdt( coevmats_raw , col1 , verbose = False)\n",
    "L_single(tree , coevmats , P , col1 , align_array[:, col1], verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outsankof in eventmats:\n",
    "\n",
    "    transition_type = outsankof.replace('./16s/16s_salaminWstruct_aln.fastasparsemat_AAtransitionbootstrap_runbootstrap_run_transition_'  , '')\n",
    "    transition_type = transition_type.replace( '_coevmat.pkl' , '')\n",
    "    transition_type = transition_type.replace(\"'\" , '')\n",
    "    with open( outsankof , 'rb') as matin:\n",
    "        coevmat = pickle.loads(matin.read())[1]\n",
    "    coevmat = coevmat.todense()\n",
    "    \n",
    "    sumv = np.sum(coevmat , axis = 0)\n",
    "    cdf = np.array( np.cumsum(sumv) / np.sum(sumv))\n",
    "    plt.plot( cdf[0] , label= transition_type  )\n",
    "    \n",
    "cdf_uniform = np.cumsum(np.ones(len(cdf.ravel())) ) / np.sum(len(cdf.ravel()))\n",
    "\n",
    "plt.plot(cdf_uniform, lw = 2 , ls = 'dashed' , c = 'black' , label= 'uniform')\n",
    "plt.legend()\n",
    "plt.title( 'spatial distribution of mutation events in 16sRNA' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####define the aln row and structure here\n",
    "\n",
    "print(IDindex['ga.Esc.col'])\n",
    "print(align_array[0])\n",
    "#filter to columns without gaps\n",
    "msa_row = list(align_array[0])\n",
    "print(len(msa_row))\n",
    "non_gap = np.where( align_array[IDindex['ga.Esc.col']] != b'-')[0]\n",
    "print(non_gap)\n",
    "\n",
    "print(non_gap.shape)\n",
    "print(align_array[IDindex['ga.Esc.col'],non_gap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import MMCIFParser\n",
    "from Bio.PDB.mmcifio import MMCIFIO\n",
    "parser = MMCIFParser()\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    structure = parser.get_structure(\"4v9d\", \"./16s/4v9d.cif\")\n",
    "io=MMCIFIO()\n",
    "io.set_structure(structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab RNA\n",
    "\n",
    "totalatoms = { c.id : [ r for r in c.get_residues() if r.get_resname() in {'A', 'U' , 'C' , 'G'} ] for c in structure.get_chains() }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure from phosphorus to phosphorus\n",
    "distmats = {}\n",
    "for chain in totalatoms:\n",
    "    distmats[chain] = np.array( [[ a1['P'] - a2['P']  if (i<j and 'P' in a1 and 'P' in a2) else 0 for i,a1 in enumerate(totalatoms[chain])] for j,a2 in enumerate(totalatoms[chain]) ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_distmats = False\n",
    "\n",
    "for chain in distmats:\n",
    "    distmats[chain] += distmats[chain].T\n",
    "if show_distmats == True:\n",
    "    for i,chain in enumerate(distmats):\n",
    "        if distmats[chain].shape[0]>0:\n",
    "            plt.figure(figsize= (20,20))\n",
    "            plt.title( 'distmat ' + chain )\n",
    "\n",
    "            plt.imshow(distmats[chain])\n",
    "            plt.colorbar()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "subunit = distmats['BA']\n",
    "print(subunit)\n",
    "subthresh = copy.deepcopy(subunit)\n",
    "subthresh[ subthresh < 25 ] = 1 \n",
    "subthresh[ subthresh > 25 ] = 0 \n",
    "np.fill_diagonal(subthresh , 0)\n",
    "print(subthresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap , MDS , locally_linear_embedding\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans , DBSCAN ,SpectralClustering ,OPTICS , AgglomerativeClustering\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_clustering(codonmat, calc_isomap = False , scale = True , dbscan = False , xi = 0.035 , samples = 5 , jaccard = True , reductionfactor= .75):\n",
    "    if scale == True:\n",
    "        scaler = sklearn.preprocessing.RobustScaler( )\n",
    "        codonmat = scaler.fit_transform(codonmat.T).T\n",
    "    if calc_isomap == True:\n",
    "        try:\n",
    "            isomap = Isomap( n_neighbors= 25, n_components= int(codonmat.shape[0]*reductionfactor) , eigen_solver='auto', tol=0, \n",
    "               max_iter=None, path_method='auto', neighbors_algorithm='auto', n_jobs=-1 )\n",
    "            #isomap = sklearn.manifold.locally_linear_embedding(n_neighbors=25, n_components=50)\n",
    "            codonmat = isomap.fit_transform( X=codonmat.T ).T\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    if jaccard == False:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            apb = OPTICS(min_samples=samples, metric='minkowski', p=1.5, metric_params=None, cluster_method='xi', eps=None, xi=xi, predecessor_correction=True, min_cluster_size=2 , algorithm='auto', leaf_size=20, n_jobs=-1)\n",
    "            cluster_labels_codon = apb.fit_predict(codonmat.T )    \n",
    "    else:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            apb = OPTICS(min_samples=samples, metric='jaccard', p=1.5, metric_params=None, cluster_method='xi', eps=None, xi=xi, predecessor_correction=True, min_cluster_size=2 , algorithm='auto', leaf_size=20, n_jobs=-1)\n",
    "            cluster_labels_codon = apb.fit_predict(codonmat.T )\n",
    "    \n",
    "    return cluster_labels_codon\n",
    "\n",
    "def subcluster( mat , cluster_thresh=10 , indices = None  ,  iter_thresh = 15 , clusterlevel = 'head' , clustiter =0, calc_isomap = False , scale = True , xi = .035 , samples = 5 , verbose = False , jaccard = False  ):\n",
    "    #break a large cluster into smaller subclusters until avg cluster size is below thresh\n",
    "    if verbose == True:\n",
    "        print('clustering' , clusterlevel)\n",
    "    \n",
    "    cluster_labels = return_clustering(mat, calc_isomap=calc_isomap  , scale=scale  , dbscan=dbscan  ,xi = xi , samples = samples , jaccard = jaccard)\n",
    "    \n",
    "    if verbose == True:\n",
    "        print('DONE clustering ', clusterlevel)\n",
    "    if indices is None:\n",
    "        indices = np.arange(mat.shape[1])\n",
    "    \n",
    "    l,c = np.unique(cluster_labels, return_counts= True)\n",
    "    if len(l) == 1:\n",
    "        xi += .01\n",
    "        if verbose == True:\n",
    "            print(l)\n",
    "            print('xi',xi)\n",
    "    if verbose == True:\n",
    "        print('l',l)\n",
    "        print('c',c)\n",
    "    clustering = {  'labels': cluster_labels , 'sub':{} , 'mappings':{}  }\n",
    "    #if we're over the max recursion\n",
    "    kmeans = False\n",
    "    if clustiter < iter_thresh:\n",
    "        \n",
    "        for i,label in enumerate(list(l)):\n",
    "            clustering['mappings'][label] = indices[cluster_labels == label]\n",
    "            if c[i] > cluster_thresh:\n",
    "                if verbose == True:\n",
    "                    print( 'reclustring',label , c[i] )\n",
    "                    print( 'min samples' , samples )\n",
    "                #isomap within clusters\n",
    "                \n",
    "                #don't split up clusters formed from columns with no events\n",
    "                \n",
    "                clustering['sub'][label] = subcluster( mat[:,cluster_labels==label] , cluster_thresh =cluster_thresh ,indices= clustering['mappings'][label] , iter_thresh = iter_thresh , clusterlevel= str(label)+'_'+str(clustiter), samples = samples,  clustiter = clustiter + 1 , calc_isomap = True , jaccard = False ,  xi= xi , scale = scale  )\n",
    "                samples = max( 2 , samples-1 )\n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sub(indict, level= 0 ,mapping = None):\n",
    "    l,c = np.unique(indict['labels'] , return_counts= True)\n",
    "    for clust in list(l):\n",
    "        if 'sub' not in indict or clust not in indict['sub']:\n",
    "            if clust != -1:\n",
    "                yield indict['mappings'][clust]  \n",
    "        else:\n",
    "            return_sub( indict['sub'][clust], level = level +1 )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import GraphicalLassoCV , ShrunkCovariance , EmpiricalCovariance\n",
    "\n",
    "def divide_clusters( subclustering_dict , cols ):\n",
    "    #only go down one level for now\n",
    "    cluster_dict={}\n",
    "    count =0\n",
    "    total_labels = np.ones((cols,))*-1\n",
    "    for mapping in return_sub(subclustering_dict):\n",
    "        total_labels[mapping] = count\n",
    "        count += 1\n",
    "    return total_labels\n",
    "    #setup sublcluster dictionary\n",
    "\n",
    "def clusters2pairs( cluster_labels , mat , use_outliers =True,  verbose = True ):\n",
    "    #subcluster further using sparse inv cov within clusters to form pairs\n",
    "    count = 0\n",
    "    l,c = np.unique(cluster_labels, return_counts= True)\n",
    "    print(l)\n",
    "    print(c)\n",
    "    print(mat.shape)\n",
    "    pairs = []\n",
    "    for label in l:\n",
    "        print(label)\n",
    "        if label != -1 or use_outliers == True:\n",
    "            submat = mat[:, cluster_labels == int(label)]\n",
    "            \n",
    "            print(submat.shape)\n",
    "\n",
    "            #edge_model = GraphicalLassoCV(mode ='lars',max_iter = 1000)\n",
    "            #edge_model = EmpiricalCovariance()\n",
    "            #edge_model = ShrunkCovariance()\n",
    "            \n",
    "            submat /= submat.std(axis=0)\n",
    "            \n",
    "            edge_model.fit(submat)\n",
    "            partial_correlations = edge_model.precision_.copy()\n",
    "            \n",
    "            if verbose == True:\n",
    "                \n",
    "                plt.imshow(edge_model.covariance_)\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "\n",
    "                plt.imshow(partial_correlations)\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "            print(partial_correlations)\n",
    "            d = 1 / np.sqrt(np.diag(partial_correlations))\n",
    "            partial_correlations *= d\n",
    "            partial_correlations *= d[:, np.newaxis]\n",
    "                \n",
    "            non_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n",
    "            if verbose == True:\n",
    "                print(non_zero)\n",
    "            #extract pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_point(contactsx , contactsy, ptx,pty, radius = 10 , verbose = False):\n",
    "    #find non zero contact pts\n",
    "    for center_x, center_y, in zip(contactsx,contactsy):\n",
    "        if (ptx - center_x)**2 + (pty - center_y)**2 < radius**2:\n",
    "            if verbose == True:\n",
    "                print(ptx,pty,center_x, center_y)\n",
    "                print('left', (ptx - center_x)**2 + (pty - center_y)**2 )\n",
    "                print( 'right', radius**2 )\n",
    "                \n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "#\n",
    "contactsx , contactsy  = (np.nonzero(subthresh)[0] ,np.nonzero(subthresh)[1])\n",
    "\n",
    "#multiply connectivity mat to get 1 degree of separation \n",
    "\n",
    "connected = np.dot(subthresh,subthresh)\n",
    "connnectedx , connectedy  = (np.nonzero(connected)[0] ,np.nonzero(connected)[1])\n",
    "blurfactor =  .25\n",
    "\n",
    "plt.figure(figsize=(10,10) )\n",
    "plt.title('contact mat')\n",
    "plt.scatter( np.nonzero(subthresh)[0] ,np.nonzero(subthresh)[1]  , marker= 'o' , alpha = .002525,  s = 50)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10) )\n",
    "plt.title('contact 1 degree of sep')\n",
    "plt.scatter(connnectedx ,connectedy  , marker= 'o' , alpha = .002525,  s = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate clusters by distance thresholding\n",
    "#looks like a mess....\n",
    "\n",
    "bluriters = [100]\n",
    "\n",
    "for bluriter in bluriters:\n",
    "    #bluriter\n",
    "    #do all vs all \n",
    "        #blur w connectivity mat\n",
    "    coevmat = copy.deepcopy(globalmat)\n",
    "    \n",
    "    for k in range( bluriter):\n",
    "        print('blurring x' , k)\n",
    "        coevmat += np.dot(blurfactor*connectmat, coevmat )\n",
    "    greens = []\n",
    "    reds=[]\n",
    "    oranges=[]\n",
    "    dist = scipy.spatial.distance.pdist(coevmat.T)\n",
    "    print(dist.shape)\n",
    "    \n",
    "    plt.hist(dist)\n",
    "    plt.show()\n",
    "    hist,edges = np.histogram( dist , bins=10)\n",
    "    print(edges)\n",
    "    \n",
    "    plt.figure(figsize=(10,10) )\n",
    "\n",
    "    dist =  scipy.spatial.distance.squareform(dist)\n",
    "    sns.heatmap(dist)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    for thresh in list(edges)[:-1]:\n",
    "        \n",
    "        print(thresh)\n",
    "        threshmat = copy.deepcopy(dist)\n",
    "        threshmat[dist>thresh] = 0\n",
    "        threshmat[dist <= thresh] = 1\n",
    "        threshmat = threshmat[non_gap,:]\n",
    "        threshmat = threshmat[:,non_gap]\n",
    "        np.fill_diagonal(threshmat, 0)\n",
    "        \n",
    "        print('infered interactors ' , np.sum(threshmat) / 2 )\n",
    "        sns.heatmap(threshmat)\n",
    "        plt.title('thresh:'+str(thresh))\n",
    "        plt.show()\n",
    "               \n",
    "        #check for contact pts\n",
    "        ptsx,ptsy = np.nonzero( threshmat )\n",
    "        print(len(ptsx))\n",
    "        print(len(ptsy))\n",
    "        \n",
    "        ptsx = list(ptsx)\n",
    "        ptsy = list(ptsy)\n",
    "        pts =  list( zip(ptsx,ptsy) )\n",
    "        green = [ 1 if  ptx > pty and  ptx < threshmat.shape[0] and pty < threshmat.shape[1] and check_point(list(contactsx) , list(contactsy), ptx,pty, radius = 10 , verbose = False) else 0 for ptx,pty in pts ]\n",
    "        #check for connected components\n",
    "        print('green pts' ,np.sum(green))\n",
    "        \n",
    "        orange = [ 1 if  ptx > pty and  ptx < threshmat.shape[0] and pty < threshmat.shape[1] and check_point(list(connnectedx) , list(connectedy) ,ptx,pty, radius = 5 , verbose = False) else 0 for ptx,pty in pts  ]\n",
    "        print('oragne pts ' , np.sum(orange))\n",
    "\n",
    "        #all detected points not in orange and green\n",
    "        red = np.array(green) + np.array(orange)\n",
    "        red[red > 0 ] = 1\n",
    "        red = np.abs(1 - red )\n",
    "        print('red pts ' , np.sum(orange))\n",
    "        \n",
    "        \n",
    "        greens.append(np.sum(green))\n",
    "        oranges.append(np.sum(orange))\n",
    "        reds.append(np.sum(red))\n",
    "    print(greens)\n",
    "    print(oranges)\n",
    "    print(reds)\n",
    "    plt.plot(greens,c='green')\n",
    "    plt.plot(reds,c='red')\n",
    "    plt.plot(oranges,c='orange')\n",
    "    plt.title('bluriter '+str(bluriter) )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_labels)\n",
    "l,c = np.unique(non_gap_labels, return_counts= True)\n",
    "print(l)\n",
    "print(c)\n",
    "mat = copy.deepcopy(globalmat[:,non_gap])\n",
    "plt.hist(np.sum(mat , axis= 0 ).flat , bins = 50)\n",
    "plt.show()\n",
    "for label in list(l):\n",
    "    print(label)\n",
    "    plt.hist( np.sum(mat[:,cluster_labels == label] , axis = 0 ).flat , density = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate clustering using recursive optics and isomap\n",
    "verbose = True\n",
    "show_intra_clust_dist = True\n",
    "show_hits = True \n",
    "show_mat = True\n",
    "\n",
    "import itertools\n",
    "from scipy.stats import gumbel_l\n",
    "\n",
    "\n",
    "connected = np.dot(subthresh,subthresh)\n",
    "connnectedx , connectedy  = (np.nonzero(connected)[0] ,np.nonzero(connected)[1])\n",
    "\n",
    "globalmat_blur_thresh = copy.deepcopy(globalmat_blur)\n",
    "globalmat_blur_thresh[ globalmat_blur_thresh < 20 ] = 0\n",
    "\n",
    "#for blur iter in blur\n",
    "#count the number of green / orange / red\n",
    "\n",
    "maxiter = 20\n",
    "blurs = []\n",
    "increment = 100\n",
    "blurs = 0\n",
    "blurvec =[0]\n",
    "lastreds = 100\n",
    "totalreds = [0]\n",
    "totalgreens = [0]\n",
    "totaloranges = [0]\n",
    "shuffle_iter = 1000\n",
    "#find within cluster eval for a pair \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bluriteration in [100,110,120,130,140,150 ]:\n",
    "    mat = copy.deepcopy(globalmat)\n",
    "    for k in range(int( bluriteration ) ):\n",
    "        mat += np.dot(blurfactor*connectmat, mat )\n",
    "    print('blurs',bluriteration ) \n",
    "    \n",
    "    print(mat.shape)\n",
    "    submat = mat[:,non_gap]\n",
    "    print(submat.shape)\n",
    "    \n",
    "    print('recursive clustering')\n",
    "    cluster_labels = divide_clusters( subcluster( submat, calc_isomap= True , jaccard = False,  scale = False )  , submat.shape[1])\n",
    "    print('done')\n",
    "    \n",
    "    #add the uinformative columns to -1\n",
    "    cluster_labels[ np.sum(submat , axis = 0 ).flat == 0 ] = -1\n",
    "    non_gap_labels = cluster_labels[:,non_gap]\n",
    "    \n",
    "    l,c = np.unique(cluster_labels, return_counts= True)\n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for k in range(len(l))   ]\n",
    "\n",
    "    print('labels' , l )\n",
    "    print('counts' , c )\n",
    "    \n",
    "    if show_intra_clust_dist == True:\n",
    "        for i,label in enumerate(list(l.flat)):\n",
    "            if i < 10:\n",
    "                if c[i]< 20 and c[i]>2:\n",
    "                    print('intracluster dists: ' , label)\n",
    "                    \n",
    "                    zeroed = np.where(cluster_labels == label )[0]\n",
    "                    print(zeroed)\n",
    "\n",
    "                    combos =  [ c for c in itertools.combinations(list(zeroed) , 2)]                        \n",
    "                    \n",
    "                    if verbose == True:\n",
    "                        \n",
    "                        plt.figure(figsize=(30,10))\n",
    "                        plt.imshow(submat[:,zeroed])\n",
    "                        plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(30,10))\n",
    "                        plt.spy(submat[:,zeroed])    \n",
    "                        plt.show()\n",
    "                    \n",
    "                    \n",
    "                    if len(combos)>3:\n",
    "                        print(combos)\n",
    "                        scrambled = []\n",
    "                        distances = []\n",
    "                        shuffleiter = 100\n",
    "                        for combo in combos:\n",
    "                            #find intracluster distances \n",
    "                            c1 = copy.deepcopy(submat[:,combo[0]])\n",
    "                            c2 = copy.deepcopy(submat[:,combo[1]])\n",
    "                            dist = scipy.spatial.distance.euclidean(c1, c2 )\n",
    "                            c1_scramble = copy.deepcopy(submat[:,combo[0]])\n",
    "                            c2_scramble = copy.deepcopy(submat[:,combo[1]])\n",
    "                            distances.append( dist )\n",
    "                            for shuffle in range(shuffleiter):\n",
    "                                np.random.shuffle(c1_scramble)\n",
    "                                np.random.shuffle(c2_scramble)\n",
    "                                dist = scipy.spatial.distance.euclidean(c1_scramble, c2_scramble  )\n",
    "                                scrambled.append( dist )\n",
    "                        #fit gumbel\n",
    "\n",
    "                        #cutoff w p val\n",
    "                        \n",
    "                            \n",
    "                        scrambled = np.array(scrambled)\n",
    "                        distances = np.array(distances)\n",
    "                        \n",
    "                        #add pval cutoff\n",
    "                        plt.hist(scrambled , label = 'scrambled' , alpha = .5, density=True)\n",
    "                        plt.hist(distances, label = 'coev' ,  alpha = .5, density=True)\n",
    "                        plt.legend()\n",
    "                        plt.title('cluster'+str(label))\n",
    "                        plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    if show_mat == True:\n",
    "        plt.figure(figsize=(45,100))\n",
    "        for i,label in enumerate(list(l.flat)):\n",
    "            mask = copy.deepcopy(globalmat)\n",
    "            mask = mask[:,non_gap]\n",
    "            mask[:, label!=cluster_labels ] =0\n",
    "            plt.spy(mask, markersize= .5 ,c=color[i] )\n",
    "        plt.show()\n",
    "                        \n",
    "    if show_hits == True:\n",
    "        plt.figure(figsize=(20,20) )\n",
    "        for i,label in enumerate(list(l.flat)):\n",
    "            if c[i]< 15 and c[i]>2 : \n",
    "                zeroed = np.where(non_gap_labels == label )[0]\n",
    "                if len(list(zeroed))>2:\n",
    "                    combos = list(zip( * [ c for c in itertools.combinations(list(zeroed) , 2)]))\n",
    "                    \n",
    "                    \n",
    "                    try:\n",
    "                        plt.scatter( combos[0] , combos[1] , c= color[i] , marker= 'X', s=100)\n",
    "                        plt.scatter( combos[1], combos[0] , c= color[i] , marker= 'X' , s=100)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "        plt.scatter( np.nonzero(subthresh)[0] ,np.nonzero(subthresh)[1]  , marker= 'o' , alpha = .002525,  s = 100)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(20,20) )\n",
    "        #red and green plt\n",
    "\n",
    "        reds =[]\n",
    "        oranges=[]\n",
    "        greens =[]\n",
    "\n",
    "        for i,label in enumerate(list(l.flat)):\n",
    "            if c[i]< 15 and c[i]>2 :\n",
    "                \n",
    "                mask = copy.deepcopy(subthresh)\n",
    "                zeroed = np.where(non_gap_labels == label )[0]\n",
    "                \n",
    "                \n",
    "                if len(list(zeroed))>2:\n",
    "                    \n",
    "                    nz_pts = [ (ptx,pty) for ptx,pty in itertools.combinations(list(zeroed) , 2 ) if ptx < subthresh.shape[1] and pty <  subthresh.shape[1] ]\n",
    "                    \n",
    "                    \n",
    "                    green = np.array([ check_point(list(np.nonzero(subthresh)[0]) , list(np.nonzero(subthresh)[1]), ptx , pty,radius = 15 )  for ptx,pty in  nz_pts ])\n",
    "\n",
    "                    if len(green)>0:\n",
    "                        orange = np.array(  [ check_point(list(connnectedx) , list(connectedy), ptx , pty,radius = 10 )  for ptx,pty in nz_pts ]   )\n",
    "\n",
    "                        orange[ np.logical_and( green , orange ) ] = False\n",
    "\n",
    "                        red = copy.deepcopy(green)\n",
    "\n",
    "                        if len(red)>0:\n",
    "                            red = ~red\n",
    "                            red[orange == True] = False\n",
    "                        ones = np.ones((len(nz_pts),))\n",
    "\n",
    "\n",
    "                        if len(red)==0:\n",
    "                            reds.append(0)\n",
    "                        else:\n",
    "                            reds.append(np.sum(ones[red]))\n",
    "\n",
    "                        oranges.append(np.sum(ones[orange]))\n",
    "                        greens.append(np.sum(ones[green]))\n",
    "\n",
    "                        #pts = np.array( [[ptx,pty] for ptx,pty in  itertools.combinations(list(zeroed) ,2) ] )\n",
    "                        nz_pts = np.array(nz_pts)\n",
    "                        plt.scatter( nz_pts[green,0] ,nz_pts[green,1], c= 'green' , marker= 'X', s=100)\n",
    "                        plt.scatter( nz_pts[green,1] ,nz_pts[green,0], c= 'green' , marker= 'X', s=100)\n",
    "\n",
    "\n",
    "                        plt.scatter( nz_pts[red,0] ,nz_pts[red,1] , c= 'red' , marker= 'X' , s=100)\n",
    "                        plt.scatter( nz_pts[red,1] ,nz_pts[red,0] , c= 'red' , marker= 'X' , s=100)\n",
    "\n",
    "\n",
    "                        plt.scatter( nz_pts[orange,1] ,nz_pts[orange,0], c= 'orange' , marker= 'X', s=100)\n",
    "                        plt.scatter( nz_pts[orange,0] ,nz_pts[orange,1], c= 'orange' , marker= 'X', s=100)\n",
    "\n",
    "        totalreds.append(np.sum(reds))\n",
    "        totaloranges.append(np.sum(oranges))\n",
    "        totalgreens.append(np.sum(greens))\n",
    "        blurvec.append(blurs)\n",
    "        plt.scatter( np.nonzero(subthresh)[0] ,np.nonzero(subthresh)[1]  , marker= 'o' , alpha = .002525,  s = 100)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "green2red = np.divide( np.array(totalgreens) , np.array(totalreds)+np.array(totalgreens ) )\n",
    "\n",
    "bluriters = [1,5,10,20,30,40,50,60, 70 ,80 , 90, 100 ]+[100,110,120,130,140,150 ]\n",
    "\n",
    "fig, ax1 = plt.subplots( figsize=(10,10))\n",
    "\n",
    "ax1.set_xlabel('blurs')\n",
    "ax1.set_ylabel('counts', color='black')\n",
    "ax1.plot( bluriters,  totalgreens[1:] , color='green' , label= 'greens')\n",
    "ax1.plot(bluriters,  totalreds[1:] , color='red', label = 'reds')\n",
    "ax1.plot(bluriters, totaloranges[1:] , color='orange', label = 'oranges')\n",
    "\n",
    "positives = np.array(totaloranges[1:]) + np.array(totalgreens[1:])\n",
    "ax1.plot(bluriters, positives , color='orange' , linestyle='dashed' , label = \"positives (g+o) \")\n",
    "\n",
    "\n",
    "positives2red = np.divide( positives  ,positives + np.array(totalreds[1:])  )\n",
    "\n",
    "ax1.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel('green to red ratio', color='black')  # we already handled the x-label with ax1\n",
    "ax2.set_ylim( (0,1.25))\n",
    "ax2.plot( bluriters, green2red[1:] , linestyle='dashed'  , color='black' , label = 'green/green+red')\n",
    "ax2.plot( bluriters , positives2red , linestyle='dotted'  , color='black' , label = 'positives/positives+red')\n",
    "\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "ax1.legend(loc='center right')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize blur factor and blur iter as a function of tree branch length distribution\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate plot "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
